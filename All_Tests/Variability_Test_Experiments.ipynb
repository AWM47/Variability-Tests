{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.2014\n",
      "RMS_H: Importance 0.1676\n",
      "Mean_L: Importance 0.0825\n",
      "Variance_L: Importance 0.0524\n",
      "RMS_L: Importance 0.0807\n",
      "Mean_Freq_H: Importance 0.0285\n",
      "Entropy_L: Importance 0.0177\n",
      "Max_H: Importance 0.0459\n",
      "Entropy_H: Importance 0.0973\n",
      "Mean Deviation_H: Importance 0.0128\n",
      "Min_L: Importance 0.0351\n",
      "Variance_H: Importance 0.0903\n",
      "Mean_Freq_L: Importance 0.0061\n",
      "Min_H: Importance 0.0052\n",
      "Mean Deviation_L: Importance 0.0139\n",
      "Max_L: Importance 0.0356\n",
      "Std_L: Importance 0.0130\n",
      "Skewness_H: Importance 0.0047\n",
      "Skewness_L: Importance 0.0092\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9574\n",
      "Balanced Accuracy: 0.9574\n",
      "MCC: 0.9169\n",
      "Log Loss: 0.1001\n",
      "F1 Score: 0.9588\n",
      "Recall: 0.9915\n",
      "Precision: 0.9282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.92      0.96      9998\n",
      "         CH2       0.93      0.99      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9231  767]\n",
      " [  85 9913]]\n",
      "False Positive Rate (FPR): 0.0767\n",
      "False Negative Rate (FNR): 0.0085\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 10MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9520\n",
      "Balanced Accuracy: 0.9520\n",
      "MCC: 0.9082\n",
      "Log Loss: 0.1638\n",
      "F1 Score: 0.9497\n",
      "Recall: 0.9047\n",
      "Precision: 0.9993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.91      1.00      0.95      9998\n",
      "         CH2       1.00      0.90      0.95      9998\n",
      "\n",
      "    accuracy                           0.95     19996\n",
      "   macro avg       0.96      0.95      0.95     19996\n",
      "weighted avg       0.96      0.95      0.95     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9992    6]\n",
      " [ 953 9045]]\n",
      "False Positive Rate (FPR): 0.0006\n",
      "False Negative Rate (FNR): 0.0953\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_10_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 10MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9985\n",
      "Balanced Accuracy: 0.9985\n",
      "MCC: 0.9971\n",
      "Log Loss: 0.0092\n",
      "F1 Score: 0.9985\n",
      "Recall: 0.9971\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9998    0]\n",
      " [  29 9969]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0029\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_10_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.2719\n",
      "RMS_H: Importance 0.1919\n",
      "Mean_L: Importance 0.1784\n",
      "Variance_L: Importance 0.0175\n",
      "RMS_L: Importance 0.1202\n",
      "Mean_Freq_H: Importance 0.0316\n",
      "Entropy_L: Importance 0.0014\n",
      "Max_H: Importance 0.0406\n",
      "Entropy_H: Importance 0.0840\n",
      "Mean Deviation_H: Importance 0.0000\n",
      "Min_L: Importance 0.0028\n",
      "Variance_H: Importance 0.0413\n",
      "Mean_Freq_L: Importance 0.0025\n",
      "Min_H: Importance 0.0135\n",
      "Mean Deviation_L: Importance 0.0002\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0019\n",
      "Skewness_H: Importance 0.0001\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9999\n",
      "Balanced Accuracy: 0.9999\n",
      "MCC: 0.9998\n",
      "Log Loss: 0.0020\n",
      "F1 Score: 0.9999\n",
      "Recall: 0.9998\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   1 4997]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe 3 Test at 20 MSPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_20_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 20MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0011\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_20_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1816\n",
      "RMS_H: Importance 0.1438\n",
      "Mean_L: Importance 0.1637\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1506\n",
      "Mean_Freq_H: Importance 0.0138\n",
      "Entropy_L: Importance 0.0003\n",
      "Max_H: Importance 0.0362\n",
      "Entropy_H: Importance 0.0908\n",
      "Mean Deviation_H: Importance 0.0001\n",
      "Min_L: Importance 0.0000\n",
      "Variance_H: Importance 0.1083\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0309\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9973\n",
      "Balanced Accuracy: 0.9973\n",
      "MCC: 0.9946\n",
      "Log Loss: 0.0050\n",
      "F1 Score: 0.9973\n",
      "Recall: 0.9949\n",
      "Precision: 0.9997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      3332\n",
      "         CH2       1.00      0.99      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    1]\n",
      " [  17 3314]]\n",
      "False Positive Rate (FPR): 0.0003\n",
      "False Negative Rate (FNR): 0.0051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 30MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.8922\n",
      "Balanced Accuracy: 0.8923\n",
      "MCC: 0.8034\n",
      "Log Loss: 0.7624\n",
      "F1 Score: 0.8792\n",
      "Recall: 0.7845\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.82      1.00      0.90      3331\n",
      "         CH2       1.00      0.78      0.88      3332\n",
      "\n",
      "    accuracy                           0.89      6663\n",
      "   macro avg       0.91      0.89      0.89      6663\n",
      "weighted avg       0.91      0.89      0.89      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [ 718 2614]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.2155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_30_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 30MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9958\n",
      "Balanced Accuracy: 0.9958\n",
      "MCC: 0.9916\n",
      "Log Loss: 0.0151\n",
      "F1 Score: 0.9958\n",
      "Recall: 0.9916\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      3332\n",
      "         CH2       1.00      0.99      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [  28 3303]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_30_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1860\n",
      "RMS_H: Importance 0.1447\n",
      "Mean_L: Importance 0.1713\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1565\n",
      "Mean_Freq_H: Importance 0.0142\n",
      "Entropy_L: Importance 0.0305\n",
      "Max_H: Importance 0.0394\n",
      "Entropy_H: Importance 0.0290\n",
      "Mean Deviation_H: Importance 0.0005\n",
      "Min_L: Importance 0.0000\n",
      "Variance_H: Importance 0.1050\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0287\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "Max_L: Importance 0.0142\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 40MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0037\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_40_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 40MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0038\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_40_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1800\n",
      "RMS_H: Importance 0.1200\n",
      "Mean_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.1800\n",
      "Entropy_L: Importance 0.0196\n",
      "Max_H: Importance 0.0399\n",
      "Entropy_H: Importance 0.1001\n",
      "Mean Deviation_H: Importance 0.0000\n",
      "Min_L: Importance 0.0000\n",
      "Variance_H: Importance 0.0803\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0000\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0416\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 50 MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9992\n",
      "Balanced Accuracy: 0.9992\n",
      "MCC: 0.9985\n",
      "Log Loss: 0.0402\n",
      "F1 Score: 0.9992\n",
      "Recall: 0.9985\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   3 1995]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_50_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 50MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0415\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_50_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1886\n",
      "RMS_H: Importance 0.1493\n",
      "Mean_L: Importance 0.1782\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1576\n",
      "Mean_Freq_H: Importance 0.0000\n",
      "Entropy_L: Importance 0.0000\n",
      "Max_H: Importance 0.0457\n",
      "Entropy_H: Importance 0.0205\n",
      "Mean Deviation_H: Importance 0.0230\n",
      "Min_L: Importance 0.0094\n",
      "Variance_H: Importance 0.1063\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0000\n",
      "Mean Deviation_L: Importance 0.0406\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0007\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 100MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_100_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 100MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0016\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_100_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 150 MSPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1200\n",
      "RMS_H: Importance 0.1208\n",
      "Mean_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0610\n",
      "Mean_Freq_H: Importance 0.0190\n",
      "Entropy_L: Importance 0.0000\n",
      "Max_H: Importance 0.1000\n",
      "Entropy_H: Importance 0.0382\n",
      "Mean Deviation_H: Importance 0.1200\n",
      "Min_L: Importance 0.0010\n",
      "Variance_H: Importance 0.1000\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0000\n",
      "Mean Deviation_L: Importance 0.1400\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 150MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0462\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_150_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 150MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0549\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_150_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1200\n",
      "RMS_H: Importance 0.1202\n",
      "Mean_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0000\n",
      "Entropy_L: Importance 0.0195\n",
      "Max_H: Importance 0.1000\n",
      "Entropy_H: Importance 0.0000\n",
      "Mean Deviation_H: Importance 0.1203\n",
      "Min_L: Importance 0.0600\n",
      "Variance_H: Importance 0.1000\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0000\n",
      "Mean Deviation_L: Importance 0.1400\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0082\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 200 MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1617\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_200_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 200 MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1577\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_200_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 250 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1200\n",
      "RMS_H: Importance 0.1202\n",
      "Mean_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0601\n",
      "Mean_Freq_H: Importance 0.0000\n",
      "Entropy_L: Importance 0.0398\n",
      "Max_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0000\n",
      "Mean Deviation_H: Importance 0.1000\n",
      "Min_L: Importance 0.0799\n",
      "Variance_H: Importance 0.1000\n",
      "Mean_Freq_L: Importance 0.0000\n",
      "Min_H: Importance 0.0000\n",
      "Mean Deviation_L: Importance 0.1200\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0000\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0013\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 250 MSPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0289\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_250_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 250MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0289\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_250_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300 MSPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with 19 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "\n",
      "Selected Features and Importances:\n",
      "Mean_H: Importance 0.1000\n",
      "RMS_H: Importance 0.0800\n",
      "Mean_L: Importance 0.0400\n",
      "Variance_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.1000\n",
      "Entropy_L: Importance 0.1000\n",
      "Max_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0600\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "Min_L: Importance 0.0400\n",
      "Variance_H: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Min_H: Importance 0.0000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "Max_L: Importance 0.0000\n",
      "Std_L: Importance 0.0400\n",
      "Skewness_H: Importance 0.0000\n",
      "Skewness_L: Importance 0.0000\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0022\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Select only the 19 required features from the 38 loaded features\n",
    "selected_features = [\n",
    "    \"Mean_H\", \"RMS_H\", \"Mean_L\", \"Variance_L\", \"RMS_L\", \"Mean_Freq_H\", \"Entropy_L\", \n",
    "    \"Max_H\", \"Entropy_H\", \"Mean Deviation_H\", \"Min_L\", \"Variance_H\", \"Mean_Freq_L\", \n",
    "    \"Min_H\", \"Mean Deviation_L\", \"Max_L\", \"Std_L\", \"Skewness_H\", \"Skewness_L\"\n",
    "]\n",
    "\n",
    "# Get indices of selected features in the original dataset\n",
    "selected_feature_indices = [all_feature_names.index(f) for f in selected_features]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack training and test data\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_train_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Train Model with Selected Features (19 Features)\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importance only for selected features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print Selected Features and their Importances\n",
    "print(\"\\nSelected Features and Importances:\")\n",
    "for feature_idx, feature_name in zip(range(len(selected_features)), selected_features):\n",
    "    print(f\"{feature_name}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(selected_feature_indices, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the models trained from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC2 Probe3 Test at 300MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0616\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_300_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC3 Probe3 Test at 300 MSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "\n",
      "Model Performance Using 19 Features:\n",
      "Test Accuracy: 0.9955\n",
      "Balanced Accuracy: 0.9955\n",
      "MCC: 0.9910\n",
      "Log Loss: 0.1455\n",
      "F1 Score: 0.9955\n",
      "Recall: 0.9910\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00       331\n",
      "         CH2       1.00      0.99      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  3 329]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, log_loss,\n",
    "    f1_score, recall_score, precision_score, matthews_corrcoef, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Directory where the model and scaler were saved\n",
    "model_dir = f\"Models_test/19_Feature_Tuning_Probe1_to_Probe2_ADC1_300_MSPS\"\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model_path = os.path.join(model_dir, \"RandomForest_Model.pkl\")\n",
    "scaler_path = os.path.join(model_dir, \"Scaler.pkl\")\n",
    "features_path = os.path.join(model_dir, \"Selected_Features.pkl\")\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(scaler_path) or not os.path.exists(features_path):\n",
    "    raise FileNotFoundError(\"Model, scaler, or selected features file not found.\")\n",
    "\n",
    "rf_selected = joblib.load(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "selected_feature_indices = joblib.load(features_path)\n",
    "\n",
    "# List of all 38 loaded features\n",
    "all_feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\"\n",
    "]\n",
    "\n",
    "# Get feature names for selected features\n",
    "selected_features = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Load Test Data\n",
    "# ==================================================================\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "# Stack test data\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# Apply standardization using the loaded scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Select only required 19 features from the 38 loaded dataset\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "# ==================================================================\n",
    "# Evaluate the Model\n",
    "# ==================================================================\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"\\nModel Performance Using 19 Features:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
