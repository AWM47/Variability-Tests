{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADC1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 1**\n",
    "To conduct experiments, **ADC1** (data acquisition device) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on Probe 1**. The model is then tested on **Probe 2** and **Probe 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 1** data and Testing on **Probe 2** data\n",
    "To test Probe 1 with changing probe to Probe 2 and Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0527\n",
      "Min_H: Importance 0.0141\n",
      "Mean_H: Importance 0.0989\n",
      "Std_H: Importance 0.0076\n",
      "Mean Deviation_H: Importance 0.0216\n",
      "RMS_H: Importance 0.1667\n",
      "Entropy_H: Importance 0.0460\n",
      "Mean_Freq_H: Importance 0.0641\n",
      "Variance_H: Importance 0.0496\n",
      "Max_L: Importance 0.0324\n",
      "Min_L: Importance 0.0696\n",
      "Mean_L: Importance 0.0698\n",
      "Std_L: Importance 0.0075\n",
      "RMS_L: Importance 0.0890\n",
      "Skewness_L: Importance 0.0072\n",
      "Centroid_L: Importance 0.0114\n",
      "Entropy_L: Importance 0.0299\n",
      "Mean_Freq_L: Importance 0.0159\n",
      "Variance_L: Importance 0.0973\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9518\n",
      "Balanced Accuracy: 0.9518\n",
      "MCC: 0.9064\n",
      "Log Loss: 0.1153\n",
      "F1 Score: 0.9536\n",
      "Recall: 0.9912\n",
      "Precision: 0.9188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.91      0.95      9998\n",
      "         CH2       0.92      0.99      0.95      9998\n",
      "\n",
      "    accuracy                           0.95     19996\n",
      "   macro avg       0.95      0.95      0.95     19996\n",
      "weighted avg       0.95      0.95      0.95     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9122  876]\n",
      " [  88 9910]]\n",
      "False Positive Rate (FPR): 0.0876\n",
      "False Negative Rate (FNR): 0.0088\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1224\n",
      "Min_H: Importance 0.0284\n",
      "Mean_H: Importance 0.1486\n",
      "Mean Deviation_H: Importance 0.0095\n",
      "RMS_H: Importance 0.2172\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.0173\n",
      "Mean_Freq_H: Importance 0.0727\n",
      "Variance_H: Importance 0.0400\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0255\n",
      "Mean_L: Importance 0.1253\n",
      "Std_L: Importance 0.0012\n",
      "RMS_L: Importance 0.1210\n",
      "Peak-to-Peak_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0108\n",
      "Variance_L: Importance 0.0256\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9999\n",
      "Balanced Accuracy: 0.9999\n",
      "MCC: 0.9998\n",
      "Log Loss: 0.0020\n",
      "F1 Score: 0.9999\n",
      "Recall: 0.9998\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   1 4997]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0683\n",
      "Min_H: Importance 0.0315\n",
      "Mean_H: Importance 0.0935\n",
      "Std_H: Importance 0.0002\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1295\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0179\n",
      "Mean_Freq_H: Importance 0.0551\n",
      "Kurtosis_Freq_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0675\n",
      "Max_L: Importance 0.0063\n",
      "Min_L: Importance 0.0191\n",
      "Mean_L: Importance 0.2050\n",
      "RMS_L: Importance 0.1282\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0234\n",
      "Mean_Freq_L: Importance 0.0096\n",
      "Variance_L: Importance 0.1366\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9973\n",
      "Balanced Accuracy: 0.9973\n",
      "MCC: 0.9946\n",
      "Log Loss: 0.0052\n",
      "F1 Score: 0.9973\n",
      "Recall: 0.9949\n",
      "Precision: 0.9997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      3332\n",
      "         CH2       1.00      0.99      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    1]\n",
      " [  17 3314]]\n",
      "False Positive Rate (FPR): 0.0003\n",
      "False Negative Rate (FNR): 0.0051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0684\n",
      "Min_H: Importance 0.0280\n",
      "Mean_H: Importance 0.0961\n",
      "Std_H: Importance 0.0123\n",
      "Mean Deviation_H: Importance 0.0169\n",
      "RMS_H: Importance 0.1322\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0144\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0579\n",
      "Variance_H: Importance 0.0606\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0165\n",
      "Mean_L: Importance 0.1996\n",
      "RMS_L: Importance 0.1272\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0301\n",
      "Mean_Freq_L: Importance 0.0124\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0601\n",
      "Min_H: Importance 0.0371\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0381\n",
      "Mean Deviation_H: Importance 0.0205\n",
      "RMS_H: Importance 0.0808\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.1024\n",
      "Variance_H: Importance 0.0601\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1433\n",
      "Mean Deviation_L: Importance 0.0151\n",
      "RMS_L: Importance 0.0803\n",
      "Entropy_L: Importance 0.0390\n",
      "Variance_L: Importance 0.1207\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9997\n",
      "Balanced Accuracy: 0.9997\n",
      "MCC: 0.9995\n",
      "Log Loss: 0.0367\n",
      "F1 Score: 0.9997\n",
      "Recall: 0.9995\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   1 1997]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0861\n",
      "Min_H: Importance 0.0066\n",
      "Mean_H: Importance 0.0985\n",
      "Std_H: Importance 0.0055\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1472\n",
      "Skewness_H: Importance 0.0007\n",
      "Centroid_H: Importance 0.0055\n",
      "Entropy_H: Importance 0.0095\n",
      "Mean_Freq_H: Importance 0.0104\n",
      "Variance_H: Importance 0.0821\n",
      "Max_L: Importance 0.0105\n",
      "Min_L: Importance 0.0172\n",
      "Mean_L: Importance 0.1887\n",
      "Std_L: Importance 0.0048\n",
      "Mean Deviation_L: Importance 0.0267\n",
      "RMS_L: Importance 0.1411\n",
      "Entropy_L: Importance 0.0052\n",
      "Variance_L: Importance 0.1124\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0012\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0849\n",
      "Std_H: Importance 0.0575\n",
      "Mean Deviation_H: Importance 0.0650\n",
      "RMS_H: Importance 0.1222\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0191\n",
      "Mean_Freq_H: Importance 0.0381\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0215\n",
      "Mean_L: Importance 0.1811\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1208\n",
      "Entropy_L: Importance 0.0147\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0852\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0171\n",
      "Kurtosis_Freq_H: Importance 0.0293\n",
      "Variance_H: Importance 0.0401\n",
      "Min_L: Importance 0.0865\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0195\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1240\n",
      "Entropy_L: Importance 0.0194\n",
      "Mean_Freq_L: Importance 0.0136\n",
      "Variance_L: Importance 0.0609\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0064\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0803\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1239\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0189\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1001\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1211\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0398\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0011\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0203\n",
      "RMS_H: Importance 0.0802\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0195\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 1** data and Testing on **Probe 3** data\n",
    "To test Probe 1 with changing probe to Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0527\n",
      "Min_H: Importance 0.0141\n",
      "Mean_H: Importance 0.0989\n",
      "Std_H: Importance 0.0076\n",
      "Mean Deviation_H: Importance 0.0216\n",
      "RMS_H: Importance 0.1667\n",
      "Entropy_H: Importance 0.0460\n",
      "Mean_Freq_H: Importance 0.0641\n",
      "Variance_H: Importance 0.0496\n",
      "Max_L: Importance 0.0324\n",
      "Min_L: Importance 0.0696\n",
      "Mean_L: Importance 0.0698\n",
      "Std_L: Importance 0.0075\n",
      "RMS_L: Importance 0.0890\n",
      "Skewness_L: Importance 0.0072\n",
      "Centroid_L: Importance 0.0114\n",
      "Entropy_L: Importance 0.0299\n",
      "Mean_Freq_L: Importance 0.0159\n",
      "Variance_L: Importance 0.0973\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9597\n",
      "Balanced Accuracy: 0.9597\n",
      "MCC: 0.9218\n",
      "Log Loss: 0.1084\n",
      "F1 Score: 0.9611\n",
      "Recall: 0.9958\n",
      "Precision: 0.9287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.92      0.96      9998\n",
      "         CH2       0.93      1.00      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9234  764]\n",
      " [  42 9956]]\n",
      "False Positive Rate (FPR): 0.0764\n",
      "False Negative Rate (FNR): 0.0042\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1224\n",
      "Min_H: Importance 0.0284\n",
      "Mean_H: Importance 0.1486\n",
      "Mean Deviation_H: Importance 0.0095\n",
      "RMS_H: Importance 0.2172\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.0173\n",
      "Mean_Freq_H: Importance 0.0727\n",
      "Variance_H: Importance 0.0400\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0255\n",
      "Mean_L: Importance 0.1253\n",
      "Std_L: Importance 0.0012\n",
      "RMS_L: Importance 0.1210\n",
      "Peak-to-Peak_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0108\n",
      "Variance_L: Importance 0.0256\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0016\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0683\n",
      "Min_H: Importance 0.0315\n",
      "Mean_H: Importance 0.0935\n",
      "Std_H: Importance 0.0002\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1295\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0179\n",
      "Mean_Freq_H: Importance 0.0551\n",
      "Kurtosis_Freq_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0675\n",
      "Max_L: Importance 0.0063\n",
      "Min_L: Importance 0.0191\n",
      "Mean_L: Importance 0.2050\n",
      "RMS_L: Importance 0.1282\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0234\n",
      "Mean_Freq_L: Importance 0.0096\n",
      "Variance_L: Importance 0.1366\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9676\n",
      "Balanced Accuracy: 0.9676\n",
      "MCC: 0.9370\n",
      "Log Loss: 0.0684\n",
      "F1 Score: 0.9665\n",
      "Recall: 0.9364\n",
      "Precision: 0.9987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.94      1.00      0.97      3331\n",
      "         CH2       1.00      0.94      0.97      3332\n",
      "\n",
      "    accuracy                           0.97      6663\n",
      "   macro avg       0.97      0.97      0.97      6663\n",
      "weighted avg       0.97      0.97      0.97      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3327    4]\n",
      " [ 212 3120]]\n",
      "False Positive Rate (FPR): 0.0012\n",
      "False Negative Rate (FNR): 0.0636\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0684\n",
      "Min_H: Importance 0.0280\n",
      "Mean_H: Importance 0.0961\n",
      "Std_H: Importance 0.0123\n",
      "Mean Deviation_H: Importance 0.0169\n",
      "RMS_H: Importance 0.1322\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0144\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0579\n",
      "Variance_H: Importance 0.0606\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0165\n",
      "Mean_L: Importance 0.1996\n",
      "RMS_L: Importance 0.1272\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0301\n",
      "Mean_Freq_L: Importance 0.0124\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0601\n",
      "Min_H: Importance 0.0371\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0381\n",
      "Mean Deviation_H: Importance 0.0205\n",
      "RMS_H: Importance 0.0808\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.1024\n",
      "Variance_H: Importance 0.0601\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1433\n",
      "Mean Deviation_L: Importance 0.0151\n",
      "RMS_L: Importance 0.0803\n",
      "Entropy_L: Importance 0.0390\n",
      "Variance_L: Importance 0.1207\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9992\n",
      "Balanced Accuracy: 0.9992\n",
      "MCC: 0.9985\n",
      "Log Loss: 0.0408\n",
      "F1 Score: 0.9992\n",
      "Recall: 0.9985\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   3 1995]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0861\n",
      "Min_H: Importance 0.0066\n",
      "Mean_H: Importance 0.0985\n",
      "Std_H: Importance 0.0055\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1472\n",
      "Skewness_H: Importance 0.0007\n",
      "Centroid_H: Importance 0.0055\n",
      "Entropy_H: Importance 0.0095\n",
      "Mean_Freq_H: Importance 0.0104\n",
      "Variance_H: Importance 0.0821\n",
      "Max_L: Importance 0.0105\n",
      "Min_L: Importance 0.0172\n",
      "Mean_L: Importance 0.1887\n",
      "Std_L: Importance 0.0048\n",
      "Mean Deviation_L: Importance 0.0267\n",
      "RMS_L: Importance 0.1411\n",
      "Entropy_L: Importance 0.0052\n",
      "Variance_L: Importance 0.1124\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0017\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0849\n",
      "Std_H: Importance 0.0575\n",
      "Mean Deviation_H: Importance 0.0650\n",
      "RMS_H: Importance 0.1222\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0191\n",
      "Mean_Freq_H: Importance 0.0381\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0215\n",
      "Mean_L: Importance 0.1811\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1208\n",
      "Entropy_L: Importance 0.0147\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0041\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0852\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0171\n",
      "Kurtosis_Freq_H: Importance 0.0293\n",
      "Variance_H: Importance 0.0401\n",
      "Min_L: Importance 0.0865\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0195\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1240\n",
      "Entropy_L: Importance 0.0194\n",
      "Mean_Freq_L: Importance 0.0136\n",
      "Variance_L: Importance 0.0609\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0023\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0803\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1239\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0189\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1001\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1211\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0398\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0203\n",
      "RMS_H: Importance 0.0802\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0195\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0041\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 2**\n",
    "To conduct experiments, **ADC1** (data acquisition device) is used consistently throughout. **Probe 2** is kept constant, and the machine learning model is **always trained on Probe 2**. The model is then tested on **Probe 3** and **Probe 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 2** data and Testing on **Probe 3**\n",
    "To test Probe 2 with changing probe to Probes 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0178\n",
      "Mean_H: Importance 0.0563\n",
      "Std_H: Importance 0.0118\n",
      "Mean Deviation_H: Importance 0.0153\n",
      "RMS_H: Importance 0.1167\n",
      "Centroid_H: Importance 0.0096\n",
      "Entropy_H: Importance 0.0320\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Irregularity_H: Importance 0.0143\n",
      "Variance_H: Importance 0.0354\n",
      "Max_L: Importance 0.0187\n",
      "Min_L: Importance 0.0724\n",
      "Mean_L: Importance 0.1135\n",
      "RMS_L: Importance 0.1859\n",
      "Skewness_L: Importance 0.0125\n",
      "Centroid_L: Importance 0.0103\n",
      "Entropy_L: Importance 0.0303\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0820\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9792\n",
      "Balanced Accuracy: 0.9792\n",
      "MCC: 0.9585\n",
      "Log Loss: 0.0553\n",
      "F1 Score: 0.9793\n",
      "Recall: 0.9822\n",
      "Precision: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      0.98      0.98      9998\n",
      "         CH2       0.98      0.98      0.98      9998\n",
      "\n",
      "    accuracy                           0.98     19996\n",
      "   macro avg       0.98      0.98      0.98     19996\n",
      "weighted avg       0.98      0.98      0.98     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9761  237]\n",
      " [ 178 9820]]\n",
      "False Positive Rate (FPR): 0.0237\n",
      "False Negative Rate (FNR): 0.0178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1194\n",
      "Min_H: Importance 0.0143\n",
      "Mean_H: Importance 0.1108\n",
      "Mean Deviation_H: Importance 0.0075\n",
      "RMS_H: Importance 0.2006\n",
      "Entropy_H: Importance 0.0357\n",
      "Mean_Freq_H: Importance 0.0776\n",
      "Variance_H: Importance 0.0614\n",
      "Max_L: Importance 0.0141\n",
      "Min_L: Importance 0.0410\n",
      "Mean_L: Importance 0.1045\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1372\n",
      "Skewness_L: Importance 0.0021\n",
      "Kurtosis_L: Importance 0.0013\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0486\n",
      "Mean_Freq_L: Importance 0.0137\n",
      "Variance_L: Importance 0.0078\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0006\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0676\n",
      "Min_H: Importance 0.0269\n",
      "Mean_H: Importance 0.1364\n",
      "Mean Deviation_H: Importance 0.0034\n",
      "RMS_H: Importance 0.2181\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0171\n",
      "Mean_Freq_H: Importance 0.0392\n",
      "Irregularity_H: Importance 0.0009\n",
      "Variance_H: Importance 0.0507\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1537\n",
      "RMS_L: Importance 0.1082\n",
      "Kurtosis_L: Importance 0.0019\n",
      "Entropy_L: Importance 0.0264\n",
      "Mean_Freq_L: Importance 0.0095\n",
      "Irregularity_L: Importance 0.0008\n",
      "Variance_L: Importance 0.0949\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9865\n",
      "Balanced Accuracy: 0.9865\n",
      "MCC: 0.9732\n",
      "Log Loss: 0.0347\n",
      "F1 Score: 0.9866\n",
      "Recall: 0.9973\n",
      "Precision: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.98      0.99      3331\n",
      "         CH2       0.98      1.00      0.99      3332\n",
      "\n",
      "    accuracy                           0.99      6663\n",
      "   macro avg       0.99      0.99      0.99      6663\n",
      "weighted avg       0.99      0.99      0.99      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3250   81]\n",
      " [   9 3323]]\n",
      "False Positive Rate (FPR): 0.0243\n",
      "False Negative Rate (FNR): 0.0027\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0664\n",
      "Min_H: Importance 0.0285\n",
      "Mean_H: Importance 0.0956\n",
      "Mean Deviation_H: Importance 0.0158\n",
      "RMS_H: Importance 0.1320\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Irregularity_H: Importance 0.0000\n",
      "Variance_H: Importance 0.0610\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.2098\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1206\n",
      "Entropy_L: Importance 0.0309\n",
      "Mean_Freq_L: Importance 0.0122\n",
      "Variance_L: Importance 0.1276\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0682\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.0993\n",
      "Std_H: Importance 0.0100\n",
      "Mean Deviation_H: Importance 0.0238\n",
      "RMS_H: Importance 0.1287\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0283\n",
      "Spread_H: Importance 0.0072\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0133\n",
      "Mean_L: Importance 0.1748\n",
      "Mean Deviation_L: Importance 0.0109\n",
      "RMS_L: Importance 0.1363\n",
      "Entropy_L: Importance 0.0447\n",
      "Mean_Freq_L: Importance 0.0143\n",
      "Variance_L: Importance 0.1081\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0005\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1078\n",
      "Min_H: Importance 0.0001\n",
      "Mean_H: Importance 0.0924\n",
      "Mean Deviation_H: Importance 0.0135\n",
      "RMS_H: Importance 0.1310\n",
      "Entropy_H: Importance 0.0146\n",
      "Mean_Freq_H: Importance 0.0398\n",
      "Kurtosis_Freq_H: Importance 0.0045\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0601\n",
      "Max_L: Importance 0.0009\n",
      "Min_L: Importance 0.0966\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0169\n",
      "RMS_L: Importance 0.1265\n",
      "Peak-to-Peak_L: Importance 0.0011\n",
      "Entropy_L: Importance 0.0139\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0873\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0088\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0848\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0811\n",
      "RMS_H: Importance 0.0635\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0154\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0802\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0422\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0194\n",
      "Mean_Freq_H: Importance 0.0184\n",
      "Kurtosis_Freq_H: Importance 0.0080\n",
      "Variance_H: Importance 0.0201\n",
      "Min_L: Importance 0.0732\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1216\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0400\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.0800\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0174\n",
      "Min_L: Importance 0.0024\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0802\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0037\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 2** data and Testing on **Probe 1**\n",
    "To test Probe 2 with changing probe to Probes 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0178\n",
      "Mean_H: Importance 0.0563\n",
      "Std_H: Importance 0.0118\n",
      "Mean Deviation_H: Importance 0.0153\n",
      "RMS_H: Importance 0.1167\n",
      "Centroid_H: Importance 0.0096\n",
      "Entropy_H: Importance 0.0320\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Irregularity_H: Importance 0.0143\n",
      "Variance_H: Importance 0.0354\n",
      "Max_L: Importance 0.0187\n",
      "Min_L: Importance 0.0724\n",
      "Mean_L: Importance 0.1135\n",
      "RMS_L: Importance 0.1859\n",
      "Skewness_L: Importance 0.0125\n",
      "Centroid_L: Importance 0.0103\n",
      "Entropy_L: Importance 0.0303\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0820\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9386\n",
      "Balanced Accuracy: 0.9386\n",
      "MCC: 0.8802\n",
      "Log Loss: 0.1438\n",
      "F1 Score: 0.9411\n",
      "Recall: 0.9796\n",
      "Precision: 0.9054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      0.90      0.94      9998\n",
      "         CH2       0.91      0.98      0.94      9998\n",
      "\n",
      "    accuracy                           0.94     19996\n",
      "   macro avg       0.94      0.94      0.94     19996\n",
      "weighted avg       0.94      0.94      0.94     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8975 1023]\n",
      " [ 204 9794]]\n",
      "False Positive Rate (FPR): 0.1023\n",
      "False Negative Rate (FNR): 0.0204\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1194\n",
      "Min_H: Importance 0.0143\n",
      "Mean_H: Importance 0.1108\n",
      "Mean Deviation_H: Importance 0.0075\n",
      "RMS_H: Importance 0.2006\n",
      "Entropy_H: Importance 0.0357\n",
      "Mean_Freq_H: Importance 0.0776\n",
      "Variance_H: Importance 0.0614\n",
      "Max_L: Importance 0.0141\n",
      "Min_L: Importance 0.0410\n",
      "Mean_L: Importance 0.1045\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1372\n",
      "Skewness_L: Importance 0.0021\n",
      "Kurtosis_L: Importance 0.0013\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0486\n",
      "Mean_Freq_L: Importance 0.0137\n",
      "Variance_L: Importance 0.0078\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0676\n",
      "Min_H: Importance 0.0269\n",
      "Mean_H: Importance 0.1364\n",
      "Mean Deviation_H: Importance 0.0034\n",
      "RMS_H: Importance 0.2181\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0171\n",
      "Mean_Freq_H: Importance 0.0392\n",
      "Irregularity_H: Importance 0.0009\n",
      "Variance_H: Importance 0.0507\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1537\n",
      "RMS_L: Importance 0.1082\n",
      "Kurtosis_L: Importance 0.0019\n",
      "Entropy_L: Importance 0.0264\n",
      "Mean_Freq_L: Importance 0.0095\n",
      "Irregularity_L: Importance 0.0008\n",
      "Variance_L: Importance 0.0949\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9998\n",
      "Balanced Accuracy: 0.9998\n",
      "MCC: 0.9997\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 0.9998\n",
      "Recall: 1.0000\n",
      "Precision: 0.9997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6664\n",
      "   macro avg       1.00      1.00      1.00      6664\n",
      "weighted avg       1.00      1.00      1.00      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    1]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0003\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0664\n",
      "Min_H: Importance 0.0285\n",
      "Mean_H: Importance 0.0956\n",
      "Mean Deviation_H: Importance 0.0158\n",
      "RMS_H: Importance 0.1320\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Irregularity_H: Importance 0.0000\n",
      "Variance_H: Importance 0.0610\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.2098\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1206\n",
      "Entropy_L: Importance 0.0309\n",
      "Mean_Freq_L: Importance 0.0122\n",
      "Variance_L: Importance 0.1276\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0682\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.0993\n",
      "Std_H: Importance 0.0100\n",
      "Mean Deviation_H: Importance 0.0238\n",
      "RMS_H: Importance 0.1287\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0283\n",
      "Spread_H: Importance 0.0072\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0133\n",
      "Mean_L: Importance 0.1748\n",
      "Mean Deviation_L: Importance 0.0109\n",
      "RMS_L: Importance 0.1363\n",
      "Entropy_L: Importance 0.0447\n",
      "Mean_Freq_L: Importance 0.0143\n",
      "Variance_L: Importance 0.1081\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1078\n",
      "Min_H: Importance 0.0001\n",
      "Mean_H: Importance 0.0924\n",
      "Mean Deviation_H: Importance 0.0135\n",
      "RMS_H: Importance 0.1310\n",
      "Entropy_H: Importance 0.0146\n",
      "Mean_Freq_H: Importance 0.0398\n",
      "Kurtosis_Freq_H: Importance 0.0045\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0601\n",
      "Max_L: Importance 0.0009\n",
      "Min_L: Importance 0.0966\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0169\n",
      "RMS_L: Importance 0.1265\n",
      "Peak-to-Peak_L: Importance 0.0011\n",
      "Entropy_L: Importance 0.0139\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0873\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0082\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0848\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0811\n",
      "RMS_H: Importance 0.0635\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0154\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0802\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0043\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0194\n",
      "Mean_Freq_H: Importance 0.0184\n",
      "Kurtosis_Freq_H: Importance 0.0080\n",
      "Variance_H: Importance 0.0201\n",
      "Min_L: Importance 0.0732\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1216\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0059\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0400\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.0800\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0126\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0174\n",
      "Min_L: Importance 0.0024\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0802\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0011\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 3**\n",
    "To conduct experiments, **ADC1** (data acquisition device) is used consistently throughout. **Probe 3** is kept constant, and the machine learning model is **always trained on Probe 3**. The model is then tested on **Probe 3** and **Probe 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 3** data and Testing on **Probe 1**\n",
    "To test Probe 3 with changing probe to Probes 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0062\n",
      "Min_H: Importance 0.0174\n",
      "Mean_H: Importance 0.0772\n",
      "Mean Deviation_H: Importance 0.0048\n",
      "RMS_H: Importance 0.1177\n",
      "Skewness_H: Importance 0.0029\n",
      "Entropy_H: Importance 0.0231\n",
      "Spread_H: Importance 0.0037\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Irregularity_H: Importance 0.0030\n",
      "Variance_H: Importance 0.0457\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.1182\n",
      "Mean_L: Importance 0.1648\n",
      "RMS_L: Importance 0.2142\n",
      "Skewness_L: Importance 0.0026\n",
      "Entropy_L: Importance 0.0385\n",
      "Mean_Freq_L: Importance 0.0161\n",
      "Variance_L: Importance 0.0790\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9204\n",
      "Balanced Accuracy: 0.9204\n",
      "MCC: 0.8474\n",
      "Log Loss: 0.1617\n",
      "F1 Score: 0.9251\n",
      "Recall: 0.9830\n",
      "Precision: 0.8736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      0.86      0.92      9998\n",
      "         CH2       0.87      0.98      0.93      9998\n",
      "\n",
      "    accuracy                           0.92     19996\n",
      "   macro avg       0.93      0.92      0.92     19996\n",
      "weighted avg       0.93      0.92      0.92     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8576 1422]\n",
      " [ 170 9828]]\n",
      "False Positive Rate (FPR): 0.1422\n",
      "False Negative Rate (FNR): 0.0170\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1215\n",
      "Min_H: Importance 0.0255\n",
      "Mean_H: Importance 0.1520\n",
      "Mean Deviation_H: Importance 0.0050\n",
      "RMS_H: Importance 0.2228\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0771\n",
      "Variance_H: Importance 0.0366\n",
      "Max_L: Importance 0.0011\n",
      "Min_L: Importance 0.0233\n",
      "Mean_L: Importance 0.1243\n",
      "Std_L: Importance 0.0013\n",
      "RMS_L: Importance 0.1203\n",
      "Skewness_L: Importance 0.0012\n",
      "Kurtosis_L: Importance 0.0030\n",
      "Entropy_L: Importance 0.0282\n",
      "Spread_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0117\n",
      "Variance_L: Importance 0.0252\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0010\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1332\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.1297\n",
      "Mean Deviation_H: Importance 0.0056\n",
      "RMS_H: Importance 0.2115\n",
      "Peak-to-Peak_H: Importance 0.0100\n",
      "Centroid_H: Importance 0.0061\n",
      "Entropy_H: Importance 0.0443\n",
      "Spread_H: Importance 0.0060\n",
      "Mean_Freq_H: Importance 0.0810\n",
      "Irregularity_H: Importance 0.0072\n",
      "Variance_H: Importance 0.0425\n",
      "Min_L: Importance 0.0276\n",
      "Mean_L: Importance 0.0888\n",
      "RMS_L: Importance 0.1151\n",
      "Skewness_L: Importance 0.0093\n",
      "Entropy_L: Importance 0.0251\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.0249\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0008\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6664\n",
      "   macro avg       1.00      1.00      1.00      6664\n",
      "weighted avg       1.00      1.00      1.00      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0150\n",
      "Mean_H: Importance 0.0941\n",
      "Mean Deviation_H: Importance 0.0277\n",
      "RMS_H: Importance 0.1154\n",
      "Peak-to-Peak_H: Importance 0.0033\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0142\n",
      "Mean_Freq_H: Importance 0.0588\n",
      "Variance_H: Importance 0.0632\n",
      "Max_L: Importance 0.0138\n",
      "Min_L: Importance 0.0464\n",
      "Mean_L: Importance 0.1817\n",
      "RMS_L: Importance 0.1361\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0457\n",
      "Spread_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0130\n",
      "Variance_L: Importance 0.1077\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0005\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0094\n",
      "Mean_H: Importance 0.0944\n",
      "Mean Deviation_H: Importance 0.0007\n",
      "RMS_H: Importance 0.1411\n",
      "Peak-to-Peak_H: Importance 0.0051\n",
      "Entropy_H: Importance 0.0147\n",
      "Mean_Freq_H: Importance 0.0505\n",
      "Variance_H: Importance 0.0836\n",
      "Max_L: Importance 0.0092\n",
      "Min_L: Importance 0.0402\n",
      "Mean_L: Importance 0.1728\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.0123\n",
      "RMS_L: Importance 0.1333\n",
      "Kurtosis_L: Importance 0.0020\n",
      "Entropy_L: Importance 0.0449\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0771\n",
      "Min_H: Importance 0.0103\n",
      "Mean_H: Importance 0.1020\n",
      "Mean Deviation_H: Importance 0.0307\n",
      "RMS_H: Importance 0.1455\n",
      "Skewness_H: Importance 0.0126\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0105\n",
      "Variance_H: Importance 0.0713\n",
      "Max_L: Importance 0.0098\n",
      "Min_L: Importance 0.0197\n",
      "Mean_L: Importance 0.1949\n",
      "Mean Deviation_L: Importance 0.0219\n",
      "RMS_L: Importance 0.1386\n",
      "Kurtosis_L: Importance 0.0161\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Skewness_Freq_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0024\n",
      "Variance_L: Importance 0.1334\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0807\n",
      "Mean Deviation_H: Importance 0.0430\n",
      "RMS_H: Importance 0.1256\n",
      "Entropy_H: Importance 0.0165\n",
      "Mean_Freq_H: Importance 0.0323\n",
      "Kurtosis_Freq_H: Importance 0.0108\n",
      "Variance_H: Importance 0.0206\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1805\n",
      "Std_L: Importance 0.0005\n",
      "Mean Deviation_L: Importance 0.1077\n",
      "RMS_L: Importance 0.1231\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0189\n",
      "Skewness_Freq_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0155\n",
      "Variance_L: Importance 0.0836\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0026\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1042\n",
      "Entropy_H: Importance 0.0600\n",
      "Mean_Freq_H: Importance 0.0186\n",
      "Irregularity_H: Importance 0.0116\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0858\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0208\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1007\n",
      "Kurtosis_L: Importance 0.0031\n",
      "Centroid_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0052\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0014\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0824\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0133\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0200\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0600\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Irregularity_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0500\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 3** data and Testing on **Probe 2**\n",
    "To test Probe 3 with changing probe to Probes 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0062\n",
      "Min_H: Importance 0.0174\n",
      "Mean_H: Importance 0.0772\n",
      "Mean Deviation_H: Importance 0.0048\n",
      "RMS_H: Importance 0.1177\n",
      "Skewness_H: Importance 0.0029\n",
      "Entropy_H: Importance 0.0231\n",
      "Spread_H: Importance 0.0037\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Irregularity_H: Importance 0.0030\n",
      "Variance_H: Importance 0.0457\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.1182\n",
      "Mean_L: Importance 0.1648\n",
      "RMS_L: Importance 0.2142\n",
      "Skewness_L: Importance 0.0026\n",
      "Entropy_L: Importance 0.0385\n",
      "Mean_Freq_L: Importance 0.0161\n",
      "Variance_L: Importance 0.0790\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9643\n",
      "Balanced Accuracy: 0.9643\n",
      "MCC: 0.9292\n",
      "Log Loss: 0.0969\n",
      "F1 Score: 0.9649\n",
      "Recall: 0.9825\n",
      "Precision: 0.9480\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      0.95      0.96      9998\n",
      "         CH2       0.95      0.98      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9459  539]\n",
      " [ 175 9823]]\n",
      "False Positive Rate (FPR): 0.0539\n",
      "False Negative Rate (FNR): 0.0175\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1215\n",
      "Min_H: Importance 0.0255\n",
      "Mean_H: Importance 0.1520\n",
      "Mean Deviation_H: Importance 0.0050\n",
      "RMS_H: Importance 0.2228\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0771\n",
      "Variance_H: Importance 0.0366\n",
      "Max_L: Importance 0.0011\n",
      "Min_L: Importance 0.0233\n",
      "Mean_L: Importance 0.1243\n",
      "Std_L: Importance 0.0013\n",
      "RMS_L: Importance 0.1203\n",
      "Skewness_L: Importance 0.0012\n",
      "Kurtosis_L: Importance 0.0030\n",
      "Entropy_L: Importance 0.0282\n",
      "Spread_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0117\n",
      "Variance_L: Importance 0.0252\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0010\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1332\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.1297\n",
      "Mean Deviation_H: Importance 0.0056\n",
      "RMS_H: Importance 0.2115\n",
      "Peak-to-Peak_H: Importance 0.0100\n",
      "Centroid_H: Importance 0.0061\n",
      "Entropy_H: Importance 0.0443\n",
      "Spread_H: Importance 0.0060\n",
      "Mean_Freq_H: Importance 0.0810\n",
      "Irregularity_H: Importance 0.0072\n",
      "Variance_H: Importance 0.0425\n",
      "Min_L: Importance 0.0276\n",
      "Mean_L: Importance 0.0888\n",
      "RMS_L: Importance 0.1151\n",
      "Skewness_L: Importance 0.0093\n",
      "Entropy_L: Importance 0.0251\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.0249\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9977\n",
      "Balanced Accuracy: 0.9977\n",
      "MCC: 0.9955\n",
      "Log Loss: 0.0145\n",
      "F1 Score: 0.9978\n",
      "Recall: 1.0000\n",
      "Precision: 0.9955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3317   15]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.0045\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0150\n",
      "Mean_H: Importance 0.0941\n",
      "Mean Deviation_H: Importance 0.0277\n",
      "RMS_H: Importance 0.1154\n",
      "Peak-to-Peak_H: Importance 0.0033\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0142\n",
      "Mean_Freq_H: Importance 0.0588\n",
      "Variance_H: Importance 0.0632\n",
      "Max_L: Importance 0.0138\n",
      "Min_L: Importance 0.0464\n",
      "Mean_L: Importance 0.1817\n",
      "RMS_L: Importance 0.1361\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0457\n",
      "Spread_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0130\n",
      "Variance_L: Importance 0.1077\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0010\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0094\n",
      "Mean_H: Importance 0.0944\n",
      "Mean Deviation_H: Importance 0.0007\n",
      "RMS_H: Importance 0.1411\n",
      "Peak-to-Peak_H: Importance 0.0051\n",
      "Entropy_H: Importance 0.0147\n",
      "Mean_Freq_H: Importance 0.0505\n",
      "Variance_H: Importance 0.0836\n",
      "Max_L: Importance 0.0092\n",
      "Min_L: Importance 0.0402\n",
      "Mean_L: Importance 0.1728\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.0123\n",
      "RMS_L: Importance 0.1333\n",
      "Kurtosis_L: Importance 0.0020\n",
      "Entropy_L: Importance 0.0449\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0771\n",
      "Min_H: Importance 0.0103\n",
      "Mean_H: Importance 0.1020\n",
      "Mean Deviation_H: Importance 0.0307\n",
      "RMS_H: Importance 0.1455\n",
      "Skewness_H: Importance 0.0126\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0105\n",
      "Variance_H: Importance 0.0713\n",
      "Max_L: Importance 0.0098\n",
      "Min_L: Importance 0.0197\n",
      "Mean_L: Importance 0.1949\n",
      "Mean Deviation_L: Importance 0.0219\n",
      "RMS_L: Importance 0.1386\n",
      "Kurtosis_L: Importance 0.0161\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Skewness_Freq_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0024\n",
      "Variance_L: Importance 0.1334\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0005\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0807\n",
      "Mean Deviation_H: Importance 0.0430\n",
      "RMS_H: Importance 0.1256\n",
      "Entropy_H: Importance 0.0165\n",
      "Mean_Freq_H: Importance 0.0323\n",
      "Kurtosis_Freq_H: Importance 0.0108\n",
      "Variance_H: Importance 0.0206\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1805\n",
      "Std_L: Importance 0.0005\n",
      "Mean Deviation_L: Importance 0.1077\n",
      "RMS_L: Importance 0.1231\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0189\n",
      "Skewness_Freq_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0155\n",
      "Variance_L: Importance 0.0836\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0008\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1042\n",
      "Entropy_H: Importance 0.0600\n",
      "Mean_Freq_H: Importance 0.0186\n",
      "Irregularity_H: Importance 0.0116\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0858\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0208\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1007\n",
      "Kurtosis_L: Importance 0.0031\n",
      "Centroid_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0014\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0824\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0200\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0600\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Irregularity_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0675\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 1**\n",
    "To conduct experiments, **ADC2** (data acquisition device) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on Probe 1**. The model is then tested on **Probe 2** and **Probe 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 1** data and Testing on **Probe 2** data\n",
    "To test Probe 1 with changing probe to Probe 2 and Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1456\n",
      "Min_H: Importance 0.0167\n",
      "Mean_H: Importance 0.0920\n",
      "Mean Deviation_H: Importance 0.0084\n",
      "RMS_H: Importance 0.1570\n",
      "Skewness_H: Importance 0.0075\n",
      "Kurtosis_H: Importance 0.0072\n",
      "Entropy_H: Importance 0.1037\n",
      "Mean_Freq_H: Importance 0.0742\n",
      "Variance_H: Importance 0.0309\n",
      "Max_L: Importance 0.0124\n",
      "Min_L: Importance 0.0557\n",
      "Mean_L: Importance 0.0467\n",
      "RMS_L: Importance 0.0719\n",
      "Skewness_L: Importance 0.0199\n",
      "Kurtosis_L: Importance 0.0082\n",
      "Entropy_L: Importance 0.0349\n",
      "Mean_Freq_L: Importance 0.0153\n",
      "Variance_L: Importance 0.0319\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9487\n",
      "Balanced Accuracy: 0.9487\n",
      "MCC: 0.9017\n",
      "Log Loss: 0.1354\n",
      "F1 Score: 0.9511\n",
      "Recall: 0.9975\n",
      "Precision: 0.9088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.90      0.95      9998\n",
      "         CH2       0.91      1.00      0.95      9998\n",
      "\n",
      "    accuracy                           0.95     19996\n",
      "   macro avg       0.95      0.95      0.95     19996\n",
      "weighted avg       0.95      0.95      0.95     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8997 1001]\n",
      " [  25 9973]]\n",
      "False Positive Rate (FPR): 0.1001\n",
      "False Negative Rate (FNR): 0.0025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0831\n",
      "Min_H: Importance 0.0256\n",
      "Mean_H: Importance 0.1231\n",
      "Mean Deviation_H: Importance 0.0052\n",
      "RMS_H: Importance 0.2181\n",
      "Skewness_H: Importance 0.0008\n",
      "Entropy_H: Importance 0.0228\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0733\n",
      "Variance_H: Importance 0.0369\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.1224\n",
      "Std_L: Importance 0.0017\n",
      "RMS_L: Importance 0.1415\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Entropy_L: Importance 0.0361\n",
      "Spread_L: Importance 0.0008\n",
      "Mean_Freq_L: Importance 0.0120\n",
      "Variance_L: Importance 0.0648\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9897\n",
      "Balanced Accuracy: 0.9897\n",
      "MCC: 0.9796\n",
      "Log Loss: 0.0200\n",
      "F1 Score: 0.9896\n",
      "Recall: 0.9794\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      1.00      0.99      4998\n",
      "         CH2       1.00      0.98      0.99      4998\n",
      "\n",
      "    accuracy                           0.99      9996\n",
      "   macro avg       0.99      0.99      0.99      9996\n",
      "weighted avg       0.99      0.99      0.99      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [ 103 4895]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0189\n",
      "Min_H: Importance 0.0060\n",
      "Mean_H: Importance 0.0989\n",
      "Mean Deviation_H: Importance 0.0030\n",
      "RMS_H: Importance 0.1652\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0099\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Variance_H: Importance 0.0314\n",
      "Max_L: Importance 0.0052\n",
      "Min_L: Importance 0.1486\n",
      "Mean_L: Importance 0.1112\n",
      "RMS_L: Importance 0.2474\n",
      "Peak-to-Peak_L: Importance 0.0012\n",
      "Entropy_L: Importance 0.0229\n",
      "Mean_Freq_L: Importance 0.0088\n",
      "Irregularity_L: Importance 0.0010\n",
      "Variance_L: Importance 0.0910\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0223\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3331\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0651\n",
      "Min_H: Importance 0.0282\n",
      "Mean_H: Importance 0.0975\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1401\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0282\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0136\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1900\n",
      "RMS_L: Importance 0.1277\n",
      "Skewness_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0345\n",
      "Mean_Freq_L: Importance 0.0111\n",
      "Variance_L: Importance 0.1015\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0018\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0565\n",
      "Min_H: Importance 0.0079\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0270\n",
      "Mean Deviation_H: Importance 0.0008\n",
      "RMS_H: Importance 0.1352\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0402\n",
      "Mean_Freq_H: Importance 0.0817\n",
      "Variance_H: Importance 0.0861\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0007\n",
      "Mean_L: Importance 0.1734\n",
      "Mean Deviation_L: Importance 0.0106\n",
      "RMS_L: Importance 0.1227\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0432\n",
      "Spread_L: Importance 0.0002\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0034\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0398\n",
      "Mean Deviation_H: Importance 0.0428\n",
      "RMS_H: Importance 0.0640\n",
      "Peak-to-Peak_H: Importance 0.0010\n",
      "Entropy_H: Importance 0.1402\n",
      "Spread_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.1013\n",
      "Kurtosis_Freq_H: Importance 0.0125\n",
      "Variance_H: Importance 0.0800\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1006\n",
      "RMS_L: Importance 0.0802\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0379\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0604\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1017\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.0831\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0169\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0046\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0002\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0142\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1212\n",
      "Entropy_H: Importance 0.0399\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0566\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0217\n",
      "Min_L: Importance 0.0252\n",
      "Mean_L: Importance 0.1820\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.1047\n",
      "RMS_L: Importance 0.1006\n",
      "Entropy_L: Importance 0.0185\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0077\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0615\n",
      "Peak-to-Peak_H: Importance 0.0200\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Irregularity_H: Importance 0.0185\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0400\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0111\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1022\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0008\n",
      "Mean Deviation_H: Importance 0.0001\n",
      "RMS_H: Importance 0.1231\n",
      "Entropy_H: Importance 0.0188\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0021\n",
      "Irregularity_H: Importance 0.0344\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0205\n",
      "Mean_L: Importance 0.1809\n",
      "Std_L: Importance 0.0606\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0196\n",
      "Mean_Freq_L: Importance 0.0171\n",
      "Kurtosis_Freq_L: Importance 0.0170\n",
      "Variance_L: Importance 0.0808\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0394\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       662\n",
      "   macro avg       1.00      1.00      1.00       662\n",
      "weighted avg       1.00      1.00      1.00       662\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 1** data and Testing on **Probe 3** data\n",
    "To test Probe 1 with changing probe to Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1456\n",
      "Min_H: Importance 0.0167\n",
      "Mean_H: Importance 0.0920\n",
      "Mean Deviation_H: Importance 0.0084\n",
      "RMS_H: Importance 0.1570\n",
      "Skewness_H: Importance 0.0075\n",
      "Kurtosis_H: Importance 0.0072\n",
      "Entropy_H: Importance 0.1037\n",
      "Mean_Freq_H: Importance 0.0742\n",
      "Variance_H: Importance 0.0309\n",
      "Max_L: Importance 0.0124\n",
      "Min_L: Importance 0.0557\n",
      "Mean_L: Importance 0.0467\n",
      "RMS_L: Importance 0.0719\n",
      "Skewness_L: Importance 0.0199\n",
      "Kurtosis_L: Importance 0.0082\n",
      "Entropy_L: Importance 0.0349\n",
      "Mean_Freq_L: Importance 0.0153\n",
      "Variance_L: Importance 0.0319\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9954\n",
      "Balanced Accuracy: 0.9954\n",
      "MCC: 0.9908\n",
      "Log Loss: 0.0212\n",
      "F1 Score: 0.9954\n",
      "Recall: 0.9935\n",
      "Precision: 0.9973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      9998\n",
      "         CH2       1.00      0.99      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9971   27]\n",
      " [  65 9933]]\n",
      "False Positive Rate (FPR): 0.0027\n",
      "False Negative Rate (FNR): 0.0065\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0831\n",
      "Min_H: Importance 0.0256\n",
      "Mean_H: Importance 0.1231\n",
      "Mean Deviation_H: Importance 0.0052\n",
      "RMS_H: Importance 0.2181\n",
      "Skewness_H: Importance 0.0008\n",
      "Entropy_H: Importance 0.0228\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0733\n",
      "Variance_H: Importance 0.0369\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.1224\n",
      "Std_L: Importance 0.0017\n",
      "RMS_L: Importance 0.1415\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Entropy_L: Importance 0.0361\n",
      "Spread_L: Importance 0.0008\n",
      "Mean_Freq_L: Importance 0.0120\n",
      "Variance_L: Importance 0.0648\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0189\n",
      "Min_H: Importance 0.0060\n",
      "Mean_H: Importance 0.0989\n",
      "Mean Deviation_H: Importance 0.0030\n",
      "RMS_H: Importance 0.1652\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0099\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Variance_H: Importance 0.0314\n",
      "Max_L: Importance 0.0052\n",
      "Min_L: Importance 0.1486\n",
      "Mean_L: Importance 0.1112\n",
      "RMS_L: Importance 0.2474\n",
      "Peak-to-Peak_L: Importance 0.0012\n",
      "Entropy_L: Importance 0.0229\n",
      "Mean_Freq_L: Importance 0.0088\n",
      "Irregularity_L: Importance 0.0010\n",
      "Variance_L: Importance 0.0910\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9868\n",
      "Balanced Accuracy: 0.9868\n",
      "MCC: 0.9739\n",
      "Log Loss: 0.0370\n",
      "F1 Score: 0.9870\n",
      "Recall: 1.0000\n",
      "Precision: 0.9743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.97      0.99      3331\n",
      "         CH2       0.97      1.00      0.99      3332\n",
      "\n",
      "    accuracy                           0.99      6663\n",
      "   macro avg       0.99      0.99      0.99      6663\n",
      "weighted avg       0.99      0.99      0.99      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3243   88]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0264\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0651\n",
      "Min_H: Importance 0.0282\n",
      "Mean_H: Importance 0.0975\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1401\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0282\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0136\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1900\n",
      "RMS_L: Importance 0.1277\n",
      "Skewness_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0345\n",
      "Mean_Freq_L: Importance 0.0111\n",
      "Variance_L: Importance 0.1015\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0565\n",
      "Min_H: Importance 0.0079\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0270\n",
      "Mean Deviation_H: Importance 0.0008\n",
      "RMS_H: Importance 0.1352\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0402\n",
      "Mean_Freq_H: Importance 0.0817\n",
      "Variance_H: Importance 0.0861\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0007\n",
      "Mean_L: Importance 0.1734\n",
      "Mean Deviation_L: Importance 0.0106\n",
      "RMS_L: Importance 0.1227\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0432\n",
      "Spread_L: Importance 0.0002\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0398\n",
      "Mean Deviation_H: Importance 0.0428\n",
      "RMS_H: Importance 0.0640\n",
      "Peak-to-Peak_H: Importance 0.0010\n",
      "Entropy_H: Importance 0.1402\n",
      "Spread_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.1013\n",
      "Kurtosis_Freq_H: Importance 0.0125\n",
      "Variance_H: Importance 0.0800\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1006\n",
      "RMS_L: Importance 0.0802\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0379\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0604\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0256\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.0831\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0169\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0692\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0002\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0142\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1212\n",
      "Entropy_H: Importance 0.0399\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0566\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0217\n",
      "Min_L: Importance 0.0252\n",
      "Mean_L: Importance 0.1820\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.1047\n",
      "RMS_L: Importance 0.1006\n",
      "Entropy_L: Importance 0.0185\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0112\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0615\n",
      "Peak-to-Peak_H: Importance 0.0200\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Irregularity_H: Importance 0.0185\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0400\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0416\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1022\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0008\n",
      "Mean Deviation_H: Importance 0.0001\n",
      "RMS_H: Importance 0.1231\n",
      "Entropy_H: Importance 0.0188\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0021\n",
      "Irregularity_H: Importance 0.0344\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0205\n",
      "Mean_L: Importance 0.1809\n",
      "Std_L: Importance 0.0606\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0196\n",
      "Mean_Freq_L: Importance 0.0171\n",
      "Kurtosis_Freq_L: Importance 0.0170\n",
      "Variance_L: Importance 0.0808\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0039\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 2**\n",
    "To conduct experiments, **ADC2** (data acquisition device) is used consistently throughout. **Probe 2** is kept constant, and the machine learning model is **always trained on Probe 2**. The model is then tested on **Probe 3** and **Probe 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 2** data and Testing on **Probe 3**\n",
    "To test Probe 2 with changing probe to Probes 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0768\n",
      "RMS_H: Importance 0.1119\n",
      "Skewness_H: Importance 0.0067\n",
      "Entropy_H: Importance 0.0116\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Variance_H: Importance 0.0360\n",
      "Max_L: Importance 0.0059\n",
      "Min_L: Importance 0.1096\n",
      "Mean_L: Importance 0.1263\n",
      "Std_L: Importance 0.0059\n",
      "RMS_L: Importance 0.2135\n",
      "Skewness_L: Importance 0.0179\n",
      "Peak-to-Peak_L: Importance 0.0129\n",
      "Entropy_L: Importance 0.0403\n",
      "Spread_L: Importance 0.0075\n",
      "Mean_Freq_L: Importance 0.0380\n",
      "Irregularity_L: Importance 0.0114\n",
      "Variance_L: Importance 0.0734\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9256\n",
      "Balanced Accuracy: 0.9256\n",
      "MCC: 0.8528\n",
      "Log Loss: 0.1925\n",
      "F1 Score: 0.9278\n",
      "Recall: 0.9562\n",
      "Precision: 0.9010\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.95      0.89      0.92      9998\n",
      "         CH2       0.90      0.96      0.93      9998\n",
      "\n",
      "    accuracy                           0.93     19996\n",
      "   macro avg       0.93      0.93      0.93     19996\n",
      "weighted avg       0.93      0.93      0.93     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8948 1050]\n",
      " [ 438 9560]]\n",
      "False Positive Rate (FPR): 0.1050\n",
      "False Negative Rate (FNR): 0.0438\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0350\n",
      "Mean_H: Importance 0.0743\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1520\n",
      "Peak-to-Peak_H: Importance 0.0048\n",
      "Centroid_H: Importance 0.0013\n",
      "Entropy_H: Importance 0.0346\n",
      "Spread_H: Importance 0.0016\n",
      "Mean_Freq_H: Importance 0.0758\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0140\n",
      "Min_L: Importance 0.0245\n",
      "Mean_L: Importance 0.1090\n",
      "RMS_L: Importance 0.2406\n",
      "Entropy_L: Importance 0.0278\n",
      "Skewness_Freq_L: Importance 0.0013\n",
      "Mean_Freq_L: Importance 0.0078\n",
      "Variance_L: Importance 0.0092\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1627\n",
      "Min_H: Importance 0.0243\n",
      "Mean_H: Importance 0.0740\n",
      "Mean Deviation_H: Importance 0.0027\n",
      "RMS_H: Importance 0.1712\n",
      "Entropy_H: Importance 0.0154\n",
      "Spread_H: Importance 0.0019\n",
      "Skewness_Freq_H: Importance 0.0019\n",
      "Mean_Freq_H: Importance 0.0451\n",
      "Variance_H: Importance 0.0392\n",
      "Min_L: Importance 0.0149\n",
      "Mean_L: Importance 0.1459\n",
      "RMS_L: Importance 0.2334\n",
      "Skewness_L: Importance 0.0181\n",
      "Kurtosis_L: Importance 0.0035\n",
      "Peak-to-Peak_L: Importance 0.0027\n",
      "Entropy_L: Importance 0.0114\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Variance_L: Importance 0.0216\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9935\n",
      "Balanced Accuracy: 0.9935\n",
      "MCC: 0.9872\n",
      "Log Loss: 0.0370\n",
      "F1 Score: 0.9936\n",
      "Recall: 1.0000\n",
      "Precision: 0.9873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.99      0.99      3331\n",
      "         CH2       0.99      1.00      0.99      3332\n",
      "\n",
      "    accuracy                           0.99      6663\n",
      "   macro avg       0.99      0.99      0.99      6663\n",
      "weighted avg       0.99      0.99      0.99      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3288   43]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0129\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0663\n",
      "Min_H: Importance 0.0261\n",
      "Mean_H: Importance 0.0912\n",
      "Std_H: Importance 0.0218\n",
      "Mean Deviation_H: Importance 0.0151\n",
      "RMS_H: Importance 0.1408\n",
      "Centroid_H: Importance 0.0027\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0598\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1973\n",
      "RMS_L: Importance 0.1267\n",
      "Entropy_L: Importance 0.0290\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.1266\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0011\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0615\n",
      "Min_H: Importance 0.0399\n",
      "Mean_H: Importance 0.0919\n",
      "Mean Deviation_H: Importance 0.0249\n",
      "RMS_H: Importance 0.1047\n",
      "Peak-to-Peak_H: Importance 0.0002\n",
      "Centroid_H: Importance 0.0003\n",
      "Entropy_H: Importance 0.0158\n",
      "Mean_Freq_H: Importance 0.0409\n",
      "Variance_H: Importance 0.0604\n",
      "Max_L: Importance 0.0273\n",
      "Min_L: Importance 0.0908\n",
      "Mean_L: Importance 0.1409\n",
      "Mean Deviation_L: Importance 0.0036\n",
      "RMS_L: Importance 0.1265\n",
      "Kurtosis_L: Importance 0.0036\n",
      "Entropy_L: Importance 0.0326\n",
      "Mean_Freq_L: Importance 0.0139\n",
      "Variance_L: Importance 0.1202\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0111\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1176\n",
      "Min_H: Importance 0.0039\n",
      "Mean_H: Importance 0.1050\n",
      "Mean Deviation_H: Importance 0.0073\n",
      "RMS_H: Importance 0.1557\n",
      "Centroid_H: Importance 0.0071\n",
      "Entropy_H: Importance 0.0103\n",
      "Spread_H: Importance 0.0064\n",
      "Mean_Freq_H: Importance 0.0152\n",
      "Variance_H: Importance 0.0967\n",
      "Max_L: Importance 0.0013\n",
      "Min_L: Importance 0.0797\n",
      "Mean_L: Importance 0.1486\n",
      "Mean Deviation_L: Importance 0.0202\n",
      "RMS_L: Importance 0.1366\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0088\n",
      "Mean_Freq_L: Importance 0.0069\n",
      "Variance_L: Importance 0.0689\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0223\n",
      "RMS_H: Importance 0.0814\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9609\n",
      "Balanced Accuracy: 0.9609\n",
      "MCC: 0.9246\n",
      "Log Loss: 0.1764\n",
      "F1 Score: 0.9624\n",
      "Recall: 1.0000\n",
      "Precision: 0.9275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.92      0.96       665\n",
      "         CH2       0.93      1.00      0.96       665\n",
      "\n",
      "    accuracy                           0.96      1330\n",
      "   macro avg       0.96      0.96      0.96      1330\n",
      "weighted avg       0.96      0.96      0.96      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[613  52]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0782\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0826\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0834\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0399\n",
      "Kurtosis_Freq_H: Importance 0.0166\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0199\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0177\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Min_H: Importance 0.0189\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0811\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.0400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0200\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9899\n",
      "Balanced Accuracy: 0.9899\n",
      "MCC: 0.9801\n",
      "Log Loss: 0.1417\n",
      "F1 Score: 0.9900\n",
      "Recall: 1.0000\n",
      "Precision: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.98      0.99       398\n",
      "         CH2       0.98      1.00      0.99       398\n",
      "\n",
      "    accuracy                           0.99       796\n",
      "   macro avg       0.99      0.99      0.99       796\n",
      "weighted avg       0.99      0.99      0.99       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[390   8]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0201\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0838\n",
      "Std_H: Importance 0.0200\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0400\n",
      "Variance_H: Importance 0.0229\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0133\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0406\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 2** data and Testing on **Probe 1**\n",
    "To test Probe 2 with changing probe to Probes 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0768\n",
      "RMS_H: Importance 0.1119\n",
      "Skewness_H: Importance 0.0067\n",
      "Entropy_H: Importance 0.0116\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Variance_H: Importance 0.0360\n",
      "Max_L: Importance 0.0059\n",
      "Min_L: Importance 0.1096\n",
      "Mean_L: Importance 0.1263\n",
      "Std_L: Importance 0.0059\n",
      "RMS_L: Importance 0.2135\n",
      "Skewness_L: Importance 0.0179\n",
      "Peak-to-Peak_L: Importance 0.0129\n",
      "Entropy_L: Importance 0.0403\n",
      "Spread_L: Importance 0.0075\n",
      "Mean_Freq_L: Importance 0.0380\n",
      "Irregularity_L: Importance 0.0114\n",
      "Variance_L: Importance 0.0734\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.8808\n",
      "Balanced Accuracy: 0.8808\n",
      "MCC: 0.7684\n",
      "Log Loss: 0.3835\n",
      "F1 Score: 0.8882\n",
      "Recall: 0.9475\n",
      "Precision: 0.8360\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.94      0.81      0.87      9998\n",
      "         CH2       0.84      0.95      0.89      9998\n",
      "\n",
      "    accuracy                           0.88     19996\n",
      "   macro avg       0.89      0.88      0.88     19996\n",
      "weighted avg       0.89      0.88      0.88     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8139 1859]\n",
      " [ 525 9473]]\n",
      "False Positive Rate (FPR): 0.1859\n",
      "False Negative Rate (FNR): 0.0525\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0350\n",
      "Mean_H: Importance 0.0743\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1520\n",
      "Peak-to-Peak_H: Importance 0.0048\n",
      "Centroid_H: Importance 0.0013\n",
      "Entropy_H: Importance 0.0346\n",
      "Spread_H: Importance 0.0016\n",
      "Mean_Freq_H: Importance 0.0758\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0140\n",
      "Min_L: Importance 0.0245\n",
      "Mean_L: Importance 0.1090\n",
      "RMS_L: Importance 0.2406\n",
      "Entropy_L: Importance 0.0278\n",
      "Skewness_Freq_L: Importance 0.0013\n",
      "Mean_Freq_L: Importance 0.0078\n",
      "Variance_L: Importance 0.0092\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9999\n",
      "Balanced Accuracy: 0.9999\n",
      "MCC: 0.9998\n",
      "Log Loss: 0.0035\n",
      "F1 Score: 0.9999\n",
      "Recall: 1.0000\n",
      "Precision: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4997    1]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0002\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1627\n",
      "Min_H: Importance 0.0243\n",
      "Mean_H: Importance 0.0740\n",
      "Mean Deviation_H: Importance 0.0027\n",
      "RMS_H: Importance 0.1712\n",
      "Entropy_H: Importance 0.0154\n",
      "Spread_H: Importance 0.0019\n",
      "Skewness_Freq_H: Importance 0.0019\n",
      "Mean_Freq_H: Importance 0.0451\n",
      "Variance_H: Importance 0.0392\n",
      "Min_L: Importance 0.0149\n",
      "Mean_L: Importance 0.1459\n",
      "RMS_L: Importance 0.2334\n",
      "Skewness_L: Importance 0.0181\n",
      "Kurtosis_L: Importance 0.0035\n",
      "Peak-to-Peak_L: Importance 0.0027\n",
      "Entropy_L: Importance 0.0114\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Variance_L: Importance 0.0216\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9920\n",
      "Balanced Accuracy: 0.9920\n",
      "MCC: 0.9842\n",
      "Log Loss: 0.0443\n",
      "F1 Score: 0.9920\n",
      "Recall: 0.9847\n",
      "Precision: 0.9994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      1.00      0.99      3331\n",
      "         CH2       1.00      0.98      0.99      3332\n",
      "\n",
      "    accuracy                           0.99      6663\n",
      "   macro avg       0.99      0.99      0.99      6663\n",
      "weighted avg       0.99      0.99      0.99      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3329    2]\n",
      " [  51 3281]]\n",
      "False Positive Rate (FPR): 0.0006\n",
      "False Negative Rate (FNR): 0.0153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0663\n",
      "Min_H: Importance 0.0261\n",
      "Mean_H: Importance 0.0912\n",
      "Std_H: Importance 0.0218\n",
      "Mean Deviation_H: Importance 0.0151\n",
      "RMS_H: Importance 0.1408\n",
      "Centroid_H: Importance 0.0027\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0598\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1973\n",
      "RMS_L: Importance 0.1267\n",
      "Entropy_L: Importance 0.0290\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.1266\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0013\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0615\n",
      "Min_H: Importance 0.0399\n",
      "Mean_H: Importance 0.0919\n",
      "Mean Deviation_H: Importance 0.0249\n",
      "RMS_H: Importance 0.1047\n",
      "Peak-to-Peak_H: Importance 0.0002\n",
      "Centroid_H: Importance 0.0003\n",
      "Entropy_H: Importance 0.0158\n",
      "Mean_Freq_H: Importance 0.0409\n",
      "Variance_H: Importance 0.0604\n",
      "Max_L: Importance 0.0273\n",
      "Min_L: Importance 0.0908\n",
      "Mean_L: Importance 0.1409\n",
      "Mean Deviation_L: Importance 0.0036\n",
      "RMS_L: Importance 0.1265\n",
      "Kurtosis_L: Importance 0.0036\n",
      "Entropy_L: Importance 0.0326\n",
      "Mean_Freq_L: Importance 0.0139\n",
      "Variance_L: Importance 0.1202\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9997\n",
      "Balanced Accuracy: 0.9997\n",
      "MCC: 0.9995\n",
      "Log Loss: 0.0119\n",
      "F1 Score: 0.9997\n",
      "Recall: 0.9995\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   1 1997]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1176\n",
      "Min_H: Importance 0.0039\n",
      "Mean_H: Importance 0.1050\n",
      "Mean Deviation_H: Importance 0.0073\n",
      "RMS_H: Importance 0.1557\n",
      "Centroid_H: Importance 0.0071\n",
      "Entropy_H: Importance 0.0103\n",
      "Spread_H: Importance 0.0064\n",
      "Mean_Freq_H: Importance 0.0152\n",
      "Variance_H: Importance 0.0967\n",
      "Max_L: Importance 0.0013\n",
      "Min_L: Importance 0.0797\n",
      "Mean_L: Importance 0.1486\n",
      "Mean Deviation_L: Importance 0.0202\n",
      "RMS_L: Importance 0.1366\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0088\n",
      "Mean_Freq_L: Importance 0.0069\n",
      "Variance_L: Importance 0.0689\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0223\n",
      "RMS_H: Importance 0.0814\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0306\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0826\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0834\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0399\n",
      "Kurtosis_Freq_H: Importance 0.0166\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0199\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1023\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Min_H: Importance 0.0189\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0811\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.0400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0200\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0113\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0838\n",
      "Std_H: Importance 0.0200\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0400\n",
      "Variance_H: Importance 0.0229\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0133\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1438\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 3**\n",
    "To conduct experiments, **ADC2** (data acquisition device) is used consistently throughout. **Probe 3** is kept constant, and the machine learning model is **always trained on Probe 3**. The model is then tested on **Probe 3** and **Probe 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 3** data and Testing on **Probe 1**\n",
    "To test Probe 3 with changing probe to Probes 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1694\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0882\n",
      "Mean Deviation_H: Importance 0.0057\n",
      "RMS_H: Importance 0.1780\n",
      "Skewness_H: Importance 0.0048\n",
      "Peak-to-Peak_H: Importance 0.0044\n",
      "Entropy_H: Importance 0.1111\n",
      "Spread_H: Importance 0.0043\n",
      "Mean_Freq_H: Importance 0.0611\n",
      "Variance_H: Importance 0.0211\n",
      "Max_L: Importance 0.0040\n",
      "Min_L: Importance 0.0971\n",
      "Mean_L: Importance 0.0526\n",
      "RMS_L: Importance 0.0720\n",
      "Skewness_L: Importance 0.0055\n",
      "Entropy_L: Importance 0.0421\n",
      "Mean_Freq_L: Importance 0.0188\n",
      "Variance_L: Importance 0.0180\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9697\n",
      "Balanced Accuracy: 0.9697\n",
      "MCC: 0.9402\n",
      "Log Loss: 0.1001\n",
      "F1 Score: 0.9703\n",
      "Recall: 0.9898\n",
      "Precision: 0.9516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.95      0.97      9998\n",
      "         CH2       0.95      0.99      0.97      9998\n",
      "\n",
      "    accuracy                           0.97     19996\n",
      "   macro avg       0.97      0.97      0.97     19996\n",
      "weighted avg       0.97      0.97      0.97     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9495  503]\n",
      " [ 102 9896]]\n",
      "False Positive Rate (FPR): 0.0503\n",
      "False Negative Rate (FNR): 0.0102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0596\n",
      "Min_H: Importance 0.0351\n",
      "Mean_H: Importance 0.0945\n",
      "Mean Deviation_H: Importance 0.0063\n",
      "RMS_H: Importance 0.1270\n",
      "Centroid_H: Importance 0.0036\n",
      "Entropy_H: Importance 0.0172\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0650\n",
      "Variance_H: Importance 0.0612\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0258\n",
      "Mean_L: Importance 0.2093\n",
      "RMS_L: Importance 0.1217\n",
      "Skewness_L: Importance 0.0000\n",
      "Kurtosis_L: Importance 0.0065\n",
      "Entropy_L: Importance 0.0241\n",
      "Mean_Freq_L: Importance 0.0091\n",
      "Variance_L: Importance 0.1331\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9724\n",
      "Balanced Accuracy: 0.9724\n",
      "MCC: 0.9462\n",
      "Log Loss: 0.0407\n",
      "F1 Score: 0.9716\n",
      "Recall: 0.9448\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.95      1.00      0.97      4998\n",
      "         CH2       1.00      0.94      0.97      4998\n",
      "\n",
      "    accuracy                           0.97      9996\n",
      "   macro avg       0.97      0.97      0.97      9996\n",
      "weighted avg       0.97      0.97      0.97      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [ 276 4722]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0552\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0055\n",
      "Min_H: Importance 0.0148\n",
      "Mean_H: Importance 0.1371\n",
      "Mean Deviation_H: Importance 0.0022\n",
      "RMS_H: Importance 0.2265\n",
      "Skewness_H: Importance 0.0004\n",
      "Centroid_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.0114\n",
      "Spread_H: Importance 0.0003\n",
      "Mean_Freq_H: Importance 0.0249\n",
      "Variance_H: Importance 0.0565\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0935\n",
      "Mean_L: Importance 0.1321\n",
      "RMS_L: Importance 0.1545\n",
      "Skewness_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0235\n",
      "Mean_Freq_L: Importance 0.0073\n",
      "Variance_L: Importance 0.1044\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9668\n",
      "Balanced Accuracy: 0.9668\n",
      "MCC: 0.9357\n",
      "Log Loss: 0.0699\n",
      "F1 Score: 0.9657\n",
      "Recall: 0.9337\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.94      1.00      0.97      3331\n",
      "         CH2       1.00      0.93      0.97      3332\n",
      "\n",
      "    accuracy                           0.97      6663\n",
      "   macro avg       0.97      0.97      0.97      6663\n",
      "weighted avg       0.97      0.97      0.97      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [ 221 3111]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0663\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0648\n",
      "Min_H: Importance 0.0288\n",
      "Mean_H: Importance 0.0963\n",
      "Mean Deviation_H: Importance 0.0137\n",
      "RMS_H: Importance 0.1324\n",
      "Skewness_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0138\n",
      "Mean_Freq_H: Importance 0.0559\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0003\n",
      "Min_L: Importance 0.0224\n",
      "Mean_L: Importance 0.2112\n",
      "RMS_L: Importance 0.1272\n",
      "Skewness_L: Importance 0.0005\n",
      "Kurtosis_L: Importance 0.0004\n",
      "Centroid_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0292\n",
      "Mean_Freq_L: Importance 0.0115\n",
      "Variance_L: Importance 0.1301\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0599\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0892\n",
      "Std_H: Importance 0.0296\n",
      "Mean Deviation_H: Importance 0.0339\n",
      "RMS_H: Importance 0.1279\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0245\n",
      "Mean_Freq_H: Importance 0.0806\n",
      "Irregularity_H: Importance 0.0090\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0022\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1905\n",
      "Mean Deviation_L: Importance 0.0232\n",
      "RMS_L: Importance 0.1217\n",
      "Entropy_L: Importance 0.0160\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.1250\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0006\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1024\n",
      "Mean_H: Importance 0.0815\n",
      "Std_H: Importance 0.0167\n",
      "Mean Deviation_H: Importance 0.0179\n",
      "RMS_H: Importance 0.1285\n",
      "Entropy_H: Importance 0.0198\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Kurtosis_Freq_H: Importance 0.0084\n",
      "Variance_H: Importance 0.0604\n",
      "Min_L: Importance 0.0837\n",
      "Mean_L: Importance 0.1815\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1216\n",
      "Skewness_L: Importance 0.0002\n",
      "Kurtosis_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0373\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0819\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0006\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1038\n",
      "Min_H: Importance 0.0006\n",
      "Mean_H: Importance 0.1025\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0415\n",
      "RMS_H: Importance 0.1069\n",
      "Entropy_H: Importance 0.0120\n",
      "Spread_H: Importance 0.0049\n",
      "Mean_Freq_H: Importance 0.0215\n",
      "Variance_H: Importance 0.0687\n",
      "Min_L: Importance 0.0630\n",
      "Mean_L: Importance 0.1443\n",
      "Mean Deviation_L: Importance 0.0921\n",
      "RMS_L: Importance 0.1328\n",
      "Skewness_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0111\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Irregularity_L: Importance 0.0063\n",
      "Variance_L: Importance 0.0690\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1004\n",
      "Mean_H: Importance 0.0807\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0641\n",
      "RMS_H: Importance 0.1223\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0390\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0392\n",
      "Mean_L: Importance 0.1410\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0808\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0100\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1002\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0206\n",
      "RMS_H: Importance 0.1009\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0185\n",
      "Irregularity_H: Importance 0.0001\n",
      "Min_L: Importance 0.0190\n",
      "Mean_L: Importance 0.1208\n",
      "Std_L: Importance 0.1202\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0052\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0850\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.0856\n",
      "Skewness_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.1400\n",
      "Spread_H: Importance 0.0157\n",
      "Mean_Freq_H: Importance 0.0405\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0398\n",
      "Mean_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0196\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0265\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 3** data and Testing on **Probe 2**\n",
    "To test Probe 3 with changing probe to Probes 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1694\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0882\n",
      "Mean Deviation_H: Importance 0.0057\n",
      "RMS_H: Importance 0.1780\n",
      "Skewness_H: Importance 0.0048\n",
      "Peak-to-Peak_H: Importance 0.0044\n",
      "Entropy_H: Importance 0.1111\n",
      "Spread_H: Importance 0.0043\n",
      "Mean_Freq_H: Importance 0.0611\n",
      "Variance_H: Importance 0.0211\n",
      "Max_L: Importance 0.0040\n",
      "Min_L: Importance 0.0971\n",
      "Mean_L: Importance 0.0526\n",
      "RMS_L: Importance 0.0720\n",
      "Skewness_L: Importance 0.0055\n",
      "Entropy_L: Importance 0.0421\n",
      "Mean_Freq_L: Importance 0.0188\n",
      "Variance_L: Importance 0.0180\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9408\n",
      "Balanced Accuracy: 0.9408\n",
      "MCC: 0.8863\n",
      "Log Loss: 0.1304\n",
      "F1 Score: 0.9437\n",
      "Recall: 0.9924\n",
      "Precision: 0.8995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.89      0.94      9998\n",
      "         CH2       0.90      0.99      0.94      9998\n",
      "\n",
      "    accuracy                           0.94     19996\n",
      "   macro avg       0.95      0.94      0.94     19996\n",
      "weighted avg       0.95      0.94      0.94     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8890 1108]\n",
      " [  76 9922]]\n",
      "False Positive Rate (FPR): 0.1108\n",
      "False Negative Rate (FNR): 0.0076\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0596\n",
      "Min_H: Importance 0.0351\n",
      "Mean_H: Importance 0.0945\n",
      "Mean Deviation_H: Importance 0.0063\n",
      "RMS_H: Importance 0.1270\n",
      "Centroid_H: Importance 0.0036\n",
      "Entropy_H: Importance 0.0172\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0650\n",
      "Variance_H: Importance 0.0612\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0258\n",
      "Mean_L: Importance 0.2093\n",
      "RMS_L: Importance 0.1217\n",
      "Skewness_L: Importance 0.0000\n",
      "Kurtosis_L: Importance 0.0065\n",
      "Entropy_L: Importance 0.0241\n",
      "Mean_Freq_L: Importance 0.0091\n",
      "Variance_L: Importance 0.1331\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9902\n",
      "Balanced Accuracy: 0.9902\n",
      "MCC: 0.9806\n",
      "Log Loss: 0.0552\n",
      "F1 Score: 0.9901\n",
      "Recall: 0.9804\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      1.00      0.99      4998\n",
      "         CH2       1.00      0.98      0.99      4998\n",
      "\n",
      "    accuracy                           0.99      9996\n",
      "   macro avg       0.99      0.99      0.99      9996\n",
      "weighted avg       0.99      0.99      0.99      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [  98 4900]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0055\n",
      "Min_H: Importance 0.0148\n",
      "Mean_H: Importance 0.1371\n",
      "Mean Deviation_H: Importance 0.0022\n",
      "RMS_H: Importance 0.2265\n",
      "Skewness_H: Importance 0.0004\n",
      "Centroid_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.0114\n",
      "Spread_H: Importance 0.0003\n",
      "Mean_Freq_H: Importance 0.0249\n",
      "Variance_H: Importance 0.0565\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0935\n",
      "Mean_L: Importance 0.1321\n",
      "RMS_L: Importance 0.1545\n",
      "Skewness_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0235\n",
      "Mean_Freq_L: Importance 0.0073\n",
      "Variance_L: Importance 0.1044\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9986\n",
      "Balanced Accuracy: 0.9986\n",
      "MCC: 0.9973\n",
      "Log Loss: 0.0255\n",
      "F1 Score: 0.9986\n",
      "Recall: 0.9973\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3331\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [   9 3323]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0027\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0648\n",
      "Min_H: Importance 0.0288\n",
      "Mean_H: Importance 0.0963\n",
      "Mean Deviation_H: Importance 0.0137\n",
      "RMS_H: Importance 0.1324\n",
      "Skewness_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0138\n",
      "Mean_Freq_H: Importance 0.0559\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0003\n",
      "Min_L: Importance 0.0224\n",
      "Mean_L: Importance 0.2112\n",
      "RMS_L: Importance 0.1272\n",
      "Skewness_L: Importance 0.0005\n",
      "Kurtosis_L: Importance 0.0004\n",
      "Centroid_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0292\n",
      "Mean_Freq_L: Importance 0.0115\n",
      "Variance_L: Importance 0.1301\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0599\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0892\n",
      "Std_H: Importance 0.0296\n",
      "Mean Deviation_H: Importance 0.0339\n",
      "RMS_H: Importance 0.1279\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0245\n",
      "Mean_Freq_H: Importance 0.0806\n",
      "Irregularity_H: Importance 0.0090\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0022\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1905\n",
      "Mean Deviation_L: Importance 0.0232\n",
      "RMS_L: Importance 0.1217\n",
      "Entropy_L: Importance 0.0160\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.1250\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0063\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1024\n",
      "Mean_H: Importance 0.0815\n",
      "Std_H: Importance 0.0167\n",
      "Mean Deviation_H: Importance 0.0179\n",
      "RMS_H: Importance 0.1285\n",
      "Entropy_H: Importance 0.0198\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Kurtosis_Freq_H: Importance 0.0084\n",
      "Variance_H: Importance 0.0604\n",
      "Min_L: Importance 0.0837\n",
      "Mean_L: Importance 0.1815\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1216\n",
      "Skewness_L: Importance 0.0002\n",
      "Kurtosis_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0373\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0819\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0040\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1038\n",
      "Min_H: Importance 0.0006\n",
      "Mean_H: Importance 0.1025\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0415\n",
      "RMS_H: Importance 0.1069\n",
      "Entropy_H: Importance 0.0120\n",
      "Spread_H: Importance 0.0049\n",
      "Mean_Freq_H: Importance 0.0215\n",
      "Variance_H: Importance 0.0687\n",
      "Min_L: Importance 0.0630\n",
      "Mean_L: Importance 0.1443\n",
      "Mean Deviation_L: Importance 0.0921\n",
      "RMS_L: Importance 0.1328\n",
      "Skewness_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0111\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Irregularity_L: Importance 0.0063\n",
      "Variance_L: Importance 0.0690\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0157\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1004\n",
      "Mean_H: Importance 0.0807\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0641\n",
      "RMS_H: Importance 0.1223\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0390\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0392\n",
      "Mean_L: Importance 0.1410\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0808\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0049\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1002\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0206\n",
      "RMS_H: Importance 0.1009\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0185\n",
      "Irregularity_H: Importance 0.0001\n",
      "Min_L: Importance 0.0190\n",
      "Mean_L: Importance 0.1208\n",
      "Std_L: Importance 0.1202\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0610\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0850\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.0856\n",
      "Skewness_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.1400\n",
      "Spread_H: Importance 0.0157\n",
      "Mean_Freq_H: Importance 0.0405\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0398\n",
      "Mean_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0196\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0447\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       662\n",
      "   macro avg       1.00      1.00      1.00       662\n",
      "weighted avg       1.00      1.00      1.00       662\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 2\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 1**\n",
    "To conduct experiments, **ADC3** (data acquisition device) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on Probe 1**. The model is then tested on **Probe 2** and **Probe 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 1** data and Testing on **Probe 2** data\n",
    "To test Probe 1 with changing probe to Probe 2 and Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0487\n",
      "Min_H: Importance 0.0563\n",
      "Mean_H: Importance 0.0667\n",
      "Mean Deviation_H: Importance 0.0065\n",
      "RMS_H: Importance 0.1228\n",
      "Skewness_H: Importance 0.0020\n",
      "Peak-to-Peak_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.2617\n",
      "Spread_H: Importance 0.0036\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0383\n",
      "Max_L: Importance 0.0552\n",
      "Min_L: Importance 0.0363\n",
      "Mean_L: Importance 0.0386\n",
      "RMS_L: Importance 0.0186\n",
      "Peak-to-Peak_L: Importance 0.0031\n",
      "Entropy_L: Importance 0.0350\n",
      "Mean_Freq_L: Importance 0.0191\n",
      "Variance_L: Importance 0.0967\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9996\n",
      "Balanced Accuracy: 0.9996\n",
      "MCC: 0.9992\n",
      "Log Loss: 0.0041\n",
      "F1 Score: 0.9996\n",
      "Recall: 0.9993\n",
      "Precision: 0.9999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9997    1]\n",
      " [   7 9991]]\n",
      "False Positive Rate (FPR): 0.0001\n",
      "False Negative Rate (FNR): 0.0007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1472\n",
      "Min_H: Importance 0.0303\n",
      "Mean_H: Importance 0.1141\n",
      "Mean Deviation_H: Importance 0.0155\n",
      "RMS_H: Importance 0.1951\n",
      "Peak-to-Peak_H: Importance 0.0046\n",
      "Entropy_H: Importance 0.0420\n",
      "Mean_Freq_H: Importance 0.0746\n",
      "Variance_H: Importance 0.0437\n",
      "Max_L: Importance 0.0014\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0907\n",
      "Std_L: Importance 0.0006\n",
      "RMS_L: Importance 0.1313\n",
      "Peak-to-Peak_L: Importance 0.0010\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0009\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0184\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0019\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1463\n",
      "Min_H: Importance 0.0133\n",
      "Mean_H: Importance 0.1364\n",
      "Std_H: Importance 0.0115\n",
      "Mean Deviation_H: Importance 0.0126\n",
      "RMS_H: Importance 0.1499\n",
      "Entropy_H: Importance 0.0166\n",
      "Mean_Freq_H: Importance 0.0692\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0475\n",
      "Mean_L: Importance 0.1051\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1668\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Centroid_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0293\n",
      "Variance_L: Importance 0.0270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0008\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6664\n",
      "   macro avg       1.00      1.00      1.00      6664\n",
      "weighted avg       1.00      1.00      1.00      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1030\n",
      "Min_H: Importance 0.0296\n",
      "Mean_H: Importance 0.0901\n",
      "Std_H: Importance 0.0272\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1376\n",
      "Kurtosis_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0028\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1909\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1324\n",
      "Kurtosis_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0299\n",
      "Variance_L: Importance 0.0863\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0008\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1081\n",
      "Min_H: Importance 0.0196\n",
      "Mean_H: Importance 0.1027\n",
      "RMS_H: Importance 0.1425\n",
      "Peak-to-Peak_H: Importance 0.0076\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0288\n",
      "Mean_Freq_H: Importance 0.0501\n",
      "Variance_H: Importance 0.0835\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0441\n",
      "Mean_L: Importance 0.1598\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1222\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0277\n",
      "Spread_L: Importance 0.0002\n",
      "Mean_Freq_L: Importance 0.0098\n",
      "Variance_L: Importance 0.0884\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0027\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1108\n",
      "Min_H: Importance 0.0052\n",
      "Mean_H: Importance 0.0933\n",
      "Std_H: Importance 0.0222\n",
      "Mean Deviation_H: Importance 0.0288\n",
      "RMS_H: Importance 0.1419\n",
      "Skewness_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0160\n",
      "Spread_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0336\n",
      "Irregularity_H: Importance 0.0012\n",
      "Variance_H: Importance 0.0700\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0122\n",
      "Mean_L: Importance 0.1918\n",
      "Mean Deviation_L: Importance 0.0417\n",
      "RMS_L: Importance 0.1277\n",
      "Entropy_L: Importance 0.0091\n",
      "Variance_L: Importance 0.0905\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1001\n",
      "Mean_H: Importance 0.0833\n",
      "Std_H: Importance 0.0339\n",
      "Mean Deviation_H: Importance 0.0691\n",
      "RMS_H: Importance 0.1260\n",
      "Entropy_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0159\n",
      "Irregularity_H: Importance 0.0117\n",
      "Variance_H: Importance 0.0214\n",
      "Min_L: Importance 0.0386\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1241\n",
      "Entropy_L: Importance 0.0163\n",
      "Variance_L: Importance 0.0832\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0447\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0801\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0185\n",
      "Kurtosis_Freq_H: Importance 0.0109\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0881\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0199\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0144\n",
      "Variance_L: Importance 0.0833\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0805\n",
      "Mean_H: Importance 0.0828\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.1045\n",
      "Kurtosis_H: Importance 0.0001\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0187\n",
      "Kurtosis_Freq_H: Importance 0.0324\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0796\n",
      "Mean_L: Importance 0.1604\n",
      "Std_L: Importance 0.0202\n",
      "Mean Deviation_L: Importance 0.0802\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0198\n",
      "Mean_Freq_L: Importance 0.0381\n",
      "Variance_L: Importance 0.0602\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0039\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0801\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0599\n",
      "RMS_H: Importance 0.1244\n",
      "Peak-to-Peak_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.0181\n",
      "Kurtosis_Freq_H: Importance 0.0157\n",
      "Irregularity_H: Importance 0.0007\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0401\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0411\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1200\n",
      "Entropy_L: Importance 0.0200\n",
      "Mean_Freq_L: Importance 0.0199\n",
      "Irregularity_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 1** data and Testing on **Probe 3** data\n",
    "To test Probe 1 with changing probe to Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0487\n",
      "Min_H: Importance 0.0563\n",
      "Mean_H: Importance 0.0667\n",
      "Mean Deviation_H: Importance 0.0065\n",
      "RMS_H: Importance 0.1228\n",
      "Skewness_H: Importance 0.0020\n",
      "Peak-to-Peak_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.2617\n",
      "Spread_H: Importance 0.0036\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0383\n",
      "Max_L: Importance 0.0552\n",
      "Min_L: Importance 0.0363\n",
      "Mean_L: Importance 0.0386\n",
      "RMS_L: Importance 0.0186\n",
      "Peak-to-Peak_L: Importance 0.0031\n",
      "Entropy_L: Importance 0.0350\n",
      "Mean_Freq_L: Importance 0.0191\n",
      "Variance_L: Importance 0.0967\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0019\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9998    0]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1472\n",
      "Min_H: Importance 0.0303\n",
      "Mean_H: Importance 0.1141\n",
      "Mean Deviation_H: Importance 0.0155\n",
      "RMS_H: Importance 0.1951\n",
      "Peak-to-Peak_H: Importance 0.0046\n",
      "Entropy_H: Importance 0.0420\n",
      "Mean_Freq_H: Importance 0.0746\n",
      "Variance_H: Importance 0.0437\n",
      "Max_L: Importance 0.0014\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0907\n",
      "Std_L: Importance 0.0006\n",
      "RMS_L: Importance 0.1313\n",
      "Peak-to-Peak_L: Importance 0.0010\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0009\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0184\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1463\n",
      "Min_H: Importance 0.0133\n",
      "Mean_H: Importance 0.1364\n",
      "Std_H: Importance 0.0115\n",
      "Mean Deviation_H: Importance 0.0126\n",
      "RMS_H: Importance 0.1499\n",
      "Entropy_H: Importance 0.0166\n",
      "Mean_Freq_H: Importance 0.0692\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0475\n",
      "Mean_L: Importance 0.1051\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1668\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Centroid_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0293\n",
      "Variance_L: Importance 0.0270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1030\n",
      "Min_H: Importance 0.0296\n",
      "Mean_H: Importance 0.0901\n",
      "Std_H: Importance 0.0272\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1376\n",
      "Kurtosis_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0028\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1909\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1324\n",
      "Kurtosis_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0299\n",
      "Variance_L: Importance 0.0863\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1081\n",
      "Min_H: Importance 0.0196\n",
      "Mean_H: Importance 0.1027\n",
      "RMS_H: Importance 0.1425\n",
      "Peak-to-Peak_H: Importance 0.0076\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0288\n",
      "Mean_Freq_H: Importance 0.0501\n",
      "Variance_H: Importance 0.0835\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0441\n",
      "Mean_L: Importance 0.1598\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1222\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0277\n",
      "Spread_L: Importance 0.0002\n",
      "Mean_Freq_L: Importance 0.0098\n",
      "Variance_L: Importance 0.0884\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1108\n",
      "Min_H: Importance 0.0052\n",
      "Mean_H: Importance 0.0933\n",
      "Std_H: Importance 0.0222\n",
      "Mean Deviation_H: Importance 0.0288\n",
      "RMS_H: Importance 0.1419\n",
      "Skewness_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0160\n",
      "Spread_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0336\n",
      "Irregularity_H: Importance 0.0012\n",
      "Variance_H: Importance 0.0700\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0122\n",
      "Mean_L: Importance 0.1918\n",
      "Mean Deviation_L: Importance 0.0417\n",
      "RMS_L: Importance 0.1277\n",
      "Entropy_L: Importance 0.0091\n",
      "Variance_L: Importance 0.0905\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1001\n",
      "Mean_H: Importance 0.0833\n",
      "Std_H: Importance 0.0339\n",
      "Mean Deviation_H: Importance 0.0691\n",
      "RMS_H: Importance 0.1260\n",
      "Entropy_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0159\n",
      "Irregularity_H: Importance 0.0117\n",
      "Variance_H: Importance 0.0214\n",
      "Min_L: Importance 0.0386\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1241\n",
      "Entropy_L: Importance 0.0163\n",
      "Variance_L: Importance 0.0832\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0174\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0801\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0185\n",
      "Kurtosis_Freq_H: Importance 0.0109\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0881\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0199\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0144\n",
      "Variance_L: Importance 0.0833\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0805\n",
      "Mean_H: Importance 0.0828\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.1045\n",
      "Kurtosis_H: Importance 0.0001\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0187\n",
      "Kurtosis_Freq_H: Importance 0.0324\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0796\n",
      "Mean_L: Importance 0.1604\n",
      "Std_L: Importance 0.0202\n",
      "Mean Deviation_L: Importance 0.0802\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0198\n",
      "Mean_Freq_L: Importance 0.0381\n",
      "Variance_L: Importance 0.0602\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0060\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0801\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0599\n",
      "RMS_H: Importance 0.1244\n",
      "Peak-to-Peak_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.0181\n",
      "Kurtosis_Freq_H: Importance 0.0157\n",
      "Irregularity_H: Importance 0.0007\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0401\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0411\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1200\n",
      "Entropy_L: Importance 0.0200\n",
      "Mean_Freq_L: Importance 0.0199\n",
      "Irregularity_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0087\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 2**\n",
    "To conduct experiments, **ADC3** (data acquisition device) is used consistently throughout. **Probe 2** is kept constant, and the machine learning model is **always trained on Probe 2**. The model is then tested on **Probe 3** and **Probe 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 2** data and Testing on **Probe 3**\n",
    "To test Probe 2 with changing probe to Probes 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0320\n",
      "Min_H: Importance 0.0522\n",
      "Mean_H: Importance 0.1101\n",
      "Mean Deviation_H: Importance 0.0019\n",
      "RMS_H: Importance 0.1822\n",
      "Peak-to-Peak_H: Importance 0.0037\n",
      "Entropy_H: Importance 0.1499\n",
      "Spread_H: Importance 0.0031\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0194\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0582\n",
      "RMS_L: Importance 0.0378\n",
      "Skewness_L: Importance 0.0034\n",
      "Entropy_L: Importance 0.0394\n",
      "Mean_Freq_L: Importance 0.0220\n",
      "Irregularity_L: Importance 0.0045\n",
      "Variance_L: Importance 0.0828\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9999\n",
      "Balanced Accuracy: 0.9999\n",
      "MCC: 0.9998\n",
      "Log Loss: 0.0025\n",
      "F1 Score: 0.9999\n",
      "Recall: 1.0000\n",
      "Precision: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9996    2]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.0002\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0825\n",
      "Min_H: Importance 0.0317\n",
      "Mean_H: Importance 0.1433\n",
      "Mean Deviation_H: Importance 0.0046\n",
      "RMS_H: Importance 0.2186\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0184\n",
      "Mean_Freq_H: Importance 0.0679\n",
      "Variance_H: Importance 0.0391\n",
      "Max_L: Importance 0.0008\n",
      "Min_L: Importance 0.0281\n",
      "Mean_L: Importance 0.1291\n",
      "Std_L: Importance 0.0007\n",
      "RMS_L: Importance 0.1220\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0025\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0160\n",
      "Variance_L: Importance 0.0615\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1070\n",
      "Min_H: Importance 0.0266\n",
      "Mean_H: Importance 0.1418\n",
      "Mean Deviation_H: Importance 0.0104\n",
      "RMS_H: Importance 0.1501\n",
      "Centroid_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0169\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0627\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0279\n",
      "Mean_L: Importance 0.1151\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1620\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0302\n",
      "Mean_Freq_L: Importance 0.0102\n",
      "Variance_L: Importance 0.0685\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0634\n",
      "Min_H: Importance 0.0298\n",
      "Mean_H: Importance 0.0915\n",
      "Std_H: Importance 0.0221\n",
      "Mean Deviation_H: Importance 0.0164\n",
      "RMS_H: Importance 0.1400\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0141\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1975\n",
      "RMS_L: Importance 0.1205\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0293\n",
      "Mean_Freq_L: Importance 0.0045\n",
      "Variance_L: Importance 0.1270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0669\n",
      "Min_H: Importance 0.0105\n",
      "Mean_H: Importance 0.0930\n",
      "Std_H: Importance 0.0183\n",
      "RMS_H: Importance 0.1493\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0313\n",
      "Mean_Freq_H: Importance 0.0492\n",
      "Variance_H: Importance 0.0832\n",
      "Max_L: Importance 0.0121\n",
      "Min_L: Importance 0.0222\n",
      "Mean_L: Importance 0.1501\n",
      "Mean Deviation_L: Importance 0.0117\n",
      "RMS_L: Importance 0.1342\n",
      "Kurtosis_L: Importance 0.0022\n",
      "Centroid_L: Importance 0.0014\n",
      "Entropy_L: Importance 0.0419\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0042\n",
      "Mean_H: Importance 0.1130\n",
      "Std_H: Importance 0.0109\n",
      "Mean Deviation_H: Importance 0.0261\n",
      "RMS_H: Importance 0.1467\n",
      "Entropy_H: Importance 0.0170\n",
      "Spread_H: Importance 0.0052\n",
      "Mean_Freq_H: Importance 0.0232\n",
      "Variance_H: Importance 0.0819\n",
      "Max_L: Importance 0.0041\n",
      "Min_L: Importance 0.0677\n",
      "Mean_L: Importance 0.1654\n",
      "Mean Deviation_L: Importance 0.0099\n",
      "RMS_L: Importance 0.1270\n",
      "Peak-to-Peak_L: Importance 0.0016\n",
      "Centroid_L: Importance 0.0013\n",
      "Entropy_L: Importance 0.0100\n",
      "Variance_L: Importance 0.0709\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1035\n",
      "Mean_H: Importance 0.0925\n",
      "Std_H: Importance 0.0121\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1071\n",
      "Entropy_H: Importance 0.0152\n",
      "Skewness_Freq_H: Importance 0.0012\n",
      "Mean_Freq_H: Importance 0.0122\n",
      "Irregularity_H: Importance 0.0736\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0375\n",
      "Mean_L: Importance 0.1700\n",
      "Mean Deviation_L: Importance 0.0522\n",
      "RMS_L: Importance 0.1300\n",
      "Entropy_L: Importance 0.0094\n",
      "Irregularity_L: Importance 0.0174\n",
      "Variance_L: Importance 0.0889\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0084\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0832\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1259\n",
      "Entropy_H: Importance 0.0180\n",
      "Spread_H: Importance 0.0102\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0083\n",
      "Min_L: Importance 0.1283\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0806\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1280\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0640\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0606\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0202\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0400\n",
      "Min_L: Importance 0.1006\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0194\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0162\n",
      "Variance_L: Importance 0.0817\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 2** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0625\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1022\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0353\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0801\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0131\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 2** data and Testing on **Probe 1**\n",
    "To test Probe 2 with changing probe to Probes 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0320\n",
      "Min_H: Importance 0.0522\n",
      "Mean_H: Importance 0.1101\n",
      "Mean Deviation_H: Importance 0.0019\n",
      "RMS_H: Importance 0.1822\n",
      "Peak-to-Peak_H: Importance 0.0037\n",
      "Entropy_H: Importance 0.1499\n",
      "Spread_H: Importance 0.0031\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0194\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0582\n",
      "RMS_L: Importance 0.0378\n",
      "Skewness_L: Importance 0.0034\n",
      "Entropy_L: Importance 0.0394\n",
      "Mean_Freq_L: Importance 0.0220\n",
      "Irregularity_L: Importance 0.0045\n",
      "Variance_L: Importance 0.0828\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9998\n",
      "Balanced Accuracy: 0.9998\n",
      "MCC: 0.9996\n",
      "Log Loss: 0.0028\n",
      "F1 Score: 0.9998\n",
      "Recall: 1.0000\n",
      "Precision: 0.9996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9994    4]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.0004\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0825\n",
      "Min_H: Importance 0.0317\n",
      "Mean_H: Importance 0.1433\n",
      "Mean Deviation_H: Importance 0.0046\n",
      "RMS_H: Importance 0.2186\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0184\n",
      "Mean_Freq_H: Importance 0.0679\n",
      "Variance_H: Importance 0.0391\n",
      "Max_L: Importance 0.0008\n",
      "Min_L: Importance 0.0281\n",
      "Mean_L: Importance 0.1291\n",
      "Std_L: Importance 0.0007\n",
      "RMS_L: Importance 0.1220\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0025\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0160\n",
      "Variance_L: Importance 0.0615\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1070\n",
      "Min_H: Importance 0.0266\n",
      "Mean_H: Importance 0.1418\n",
      "Mean Deviation_H: Importance 0.0104\n",
      "RMS_H: Importance 0.1501\n",
      "Centroid_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0169\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0627\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0279\n",
      "Mean_L: Importance 0.1151\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1620\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0302\n",
      "Mean_Freq_L: Importance 0.0102\n",
      "Variance_L: Importance 0.0685\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3331\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0634\n",
      "Min_H: Importance 0.0298\n",
      "Mean_H: Importance 0.0915\n",
      "Std_H: Importance 0.0221\n",
      "Mean Deviation_H: Importance 0.0164\n",
      "RMS_H: Importance 0.1400\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0141\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1975\n",
      "RMS_L: Importance 0.1205\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0293\n",
      "Mean_Freq_L: Importance 0.0045\n",
      "Variance_L: Importance 0.1270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0669\n",
      "Min_H: Importance 0.0105\n",
      "Mean_H: Importance 0.0930\n",
      "Std_H: Importance 0.0183\n",
      "RMS_H: Importance 0.1493\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0313\n",
      "Mean_Freq_H: Importance 0.0492\n",
      "Variance_H: Importance 0.0832\n",
      "Max_L: Importance 0.0121\n",
      "Min_L: Importance 0.0222\n",
      "Mean_L: Importance 0.1501\n",
      "Mean Deviation_L: Importance 0.0117\n",
      "RMS_L: Importance 0.1342\n",
      "Kurtosis_L: Importance 0.0022\n",
      "Centroid_L: Importance 0.0014\n",
      "Entropy_L: Importance 0.0419\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0042\n",
      "Mean_H: Importance 0.1130\n",
      "Std_H: Importance 0.0109\n",
      "Mean Deviation_H: Importance 0.0261\n",
      "RMS_H: Importance 0.1467\n",
      "Entropy_H: Importance 0.0170\n",
      "Spread_H: Importance 0.0052\n",
      "Mean_Freq_H: Importance 0.0232\n",
      "Variance_H: Importance 0.0819\n",
      "Max_L: Importance 0.0041\n",
      "Min_L: Importance 0.0677\n",
      "Mean_L: Importance 0.1654\n",
      "Mean Deviation_L: Importance 0.0099\n",
      "RMS_L: Importance 0.1270\n",
      "Peak-to-Peak_L: Importance 0.0016\n",
      "Centroid_L: Importance 0.0013\n",
      "Entropy_L: Importance 0.0100\n",
      "Variance_L: Importance 0.0709\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1035\n",
      "Mean_H: Importance 0.0925\n",
      "Std_H: Importance 0.0121\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1071\n",
      "Entropy_H: Importance 0.0152\n",
      "Skewness_Freq_H: Importance 0.0012\n",
      "Mean_Freq_H: Importance 0.0122\n",
      "Irregularity_H: Importance 0.0736\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0375\n",
      "Mean_L: Importance 0.1700\n",
      "Mean Deviation_L: Importance 0.0522\n",
      "RMS_L: Importance 0.1300\n",
      "Entropy_L: Importance 0.0094\n",
      "Irregularity_L: Importance 0.0174\n",
      "Variance_L: Importance 0.0889\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0077\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0832\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1259\n",
      "Entropy_H: Importance 0.0180\n",
      "Spread_H: Importance 0.0102\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0083\n",
      "Min_L: Importance 0.1283\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0806\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1280\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0640\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0606\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0202\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0400\n",
      "Min_L: Importance 0.1006\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0194\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0162\n",
      "Variance_L: Importance 0.0817\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0053\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 2** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0625\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1022\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0353\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0801\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0092\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 2\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of Probe 3**\n",
    "To conduct experiments, **ADC3** (data acquisition device) is used consistently throughout. **Probe 3** is kept constant, and the machine learning model is **always trained on Probe 3**. The model is then tested on **Probe 3** and **Probe 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 3** data and Testing on **Probe 1**\n",
    "To test Probe 3 with changing probe to Probes 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0117\n",
      "Min_H: Importance 0.0313\n",
      "Mean_H: Importance 0.0859\n",
      "Mean Deviation_H: Importance 0.0079\n",
      "RMS_H: Importance 0.1399\n",
      "Skewness_H: Importance 0.0035\n",
      "Peak-to-Peak_H: Importance 0.0054\n",
      "Centroid_H: Importance 0.0052\n",
      "Entropy_H: Importance 0.2163\n",
      "Spread_H: Importance 0.0080\n",
      "Mean_Freq_H: Importance 0.0864\n",
      "Variance_H: Importance 0.0453\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0804\n",
      "Mean_L: Importance 0.0361\n",
      "RMS_L: Importance 0.0507\n",
      "Entropy_L: Importance 0.0519\n",
      "Mean_Freq_L: Importance 0.0253\n",
      "Variance_L: Importance 0.0882\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9999\n",
      "Balanced Accuracy: 0.9999\n",
      "MCC: 0.9998\n",
      "Log Loss: 0.0007\n",
      "F1 Score: 0.9999\n",
      "Recall: 1.0000\n",
      "Precision: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9996    2]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.0002\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1189\n",
      "Min_H: Importance 0.0311\n",
      "Mean_H: Importance 0.1143\n",
      "RMS_H: Importance 0.2099\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0368\n",
      "Mean_Freq_H: Importance 0.0777\n",
      "Variance_H: Importance 0.0597\n",
      "Max_L: Importance 0.0157\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1254\n",
      "Std_L: Importance 0.0009\n",
      "RMS_L: Importance 0.1253\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0339\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0127\n",
      "Variance_L: Importance 0.0070\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1478\n",
      "Min_H: Importance 0.0273\n",
      "Mean_H: Importance 0.1356\n",
      "Mean Deviation_H: Importance 0.0106\n",
      "RMS_H: Importance 0.1502\n",
      "Entropy_H: Importance 0.0193\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0682\n",
      "Variance_H: Importance 0.0617\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0353\n",
      "Mean_L: Importance 0.1086\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1615\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0107\n",
      "Variance_L: Importance 0.0273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3331\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Min_H: Importance 0.0300\n",
      "Mean_H: Importance 0.0895\n",
      "Std_H: Importance 0.0259\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1371\n",
      "Centroid_H: Importance 0.0030\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0665\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1919\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1319\n",
      "Entropy_L: Importance 0.0301\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0862\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0918\n",
      "Min_H: Importance 0.0211\n",
      "Mean_H: Importance 0.0998\n",
      "Std_H: Importance 0.0101\n",
      "Mean Deviation_H: Importance 0.0110\n",
      "RMS_H: Importance 0.1347\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0520\n",
      "Variance_H: Importance 0.0640\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.0187\n",
      "Mean_L: Importance 0.1890\n",
      "Mean Deviation_L: Importance 0.0100\n",
      "RMS_L: Importance 0.1280\n",
      "Kurtosis_L: Importance 0.0015\n",
      "Entropy_L: Importance 0.0286\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1073\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1196\n",
      "Min_H: Importance 0.0044\n",
      "Mean_H: Importance 0.0928\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0270\n",
      "RMS_H: Importance 0.1247\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0333\n",
      "Spread_H: Importance 0.0015\n",
      "Mean_Freq_H: Importance 0.0496\n",
      "Variance_H: Importance 0.0980\n",
      "Max_L: Importance 0.0087\n",
      "Min_L: Importance 0.0298\n",
      "Mean_L: Importance 0.1661\n",
      "Mean Deviation_L: Importance 0.0101\n",
      "RMS_L: Importance 0.1245\n",
      "Entropy_L: Importance 0.0075\n",
      "Variance_L: Importance 0.0911\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1017\n",
      "Mean_H: Importance 0.0875\n",
      "Std_H: Importance 0.0299\n",
      "Mean Deviation_H: Importance 0.0443\n",
      "RMS_H: Importance 0.1300\n",
      "Entropy_H: Importance 0.0151\n",
      "Mean_Freq_H: Importance 0.0147\n",
      "Irregularity_H: Importance 0.0056\n",
      "Variance_H: Importance 0.0216\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0409\n",
      "Mean_L: Importance 0.1808\n",
      "Std_L: Importance 0.0041\n",
      "Mean Deviation_L: Importance 0.1105\n",
      "RMS_L: Importance 0.1201\n",
      "Entropy_L: Importance 0.0124\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0020\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0402\n",
      "RMS_H: Importance 0.1245\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0103\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0876\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0398\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1020\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0840\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0008\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0829\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1024\n",
      "Peak-to-Peak_H: Importance 0.0007\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0198\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Irregularity_H: Importance 0.0160\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1220\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0799\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0831\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0197\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0194\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1011\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0401\n",
      "Kurtosis_Freq_H: Importance 0.0182\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0318\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **Probe 3** data and Testing on **Probe 2**\n",
    "To test Probe 3 with changing probe to Probes 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0117\n",
      "Min_H: Importance 0.0313\n",
      "Mean_H: Importance 0.0859\n",
      "Mean Deviation_H: Importance 0.0079\n",
      "RMS_H: Importance 0.1399\n",
      "Skewness_H: Importance 0.0035\n",
      "Peak-to-Peak_H: Importance 0.0054\n",
      "Centroid_H: Importance 0.0052\n",
      "Entropy_H: Importance 0.2163\n",
      "Spread_H: Importance 0.0080\n",
      "Mean_Freq_H: Importance 0.0864\n",
      "Variance_H: Importance 0.0453\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0804\n",
      "Mean_L: Importance 0.0361\n",
      "RMS_L: Importance 0.0507\n",
      "Entropy_L: Importance 0.0519\n",
      "Mean_Freq_L: Importance 0.0253\n",
      "Variance_L: Importance 0.0882\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9997\n",
      "Balanced Accuracy: 0.9997\n",
      "MCC: 0.9994\n",
      "Log Loss: 0.0019\n",
      "F1 Score: 0.9997\n",
      "Recall: 0.9996\n",
      "Precision: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9996    2]\n",
      " [   4 9994]]\n",
      "False Positive Rate (FPR): 0.0002\n",
      "False Negative Rate (FNR): 0.0004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1189\n",
      "Min_H: Importance 0.0311\n",
      "Mean_H: Importance 0.1143\n",
      "RMS_H: Importance 0.2099\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0368\n",
      "Mean_Freq_H: Importance 0.0777\n",
      "Variance_H: Importance 0.0597\n",
      "Max_L: Importance 0.0157\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1254\n",
      "Std_L: Importance 0.0009\n",
      "RMS_L: Importance 0.1253\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0339\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0127\n",
      "Variance_L: Importance 0.0070\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1478\n",
      "Min_H: Importance 0.0273\n",
      "Mean_H: Importance 0.1356\n",
      "Mean Deviation_H: Importance 0.0106\n",
      "RMS_H: Importance 0.1502\n",
      "Entropy_H: Importance 0.0193\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0682\n",
      "Variance_H: Importance 0.0617\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0353\n",
      "Mean_L: Importance 0.1086\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1615\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0107\n",
      "Variance_L: Importance 0.0273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6664\n",
      "   macro avg       1.00      1.00      1.00      6664\n",
      "weighted avg       1.00      1.00      1.00      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Min_H: Importance 0.0300\n",
      "Mean_H: Importance 0.0895\n",
      "Std_H: Importance 0.0259\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1371\n",
      "Centroid_H: Importance 0.0030\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0665\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1919\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1319\n",
      "Entropy_L: Importance 0.0301\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0862\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0005\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0918\n",
      "Min_H: Importance 0.0211\n",
      "Mean_H: Importance 0.0998\n",
      "Std_H: Importance 0.0101\n",
      "Mean Deviation_H: Importance 0.0110\n",
      "RMS_H: Importance 0.1347\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0520\n",
      "Variance_H: Importance 0.0640\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.0187\n",
      "Mean_L: Importance 0.1890\n",
      "Mean Deviation_L: Importance 0.0100\n",
      "RMS_L: Importance 0.1280\n",
      "Kurtosis_L: Importance 0.0015\n",
      "Entropy_L: Importance 0.0286\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1073\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0005\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1196\n",
      "Min_H: Importance 0.0044\n",
      "Mean_H: Importance 0.0928\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0270\n",
      "RMS_H: Importance 0.1247\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0333\n",
      "Spread_H: Importance 0.0015\n",
      "Mean_Freq_H: Importance 0.0496\n",
      "Variance_H: Importance 0.0980\n",
      "Max_L: Importance 0.0087\n",
      "Min_L: Importance 0.0298\n",
      "Mean_L: Importance 0.1661\n",
      "Mean Deviation_L: Importance 0.0101\n",
      "RMS_L: Importance 0.1245\n",
      "Entropy_L: Importance 0.0075\n",
      "Variance_L: Importance 0.0911\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1017\n",
      "Mean_H: Importance 0.0875\n",
      "Std_H: Importance 0.0299\n",
      "Mean Deviation_H: Importance 0.0443\n",
      "RMS_H: Importance 0.1300\n",
      "Entropy_H: Importance 0.0151\n",
      "Mean_Freq_H: Importance 0.0147\n",
      "Irregularity_H: Importance 0.0056\n",
      "Variance_H: Importance 0.0216\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0409\n",
      "Mean_L: Importance 0.1808\n",
      "Std_L: Importance 0.0041\n",
      "Mean Deviation_L: Importance 0.1105\n",
      "RMS_L: Importance 0.1201\n",
      "Entropy_L: Importance 0.0124\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0089\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0402\n",
      "RMS_H: Importance 0.1245\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0103\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0876\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0398\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1020\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0840\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0829\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1024\n",
      "Peak-to-Peak_H: Importance 0.0007\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0198\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Irregularity_H: Importance 0.0160\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1220\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0799\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0831\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0197\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0092\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1011\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0401\n",
      "Kurtosis_Freq_H: Importance 0.0182\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0050\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 3\n",
    "train_probe_dataset = 3\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
