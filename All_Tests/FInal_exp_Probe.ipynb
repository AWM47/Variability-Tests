{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADC1\n",
    "\n",
    "## **Testing of Probe 1**\n",
    "To conduct experiments, **ADC1** (data acquisition device) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on Probe 1**. The model is then tested on **Probe 2** and **Probe 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Probe1 data and Testing on Probe 2 data\n",
    "To test Probe 1 with changing probe to Probes 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0527\n",
      "Min_H: Importance 0.0141\n",
      "Mean_H: Importance 0.0989\n",
      "Std_H: Importance 0.0076\n",
      "Mean Deviation_H: Importance 0.0216\n",
      "RMS_H: Importance 0.1667\n",
      "Entropy_H: Importance 0.0460\n",
      "Mean_Freq_H: Importance 0.0641\n",
      "Variance_H: Importance 0.0496\n",
      "Max_L: Importance 0.0324\n",
      "Min_L: Importance 0.0696\n",
      "Mean_L: Importance 0.0698\n",
      "Std_L: Importance 0.0075\n",
      "RMS_L: Importance 0.0890\n",
      "Skewness_L: Importance 0.0072\n",
      "Centroid_L: Importance 0.0114\n",
      "Entropy_L: Importance 0.0299\n",
      "Mean_Freq_L: Importance 0.0159\n",
      "Variance_L: Importance 0.0973\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9518\n",
      "Balanced Accuracy: 0.9518\n",
      "MCC: 0.9064\n",
      "Log Loss: 0.1153\n",
      "F1 Score: 0.9536\n",
      "Recall: 0.9912\n",
      "Precision: 0.9188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.91      0.95      9998\n",
      "         CH2       0.92      0.99      0.95      9998\n",
      "\n",
      "    accuracy                           0.95     19996\n",
      "   macro avg       0.95      0.95      0.95     19996\n",
      "weighted avg       0.95      0.95      0.95     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9122  876]\n",
      " [  88 9910]]\n",
      "False Positive Rate (FPR): 0.0876\n",
      "False Negative Rate (FNR): 0.0088\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1224\n",
      "Min_H: Importance 0.0284\n",
      "Mean_H: Importance 0.1486\n",
      "Mean Deviation_H: Importance 0.0095\n",
      "RMS_H: Importance 0.2172\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.0173\n",
      "Mean_Freq_H: Importance 0.0727\n",
      "Variance_H: Importance 0.0400\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0255\n",
      "Mean_L: Importance 0.1253\n",
      "Std_L: Importance 0.0012\n",
      "RMS_L: Importance 0.1210\n",
      "Peak-to-Peak_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0108\n",
      "Variance_L: Importance 0.0256\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9999\n",
      "Balanced Accuracy: 0.9999\n",
      "MCC: 0.9998\n",
      "Log Loss: 0.0020\n",
      "F1 Score: 0.9999\n",
      "Recall: 0.9998\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   1 4997]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0683\n",
      "Min_H: Importance 0.0315\n",
      "Mean_H: Importance 0.0935\n",
      "Std_H: Importance 0.0002\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1295\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0179\n",
      "Mean_Freq_H: Importance 0.0551\n",
      "Kurtosis_Freq_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0675\n",
      "Max_L: Importance 0.0063\n",
      "Min_L: Importance 0.0191\n",
      "Mean_L: Importance 0.2050\n",
      "RMS_L: Importance 0.1282\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0234\n",
      "Mean_Freq_L: Importance 0.0096\n",
      "Variance_L: Importance 0.1366\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9973\n",
      "Balanced Accuracy: 0.9973\n",
      "MCC: 0.9946\n",
      "Log Loss: 0.0052\n",
      "F1 Score: 0.9973\n",
      "Recall: 0.9949\n",
      "Precision: 0.9997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      3332\n",
      "         CH2       1.00      0.99      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    1]\n",
      " [  17 3314]]\n",
      "False Positive Rate (FPR): 0.0003\n",
      "False Negative Rate (FNR): 0.0051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0684\n",
      "Min_H: Importance 0.0280\n",
      "Mean_H: Importance 0.0961\n",
      "Std_H: Importance 0.0123\n",
      "Mean Deviation_H: Importance 0.0169\n",
      "RMS_H: Importance 0.1322\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0144\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0579\n",
      "Variance_H: Importance 0.0606\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0165\n",
      "Mean_L: Importance 0.1996\n",
      "RMS_L: Importance 0.1272\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0301\n",
      "Mean_Freq_L: Importance 0.0124\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0601\n",
      "Min_H: Importance 0.0371\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0381\n",
      "Mean Deviation_H: Importance 0.0205\n",
      "RMS_H: Importance 0.0808\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.1024\n",
      "Variance_H: Importance 0.0601\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1433\n",
      "Mean Deviation_L: Importance 0.0151\n",
      "RMS_L: Importance 0.0803\n",
      "Entropy_L: Importance 0.0390\n",
      "Variance_L: Importance 0.1207\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9997\n",
      "Balanced Accuracy: 0.9997\n",
      "MCC: 0.9995\n",
      "Log Loss: 0.0367\n",
      "F1 Score: 0.9997\n",
      "Recall: 0.9995\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   1 1997]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0861\n",
      "Min_H: Importance 0.0066\n",
      "Mean_H: Importance 0.0985\n",
      "Std_H: Importance 0.0055\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1472\n",
      "Skewness_H: Importance 0.0007\n",
      "Centroid_H: Importance 0.0055\n",
      "Entropy_H: Importance 0.0095\n",
      "Mean_Freq_H: Importance 0.0104\n",
      "Variance_H: Importance 0.0821\n",
      "Max_L: Importance 0.0105\n",
      "Min_L: Importance 0.0172\n",
      "Mean_L: Importance 0.1887\n",
      "Std_L: Importance 0.0048\n",
      "Mean Deviation_L: Importance 0.0267\n",
      "RMS_L: Importance 0.1411\n",
      "Entropy_L: Importance 0.0052\n",
      "Variance_L: Importance 0.1124\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0012\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0849\n",
      "Std_H: Importance 0.0575\n",
      "Mean Deviation_H: Importance 0.0650\n",
      "RMS_H: Importance 0.1222\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0191\n",
      "Mean_Freq_H: Importance 0.0381\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0215\n",
      "Mean_L: Importance 0.1811\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1208\n",
      "Entropy_L: Importance 0.0147\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0004\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0852\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0171\n",
      "Kurtosis_Freq_H: Importance 0.0293\n",
      "Variance_H: Importance 0.0401\n",
      "Min_L: Importance 0.0865\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0195\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1240\n",
      "Entropy_L: Importance 0.0194\n",
      "Mean_Freq_L: Importance 0.0136\n",
      "Variance_L: Importance 0.0609\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0064\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0803\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1239\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0189\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1001\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1211\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0398\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0011\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0203\n",
      "RMS_H: Importance 0.0802\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0195\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Probe1 data and Testing on Probe 3 data\n",
    "To test Probe 1 with changing probe to Probe 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0527\n",
      "Min_H: Importance 0.0141\n",
      "Mean_H: Importance 0.0989\n",
      "Std_H: Importance 0.0076\n",
      "Mean Deviation_H: Importance 0.0216\n",
      "RMS_H: Importance 0.1667\n",
      "Entropy_H: Importance 0.0460\n",
      "Mean_Freq_H: Importance 0.0641\n",
      "Variance_H: Importance 0.0496\n",
      "Max_L: Importance 0.0324\n",
      "Min_L: Importance 0.0696\n",
      "Mean_L: Importance 0.0698\n",
      "Std_L: Importance 0.0075\n",
      "RMS_L: Importance 0.0890\n",
      "Skewness_L: Importance 0.0072\n",
      "Centroid_L: Importance 0.0114\n",
      "Entropy_L: Importance 0.0299\n",
      "Mean_Freq_L: Importance 0.0159\n",
      "Variance_L: Importance 0.0973\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9597\n",
      "Balanced Accuracy: 0.9597\n",
      "MCC: 0.9218\n",
      "Log Loss: 0.1084\n",
      "F1 Score: 0.9611\n",
      "Recall: 0.9958\n",
      "Precision: 0.9287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.92      0.96      9998\n",
      "         CH2       0.93      1.00      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9234  764]\n",
      " [  42 9956]]\n",
      "False Positive Rate (FPR): 0.0764\n",
      "False Negative Rate (FNR): 0.0042\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1224\n",
      "Min_H: Importance 0.0284\n",
      "Mean_H: Importance 0.1486\n",
      "Mean Deviation_H: Importance 0.0095\n",
      "RMS_H: Importance 0.2172\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.0173\n",
      "Mean_Freq_H: Importance 0.0727\n",
      "Variance_H: Importance 0.0400\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0255\n",
      "Mean_L: Importance 0.1253\n",
      "Std_L: Importance 0.0012\n",
      "RMS_L: Importance 0.1210\n",
      "Peak-to-Peak_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0108\n",
      "Variance_L: Importance 0.0256\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0016\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0683\n",
      "Min_H: Importance 0.0315\n",
      "Mean_H: Importance 0.0935\n",
      "Std_H: Importance 0.0002\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1295\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0179\n",
      "Mean_Freq_H: Importance 0.0551\n",
      "Kurtosis_Freq_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0675\n",
      "Max_L: Importance 0.0063\n",
      "Min_L: Importance 0.0191\n",
      "Mean_L: Importance 0.2050\n",
      "RMS_L: Importance 0.1282\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0234\n",
      "Mean_Freq_L: Importance 0.0096\n",
      "Variance_L: Importance 0.1366\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9676\n",
      "Balanced Accuracy: 0.9676\n",
      "MCC: 0.9370\n",
      "Log Loss: 0.0684\n",
      "F1 Score: 0.9665\n",
      "Recall: 0.9364\n",
      "Precision: 0.9987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.94      1.00      0.97      3331\n",
      "         CH2       1.00      0.94      0.97      3332\n",
      "\n",
      "    accuracy                           0.97      6663\n",
      "   macro avg       0.97      0.97      0.97      6663\n",
      "weighted avg       0.97      0.97      0.97      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3327    4]\n",
      " [ 212 3120]]\n",
      "False Positive Rate (FPR): 0.0012\n",
      "False Negative Rate (FNR): 0.0636\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0684\n",
      "Min_H: Importance 0.0280\n",
      "Mean_H: Importance 0.0961\n",
      "Std_H: Importance 0.0123\n",
      "Mean Deviation_H: Importance 0.0169\n",
      "RMS_H: Importance 0.1322\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0144\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0579\n",
      "Variance_H: Importance 0.0606\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0165\n",
      "Mean_L: Importance 0.1996\n",
      "RMS_L: Importance 0.1272\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0301\n",
      "Mean_Freq_L: Importance 0.0124\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "adc_num = 1\n",
    "train_probe_dataset = 1\n",
    "test_probe_dataset = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe names based on dataset numbers\n",
    "train_probe = f\"Probe{train_probe_dataset}\"\n",
    "test_probe = f\"Probe{test_probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(train_probe, adc_num, 1)\n",
    "train_ch1 = load_channel_data(train_probe, adc_num, 2)\n",
    "test_ch0 = load_channel_data(test_probe, adc_num, 1)\n",
    "test_ch1 = load_channel_data(test_probe, adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_{train_probe}_to_{test_probe}_ADC{adc_num}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **Probe 1** and tested on **Probe 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
