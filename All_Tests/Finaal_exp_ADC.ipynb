{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 1**\n",
    "To conduct experiments, **Probe 1** (SMA cable which transfers data) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on ADC 1** data. The model is then tested on **ADC 2** and **ADC 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 1** data and Testing on **ADC 2** data\n",
    "To test ADC 1 with changing probe to ADC 2 and ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0527\n",
      "Min_H: Importance 0.0141\n",
      "Mean_H: Importance 0.0989\n",
      "Std_H: Importance 0.0076\n",
      "Mean Deviation_H: Importance 0.0216\n",
      "RMS_H: Importance 0.1667\n",
      "Entropy_H: Importance 0.0460\n",
      "Mean_Freq_H: Importance 0.0641\n",
      "Variance_H: Importance 0.0496\n",
      "Max_L: Importance 0.0324\n",
      "Min_L: Importance 0.0696\n",
      "Mean_L: Importance 0.0698\n",
      "Std_L: Importance 0.0075\n",
      "RMS_L: Importance 0.0890\n",
      "Skewness_L: Importance 0.0072\n",
      "Centroid_L: Importance 0.0114\n",
      "Entropy_L: Importance 0.0299\n",
      "Mean_Freq_L: Importance 0.0159\n",
      "Variance_L: Importance 0.0973\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9379\n",
      "Balanced Accuracy: 0.9379\n",
      "MCC: 0.8791\n",
      "Log Loss: 0.2064\n",
      "F1 Score: 0.9351\n",
      "Recall: 0.8946\n",
      "Precision: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.90      0.98      0.94      9998\n",
      "         CH2       0.98      0.89      0.94      9998\n",
      "\n",
      "    accuracy                           0.94     19996\n",
      "   macro avg       0.94      0.94      0.94     19996\n",
      "weighted avg       0.94      0.94      0.94     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9810  188]\n",
      " [1054 8944]]\n",
      "False Positive Rate (FPR): 0.0188\n",
      "False Negative Rate (FNR): 0.1054\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1224\n",
      "Min_H: Importance 0.0284\n",
      "Mean_H: Importance 0.1486\n",
      "Mean Deviation_H: Importance 0.0095\n",
      "RMS_H: Importance 0.2172\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.0173\n",
      "Mean_Freq_H: Importance 0.0727\n",
      "Variance_H: Importance 0.0400\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0255\n",
      "Mean_L: Importance 0.1253\n",
      "Std_L: Importance 0.0012\n",
      "RMS_L: Importance 0.1210\n",
      "Peak-to-Peak_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0108\n",
      "Variance_L: Importance 0.0256\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9984\n",
      "Balanced Accuracy: 0.9984\n",
      "MCC: 0.9968\n",
      "Log Loss: 0.0088\n",
      "F1 Score: 0.9984\n",
      "Recall: 0.9968\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [  16 4982]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0683\n",
      "Min_H: Importance 0.0315\n",
      "Mean_H: Importance 0.0935\n",
      "Std_H: Importance 0.0002\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1295\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0179\n",
      "Mean_Freq_H: Importance 0.0551\n",
      "Kurtosis_Freq_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0675\n",
      "Max_L: Importance 0.0063\n",
      "Min_L: Importance 0.0191\n",
      "Mean_L: Importance 0.2050\n",
      "RMS_L: Importance 0.1282\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0234\n",
      "Mean_Freq_L: Importance 0.0096\n",
      "Variance_L: Importance 0.1366\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.8552\n",
      "Balanced Accuracy: 0.8552\n",
      "MCC: 0.7422\n",
      "Log Loss: 0.9122\n",
      "F1 Score: 0.8307\n",
      "Recall: 0.7104\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.78      1.00      0.87      3331\n",
      "         CH2       1.00      0.71      0.83      3332\n",
      "\n",
      "    accuracy                           0.86      6663\n",
      "   macro avg       0.89      0.86      0.85      6663\n",
      "weighted avg       0.89      0.86      0.85      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [ 965 2367]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.2896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0684\n",
      "Min_H: Importance 0.0280\n",
      "Mean_H: Importance 0.0961\n",
      "Std_H: Importance 0.0123\n",
      "Mean Deviation_H: Importance 0.0169\n",
      "RMS_H: Importance 0.1322\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0144\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0579\n",
      "Variance_H: Importance 0.0606\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0165\n",
      "Mean_L: Importance 0.1996\n",
      "RMS_L: Importance 0.1272\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0301\n",
      "Mean_Freq_L: Importance 0.0124\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0007\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0601\n",
      "Min_H: Importance 0.0371\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0381\n",
      "Mean Deviation_H: Importance 0.0205\n",
      "RMS_H: Importance 0.0808\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.1024\n",
      "Variance_H: Importance 0.0601\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1433\n",
      "Mean Deviation_L: Importance 0.0151\n",
      "RMS_L: Importance 0.0803\n",
      "Entropy_L: Importance 0.0390\n",
      "Variance_L: Importance 0.1207\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9950\n",
      "Balanced Accuracy: 0.9950\n",
      "MCC: 0.9900\n",
      "Log Loss: 0.0469\n",
      "F1 Score: 0.9950\n",
      "Recall: 0.9900\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      1998\n",
      "         CH2       1.00      0.99      0.99      1998\n",
      "\n",
      "    accuracy                           0.99      3996\n",
      "   macro avg       1.00      0.99      0.99      3996\n",
      "weighted avg       1.00      0.99      0.99      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [  20 1978]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0861\n",
      "Min_H: Importance 0.0066\n",
      "Mean_H: Importance 0.0985\n",
      "Std_H: Importance 0.0055\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1472\n",
      "Skewness_H: Importance 0.0007\n",
      "Centroid_H: Importance 0.0055\n",
      "Entropy_H: Importance 0.0095\n",
      "Mean_Freq_H: Importance 0.0104\n",
      "Variance_H: Importance 0.0821\n",
      "Max_L: Importance 0.0105\n",
      "Min_L: Importance 0.0172\n",
      "Mean_L: Importance 0.1887\n",
      "Std_L: Importance 0.0048\n",
      "Mean Deviation_L: Importance 0.0267\n",
      "RMS_L: Importance 0.1411\n",
      "Entropy_L: Importance 0.0052\n",
      "Variance_L: Importance 0.1124\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0047\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0849\n",
      "Std_H: Importance 0.0575\n",
      "Mean Deviation_H: Importance 0.0650\n",
      "RMS_H: Importance 0.1222\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0191\n",
      "Mean_Freq_H: Importance 0.0381\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0215\n",
      "Mean_L: Importance 0.1811\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1208\n",
      "Entropy_L: Importance 0.0147\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0215\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0852\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0171\n",
      "Kurtosis_Freq_H: Importance 0.0293\n",
      "Variance_H: Importance 0.0401\n",
      "Min_L: Importance 0.0865\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0195\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1240\n",
      "Entropy_L: Importance 0.0194\n",
      "Mean_Freq_L: Importance 0.0136\n",
      "Variance_L: Importance 0.0609\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0961\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0803\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1239\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0189\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1001\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1211\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0398\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0502\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0203\n",
      "RMS_H: Importance 0.0802\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0195\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0817\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1 # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 1** data and Testing on **ADC 3** data\n",
    "To test ADC 1 with changing probe to ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0527\n",
      "Min_H: Importance 0.0141\n",
      "Mean_H: Importance 0.0989\n",
      "Std_H: Importance 0.0076\n",
      "Mean Deviation_H: Importance 0.0216\n",
      "RMS_H: Importance 0.1667\n",
      "Entropy_H: Importance 0.0460\n",
      "Mean_Freq_H: Importance 0.0641\n",
      "Variance_H: Importance 0.0496\n",
      "Max_L: Importance 0.0324\n",
      "Min_L: Importance 0.0696\n",
      "Mean_L: Importance 0.0698\n",
      "Std_L: Importance 0.0075\n",
      "RMS_L: Importance 0.0890\n",
      "Skewness_L: Importance 0.0072\n",
      "Centroid_L: Importance 0.0114\n",
      "Entropy_L: Importance 0.0299\n",
      "Mean_Freq_L: Importance 0.0159\n",
      "Variance_L: Importance 0.0973\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9994\n",
      "Balanced Accuracy: 0.9994\n",
      "MCC: 0.9989\n",
      "Log Loss: 0.0056\n",
      "F1 Score: 0.9994\n",
      "Recall: 0.9990\n",
      "Precision: 0.9999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      9998\n",
      "         CH2       1.00      1.00      1.00      9998\n",
      "\n",
      "    accuracy                           1.00     19996\n",
      "   macro avg       1.00      1.00      1.00     19996\n",
      "weighted avg       1.00      1.00      1.00     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9997    1]\n",
      " [  10 9988]]\n",
      "False Positive Rate (FPR): 0.0001\n",
      "False Negative Rate (FNR): 0.0010\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1224\n",
      "Min_H: Importance 0.0284\n",
      "Mean_H: Importance 0.1486\n",
      "Mean Deviation_H: Importance 0.0095\n",
      "RMS_H: Importance 0.2172\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.0173\n",
      "Mean_Freq_H: Importance 0.0727\n",
      "Variance_H: Importance 0.0400\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0255\n",
      "Mean_L: Importance 0.1253\n",
      "Std_L: Importance 0.0012\n",
      "RMS_L: Importance 0.1210\n",
      "Peak-to-Peak_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0108\n",
      "Variance_L: Importance 0.0256\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1 # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0683\n",
      "Min_H: Importance 0.0315\n",
      "Mean_H: Importance 0.0935\n",
      "Std_H: Importance 0.0002\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1295\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0179\n",
      "Mean_Freq_H: Importance 0.0551\n",
      "Kurtosis_Freq_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0675\n",
      "Max_L: Importance 0.0063\n",
      "Min_L: Importance 0.0191\n",
      "Mean_L: Importance 0.2050\n",
      "RMS_L: Importance 0.1282\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0234\n",
      "Mean_Freq_L: Importance 0.0096\n",
      "Variance_L: Importance 0.1366\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9985\n",
      "Balanced Accuracy: 0.9985\n",
      "MCC: 0.9970\n",
      "Log Loss: 0.0137\n",
      "F1 Score: 0.9985\n",
      "Recall: 0.9970\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3331\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [  10 3322]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0684\n",
      "Min_H: Importance 0.0280\n",
      "Mean_H: Importance 0.0961\n",
      "Std_H: Importance 0.0123\n",
      "Mean Deviation_H: Importance 0.0169\n",
      "RMS_H: Importance 0.1322\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0144\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0579\n",
      "Variance_H: Importance 0.0606\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0165\n",
      "Mean_L: Importance 0.1996\n",
      "RMS_L: Importance 0.1272\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0301\n",
      "Mean_Freq_L: Importance 0.0124\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0601\n",
      "Min_H: Importance 0.0371\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0381\n",
      "Mean Deviation_H: Importance 0.0205\n",
      "RMS_H: Importance 0.0808\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.1024\n",
      "Variance_H: Importance 0.0601\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1433\n",
      "Mean Deviation_L: Importance 0.0151\n",
      "RMS_L: Importance 0.0803\n",
      "Entropy_L: Importance 0.0390\n",
      "Variance_L: Importance 0.1207\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9970\n",
      "Balanced Accuracy: 0.9970\n",
      "MCC: 0.9940\n",
      "Log Loss: 0.0427\n",
      "F1 Score: 0.9970\n",
      "Recall: 0.9940\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00      1998\n",
      "         CH2       1.00      0.99      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [  12 1986]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0060\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0861\n",
      "Min_H: Importance 0.0066\n",
      "Mean_H: Importance 0.0985\n",
      "Std_H: Importance 0.0055\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1472\n",
      "Skewness_H: Importance 0.0007\n",
      "Centroid_H: Importance 0.0055\n",
      "Entropy_H: Importance 0.0095\n",
      "Mean_Freq_H: Importance 0.0104\n",
      "Variance_H: Importance 0.0821\n",
      "Max_L: Importance 0.0105\n",
      "Min_L: Importance 0.0172\n",
      "Mean_L: Importance 0.1887\n",
      "Std_L: Importance 0.0048\n",
      "Mean Deviation_L: Importance 0.0267\n",
      "RMS_L: Importance 0.1411\n",
      "Entropy_L: Importance 0.0052\n",
      "Variance_L: Importance 0.1124\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0048\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0849\n",
      "Std_H: Importance 0.0575\n",
      "Mean Deviation_H: Importance 0.0650\n",
      "RMS_H: Importance 0.1222\n",
      "Centroid_H: Importance 0.0015\n",
      "Entropy_H: Importance 0.0191\n",
      "Mean_Freq_H: Importance 0.0381\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0215\n",
      "Mean_L: Importance 0.1811\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1208\n",
      "Entropy_L: Importance 0.0147\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0038\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1 # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0852\n",
      "Mean Deviation_H: Importance 0.0406\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0171\n",
      "Kurtosis_Freq_H: Importance 0.0293\n",
      "Variance_H: Importance 0.0401\n",
      "Min_L: Importance 0.0865\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0195\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1240\n",
      "Entropy_L: Importance 0.0194\n",
      "Mean_Freq_L: Importance 0.0136\n",
      "Variance_L: Importance 0.0609\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0983\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0803\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1239\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0189\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1001\n",
      "Mean_L: Importance 0.1400\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1211\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0398\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0151\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0203\n",
      "RMS_H: Importance 0.0802\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0195\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9985\n",
      "Balanced Accuracy: 0.9985\n",
      "MCC: 0.9970\n",
      "Log Loss: 0.1169\n",
      "F1 Score: 0.9985\n",
      "Recall: 0.9970\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  1 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 2**\n",
    "To conduct experiments, **Probe 1** (SMA cable which trandfers data) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on ADC 2**. The model is then tested on **ADC 3** and **ADC 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 2** data and Testing on **ADC 3**\n",
    "To test ADC 2 with changing probe to ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1456\n",
      "Min_H: Importance 0.0167\n",
      "Mean_H: Importance 0.0920\n",
      "Mean Deviation_H: Importance 0.0084\n",
      "RMS_H: Importance 0.1570\n",
      "Skewness_H: Importance 0.0075\n",
      "Kurtosis_H: Importance 0.0072\n",
      "Entropy_H: Importance 0.1037\n",
      "Mean_Freq_H: Importance 0.0742\n",
      "Variance_H: Importance 0.0309\n",
      "Max_L: Importance 0.0124\n",
      "Min_L: Importance 0.0557\n",
      "Mean_L: Importance 0.0467\n",
      "RMS_L: Importance 0.0719\n",
      "Skewness_L: Importance 0.0199\n",
      "Kurtosis_L: Importance 0.0082\n",
      "Entropy_L: Importance 0.0349\n",
      "Mean_Freq_L: Importance 0.0153\n",
      "Variance_L: Importance 0.0319\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9107\n",
      "Balanced Accuracy: 0.9107\n",
      "MCC: 0.8348\n",
      "Log Loss: 0.1709\n",
      "F1 Score: 0.9180\n",
      "Recall: 1.0000\n",
      "Precision: 0.8484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.82      0.90      9998\n",
      "         CH2       0.85      1.00      0.92      9998\n",
      "\n",
      "    accuracy                           0.91     19996\n",
      "   macro avg       0.92      0.91      0.91     19996\n",
      "weighted avg       0.92      0.91      0.91     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8212 1786]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.1786\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0831\n",
      "Min_H: Importance 0.0256\n",
      "Mean_H: Importance 0.1231\n",
      "Mean Deviation_H: Importance 0.0052\n",
      "RMS_H: Importance 0.2181\n",
      "Skewness_H: Importance 0.0008\n",
      "Entropy_H: Importance 0.0228\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0733\n",
      "Variance_H: Importance 0.0369\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.1224\n",
      "Std_L: Importance 0.0017\n",
      "RMS_L: Importance 0.1415\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Entropy_L: Importance 0.0361\n",
      "Spread_L: Importance 0.0008\n",
      "Mean_Freq_L: Importance 0.0120\n",
      "Variance_L: Importance 0.0648\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0189\n",
      "Min_H: Importance 0.0060\n",
      "Mean_H: Importance 0.0989\n",
      "Mean Deviation_H: Importance 0.0030\n",
      "RMS_H: Importance 0.1652\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0099\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Variance_H: Importance 0.0314\n",
      "Max_L: Importance 0.0052\n",
      "Min_L: Importance 0.1486\n",
      "Mean_L: Importance 0.1112\n",
      "RMS_L: Importance 0.2474\n",
      "Peak-to-Peak_L: Importance 0.0012\n",
      "Entropy_L: Importance 0.0229\n",
      "Mean_Freq_L: Importance 0.0088\n",
      "Irregularity_L: Importance 0.0010\n",
      "Variance_L: Importance 0.0910\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3331\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0651\n",
      "Min_H: Importance 0.0282\n",
      "Mean_H: Importance 0.0975\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1401\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0282\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0136\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1900\n",
      "RMS_L: Importance 0.1277\n",
      "Skewness_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0345\n",
      "Mean_Freq_L: Importance 0.0111\n",
      "Variance_L: Importance 0.1015\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0565\n",
      "Min_H: Importance 0.0079\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0270\n",
      "Mean Deviation_H: Importance 0.0008\n",
      "RMS_H: Importance 0.1352\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0402\n",
      "Mean_Freq_H: Importance 0.0817\n",
      "Variance_H: Importance 0.0861\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0007\n",
      "Mean_L: Importance 0.1734\n",
      "Mean Deviation_L: Importance 0.0106\n",
      "RMS_L: Importance 0.1227\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0432\n",
      "Spread_L: Importance 0.0002\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0031\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0398\n",
      "Mean Deviation_H: Importance 0.0428\n",
      "RMS_H: Importance 0.0640\n",
      "Peak-to-Peak_H: Importance 0.0010\n",
      "Entropy_H: Importance 0.1402\n",
      "Spread_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.1013\n",
      "Kurtosis_Freq_H: Importance 0.0125\n",
      "Variance_H: Importance 0.0800\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1006\n",
      "RMS_L: Importance 0.0802\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0379\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0604\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0706\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.0831\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0169\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0361\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0002\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0142\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1212\n",
      "Entropy_H: Importance 0.0399\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0566\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0217\n",
      "Min_L: Importance 0.0252\n",
      "Mean_L: Importance 0.1820\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.1047\n",
      "RMS_L: Importance 0.1006\n",
      "Entropy_L: Importance 0.0185\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0323\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0615\n",
      "Peak-to-Peak_H: Importance 0.0200\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Irregularity_H: Importance 0.0185\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0400\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0294\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1022\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0008\n",
      "Mean Deviation_H: Importance 0.0001\n",
      "RMS_H: Importance 0.1231\n",
      "Entropy_H: Importance 0.0188\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0021\n",
      "Irregularity_H: Importance 0.0344\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0205\n",
      "Mean_L: Importance 0.1809\n",
      "Std_L: Importance 0.0606\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0196\n",
      "Mean_Freq_L: Importance 0.0171\n",
      "Kurtosis_Freq_L: Importance 0.0170\n",
      "Variance_L: Importance 0.0808\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0079\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 2** data and Testing on **ADC 1**\n",
    "To test ADC 2 with changing probe to ADC 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1456\n",
      "Min_H: Importance 0.0167\n",
      "Mean_H: Importance 0.0920\n",
      "Mean Deviation_H: Importance 0.0084\n",
      "RMS_H: Importance 0.1570\n",
      "Skewness_H: Importance 0.0075\n",
      "Kurtosis_H: Importance 0.0072\n",
      "Entropy_H: Importance 0.1037\n",
      "Mean_Freq_H: Importance 0.0742\n",
      "Variance_H: Importance 0.0309\n",
      "Max_L: Importance 0.0124\n",
      "Min_L: Importance 0.0557\n",
      "Mean_L: Importance 0.0467\n",
      "RMS_L: Importance 0.0719\n",
      "Skewness_L: Importance 0.0199\n",
      "Kurtosis_L: Importance 0.0082\n",
      "Entropy_L: Importance 0.0349\n",
      "Mean_Freq_L: Importance 0.0153\n",
      "Variance_L: Importance 0.0319\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9149\n",
      "Balanced Accuracy: 0.9149\n",
      "MCC: 0.8419\n",
      "Log Loss: 0.2006\n",
      "F1 Score: 0.9215\n",
      "Recall: 0.9991\n",
      "Precision: 0.8551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.83      0.91      9998\n",
      "         CH2       0.86      1.00      0.92      9998\n",
      "\n",
      "    accuracy                           0.91     19996\n",
      "   macro avg       0.93      0.91      0.91     19996\n",
      "weighted avg       0.93      0.91      0.91     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8306 1692]\n",
      " [   9 9989]]\n",
      "False Positive Rate (FPR): 0.1692\n",
      "False Negative Rate (FNR): 0.0009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0831\n",
      "Min_H: Importance 0.0256\n",
      "Mean_H: Importance 0.1231\n",
      "Mean Deviation_H: Importance 0.0052\n",
      "RMS_H: Importance 0.2181\n",
      "Skewness_H: Importance 0.0008\n",
      "Entropy_H: Importance 0.0228\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0733\n",
      "Variance_H: Importance 0.0369\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.1224\n",
      "Std_L: Importance 0.0017\n",
      "RMS_L: Importance 0.1415\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Entropy_L: Importance 0.0361\n",
      "Spread_L: Importance 0.0008\n",
      "Mean_Freq_L: Importance 0.0120\n",
      "Variance_L: Importance 0.0648\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0041\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0189\n",
      "Min_H: Importance 0.0060\n",
      "Mean_H: Importance 0.0989\n",
      "Mean Deviation_H: Importance 0.0030\n",
      "RMS_H: Importance 0.1652\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0099\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Variance_H: Importance 0.0314\n",
      "Max_L: Importance 0.0052\n",
      "Min_L: Importance 0.1486\n",
      "Mean_L: Importance 0.1112\n",
      "RMS_L: Importance 0.2474\n",
      "Peak-to-Peak_L: Importance 0.0012\n",
      "Entropy_L: Importance 0.0229\n",
      "Mean_Freq_L: Importance 0.0088\n",
      "Irregularity_L: Importance 0.0010\n",
      "Variance_L: Importance 0.0910\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9542\n",
      "Balanced Accuracy: 0.9542\n",
      "MCC: 0.9123\n",
      "Log Loss: 0.0892\n",
      "F1 Score: 0.9562\n",
      "Recall: 1.0000\n",
      "Precision: 0.9161\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.91      0.95      3332\n",
      "         CH2       0.92      1.00      0.96      3332\n",
      "\n",
      "    accuracy                           0.95      6664\n",
      "   macro avg       0.96      0.95      0.95      6664\n",
      "weighted avg       0.96      0.95      0.95      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3027  305]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0915\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0651\n",
      "Min_H: Importance 0.0282\n",
      "Mean_H: Importance 0.0975\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1401\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0282\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0136\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1900\n",
      "RMS_L: Importance 0.1277\n",
      "Skewness_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0345\n",
      "Mean_Freq_L: Importance 0.0111\n",
      "Variance_L: Importance 0.1015\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0018\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0565\n",
      "Min_H: Importance 0.0079\n",
      "Mean_H: Importance 0.0818\n",
      "Std_H: Importance 0.0270\n",
      "Mean Deviation_H: Importance 0.0008\n",
      "RMS_H: Importance 0.1352\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0402\n",
      "Mean_Freq_H: Importance 0.0817\n",
      "Variance_H: Importance 0.0861\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0007\n",
      "Mean_L: Importance 0.1734\n",
      "Mean Deviation_L: Importance 0.0106\n",
      "RMS_L: Importance 0.1227\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0432\n",
      "Spread_L: Importance 0.0002\n",
      "Variance_L: Importance 0.1273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0017\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0398\n",
      "Mean Deviation_H: Importance 0.0428\n",
      "RMS_H: Importance 0.0640\n",
      "Peak-to-Peak_H: Importance 0.0010\n",
      "Entropy_H: Importance 0.1402\n",
      "Spread_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.1013\n",
      "Kurtosis_Freq_H: Importance 0.0125\n",
      "Variance_H: Importance 0.0800\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1006\n",
      "RMS_L: Importance 0.0802\n",
      "Skewness_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0379\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0604\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1086\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.0831\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0169\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0263\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0002\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0142\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1212\n",
      "Entropy_H: Importance 0.0399\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0566\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0217\n",
      "Min_L: Importance 0.0252\n",
      "Mean_L: Importance 0.1820\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.1047\n",
      "RMS_L: Importance 0.1006\n",
      "Entropy_L: Importance 0.0185\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0419\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0615\n",
      "Peak-to-Peak_H: Importance 0.0200\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Irregularity_H: Importance 0.0185\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0400\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9962\n",
      "Balanced Accuracy: 0.9962\n",
      "MCC: 0.9925\n",
      "Log Loss: 0.1208\n",
      "F1 Score: 0.9962\n",
      "Recall: 1.0000\n",
      "Precision: 0.9925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.99      1.00       398\n",
      "         CH2       0.99      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[395   3]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0075\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1022\n",
      "Mean_H: Importance 0.0804\n",
      "Std_H: Importance 0.0008\n",
      "Mean Deviation_H: Importance 0.0001\n",
      "RMS_H: Importance 0.1231\n",
      "Entropy_H: Importance 0.0188\n",
      "Skewness_Freq_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0021\n",
      "Irregularity_H: Importance 0.0344\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0205\n",
      "Mean_L: Importance 0.1809\n",
      "Std_L: Importance 0.0606\n",
      "Mean Deviation_L: Importance 0.1000\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0196\n",
      "Mean_Freq_L: Importance 0.0171\n",
      "Kurtosis_Freq_L: Importance 0.0170\n",
      "Variance_L: Importance 0.0808\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 3**\n",
    "To conduct experiments, **Probe 1** (SMA cable which trandfers data) is used consistently throughout. **Probe 1** is kept constant, and the machine learning model is **always trained on ADC 3**. The model is then tested on **ADC 1** and **ADC 2** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 3** data and Testing on **ADC 1**\n",
    "To test ADC 3 with changing probe to ADC 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0487\n",
      "Min_H: Importance 0.0563\n",
      "Mean_H: Importance 0.0667\n",
      "Mean Deviation_H: Importance 0.0065\n",
      "RMS_H: Importance 0.1228\n",
      "Skewness_H: Importance 0.0020\n",
      "Peak-to-Peak_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.2617\n",
      "Spread_H: Importance 0.0036\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0383\n",
      "Max_L: Importance 0.0552\n",
      "Min_L: Importance 0.0363\n",
      "Mean_L: Importance 0.0386\n",
      "RMS_L: Importance 0.0186\n",
      "Peak-to-Peak_L: Importance 0.0031\n",
      "Entropy_L: Importance 0.0350\n",
      "Mean_Freq_L: Importance 0.0191\n",
      "Variance_L: Importance 0.0967\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9478\n",
      "Balanced Accuracy: 0.9478\n",
      "MCC: 0.8962\n",
      "Log Loss: 0.1306\n",
      "F1 Score: 0.9487\n",
      "Recall: 0.9643\n",
      "Precision: 0.9336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.96      0.93      0.95      9998\n",
      "         CH2       0.93      0.96      0.95      9998\n",
      "\n",
      "    accuracy                           0.95     19996\n",
      "   macro avg       0.95      0.95      0.95     19996\n",
      "weighted avg       0.95      0.95      0.95     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9312  686]\n",
      " [ 357 9641]]\n",
      "False Positive Rate (FPR): 0.0686\n",
      "False Negative Rate (FNR): 0.0357\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1472\n",
      "Min_H: Importance 0.0303\n",
      "Mean_H: Importance 0.1141\n",
      "Mean Deviation_H: Importance 0.0155\n",
      "RMS_H: Importance 0.1951\n",
      "Peak-to-Peak_H: Importance 0.0046\n",
      "Entropy_H: Importance 0.0420\n",
      "Mean_Freq_H: Importance 0.0746\n",
      "Variance_H: Importance 0.0437\n",
      "Max_L: Importance 0.0014\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0907\n",
      "Std_L: Importance 0.0006\n",
      "RMS_L: Importance 0.1313\n",
      "Peak-to-Peak_L: Importance 0.0010\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0009\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0184\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9998\n",
      "Balanced Accuracy: 0.9998\n",
      "MCC: 0.9996\n",
      "Log Loss: 0.0058\n",
      "F1 Score: 0.9998\n",
      "Recall: 1.0000\n",
      "Precision: 0.9996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4996    2]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0004\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1463\n",
      "Min_H: Importance 0.0133\n",
      "Mean_H: Importance 0.1364\n",
      "Std_H: Importance 0.0115\n",
      "Mean Deviation_H: Importance 0.0126\n",
      "RMS_H: Importance 0.1499\n",
      "Entropy_H: Importance 0.0166\n",
      "Mean_Freq_H: Importance 0.0692\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0475\n",
      "Mean_L: Importance 0.1051\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1668\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Centroid_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0293\n",
      "Variance_L: Importance 0.0270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9760\n",
      "Balanced Accuracy: 0.9760\n",
      "MCC: 0.9531\n",
      "Log Loss: 0.0817\n",
      "F1 Score: 0.9766\n",
      "Recall: 1.0000\n",
      "Precision: 0.9542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.95      0.98      3332\n",
      "         CH2       0.95      1.00      0.98      3332\n",
      "\n",
      "    accuracy                           0.98      6664\n",
      "   macro avg       0.98      0.98      0.98      6664\n",
      "weighted avg       0.98      0.98      0.98      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3172  160]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0480\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1030\n",
      "Min_H: Importance 0.0296\n",
      "Mean_H: Importance 0.0901\n",
      "Std_H: Importance 0.0272\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1376\n",
      "Kurtosis_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0028\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1909\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1324\n",
      "Kurtosis_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0299\n",
      "Variance_L: Importance 0.0863\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0085\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1081\n",
      "Min_H: Importance 0.0196\n",
      "Mean_H: Importance 0.1027\n",
      "RMS_H: Importance 0.1425\n",
      "Peak-to-Peak_H: Importance 0.0076\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0288\n",
      "Mean_Freq_H: Importance 0.0501\n",
      "Variance_H: Importance 0.0835\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0441\n",
      "Mean_L: Importance 0.1598\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1222\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0277\n",
      "Spread_L: Importance 0.0002\n",
      "Mean_Freq_L: Importance 0.0098\n",
      "Variance_L: Importance 0.0884\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0265\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1108\n",
      "Min_H: Importance 0.0052\n",
      "Mean_H: Importance 0.0933\n",
      "Std_H: Importance 0.0222\n",
      "Mean Deviation_H: Importance 0.0288\n",
      "RMS_H: Importance 0.1419\n",
      "Skewness_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0160\n",
      "Spread_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0336\n",
      "Irregularity_H: Importance 0.0012\n",
      "Variance_H: Importance 0.0700\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0122\n",
      "Mean_L: Importance 0.1918\n",
      "Mean Deviation_L: Importance 0.0417\n",
      "RMS_L: Importance 0.1277\n",
      "Entropy_L: Importance 0.0091\n",
      "Variance_L: Importance 0.0905\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0137\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1001\n",
      "Mean_H: Importance 0.0833\n",
      "Std_H: Importance 0.0339\n",
      "Mean Deviation_H: Importance 0.0691\n",
      "RMS_H: Importance 0.1260\n",
      "Entropy_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0159\n",
      "Irregularity_H: Importance 0.0117\n",
      "Variance_H: Importance 0.0214\n",
      "Min_L: Importance 0.0386\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1241\n",
      "Entropy_L: Importance 0.0163\n",
      "Variance_L: Importance 0.0832\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0023\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0801\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0185\n",
      "Kurtosis_Freq_H: Importance 0.0109\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0881\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0199\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0144\n",
      "Variance_L: Importance 0.0833\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1336\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0805\n",
      "Mean_H: Importance 0.0828\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.1045\n",
      "Kurtosis_H: Importance 0.0001\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0187\n",
      "Kurtosis_Freq_H: Importance 0.0324\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0796\n",
      "Mean_L: Importance 0.1604\n",
      "Std_L: Importance 0.0202\n",
      "Mean Deviation_L: Importance 0.0802\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0198\n",
      "Mean_Freq_L: Importance 0.0381\n",
      "Variance_L: Importance 0.0602\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0223\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0801\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0599\n",
      "RMS_H: Importance 0.1244\n",
      "Peak-to-Peak_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.0181\n",
      "Kurtosis_Freq_H: Importance 0.0157\n",
      "Irregularity_H: Importance 0.0007\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0401\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0411\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1200\n",
      "Entropy_L: Importance 0.0200\n",
      "Mean_Freq_L: Importance 0.0199\n",
      "Irregularity_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0243\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 3** data and Testing on **ADC 2**\n",
    "To test ADC 3 with changing probe to ADC 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0487\n",
      "Min_H: Importance 0.0563\n",
      "Mean_H: Importance 0.0667\n",
      "Mean Deviation_H: Importance 0.0065\n",
      "RMS_H: Importance 0.1228\n",
      "Skewness_H: Importance 0.0020\n",
      "Peak-to-Peak_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.2617\n",
      "Spread_H: Importance 0.0036\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0383\n",
      "Max_L: Importance 0.0552\n",
      "Min_L: Importance 0.0363\n",
      "Mean_L: Importance 0.0386\n",
      "RMS_L: Importance 0.0186\n",
      "Peak-to-Peak_L: Importance 0.0031\n",
      "Entropy_L: Importance 0.0350\n",
      "Mean_Freq_L: Importance 0.0191\n",
      "Variance_L: Importance 0.0967\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9260\n",
      "Balanced Accuracy: 0.9260\n",
      "MCC: 0.8526\n",
      "Log Loss: 0.6593\n",
      "F1 Score: 0.9245\n",
      "Recall: 0.9068\n",
      "Precision: 0.9430\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.91      0.95      0.93      9998\n",
      "         CH2       0.94      0.91      0.92      9998\n",
      "\n",
      "    accuracy                           0.93     19996\n",
      "   macro avg       0.93      0.93      0.93     19996\n",
      "weighted avg       0.93      0.93      0.93     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9450  548]\n",
      " [ 932 9066]]\n",
      "False Positive Rate (FPR): 0.0548\n",
      "False Negative Rate (FNR): 0.0932\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1472\n",
      "Min_H: Importance 0.0303\n",
      "Mean_H: Importance 0.1141\n",
      "Mean Deviation_H: Importance 0.0155\n",
      "RMS_H: Importance 0.1951\n",
      "Peak-to-Peak_H: Importance 0.0046\n",
      "Entropy_H: Importance 0.0420\n",
      "Mean_Freq_H: Importance 0.0746\n",
      "Variance_H: Importance 0.0437\n",
      "Max_L: Importance 0.0014\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0907\n",
      "Std_L: Importance 0.0006\n",
      "RMS_L: Importance 0.1313\n",
      "Peak-to-Peak_L: Importance 0.0010\n",
      "Entropy_L: Importance 0.0283\n",
      "Spread_L: Importance 0.0009\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0184\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9996\n",
      "Balanced Accuracy: 0.9996\n",
      "MCC: 0.9992\n",
      "Log Loss: 0.0050\n",
      "F1 Score: 0.9996\n",
      "Recall: 0.9994\n",
      "Precision: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4997    1]\n",
      " [   3 4995]]\n",
      "False Positive Rate (FPR): 0.0002\n",
      "False Negative Rate (FNR): 0.0006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1463\n",
      "Min_H: Importance 0.0133\n",
      "Mean_H: Importance 0.1364\n",
      "Std_H: Importance 0.0115\n",
      "Mean Deviation_H: Importance 0.0126\n",
      "RMS_H: Importance 0.1499\n",
      "Entropy_H: Importance 0.0166\n",
      "Mean_Freq_H: Importance 0.0692\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0475\n",
      "Mean_L: Importance 0.1051\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1668\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Centroid_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0293\n",
      "Variance_L: Importance 0.0270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9818\n",
      "Balanced Accuracy: 0.9818\n",
      "MCC: 0.9640\n",
      "Log Loss: 0.0929\n",
      "F1 Score: 0.9816\n",
      "Recall: 0.9700\n",
      "Precision: 0.9935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.97      0.99      0.98      3331\n",
      "         CH2       0.99      0.97      0.98      3332\n",
      "\n",
      "    accuracy                           0.98      6663\n",
      "   macro avg       0.98      0.98      0.98      6663\n",
      "weighted avg       0.98      0.98      0.98      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3310   21]\n",
      " [ 100 3232]]\n",
      "False Positive Rate (FPR): 0.0063\n",
      "False Negative Rate (FNR): 0.0300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1030\n",
      "Min_H: Importance 0.0296\n",
      "Mean_H: Importance 0.0901\n",
      "Std_H: Importance 0.0272\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1376\n",
      "Kurtosis_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0028\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1909\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1324\n",
      "Kurtosis_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0299\n",
      "Variance_L: Importance 0.0863\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0021\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1081\n",
      "Min_H: Importance 0.0196\n",
      "Mean_H: Importance 0.1027\n",
      "RMS_H: Importance 0.1425\n",
      "Peak-to-Peak_H: Importance 0.0076\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0288\n",
      "Mean_Freq_H: Importance 0.0501\n",
      "Variance_H: Importance 0.0835\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0441\n",
      "Mean_L: Importance 0.1598\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1222\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0277\n",
      "Spread_L: Importance 0.0002\n",
      "Mean_Freq_L: Importance 0.0098\n",
      "Variance_L: Importance 0.0884\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0071\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1108\n",
      "Min_H: Importance 0.0052\n",
      "Mean_H: Importance 0.0933\n",
      "Std_H: Importance 0.0222\n",
      "Mean Deviation_H: Importance 0.0288\n",
      "RMS_H: Importance 0.1419\n",
      "Skewness_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0160\n",
      "Spread_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0336\n",
      "Irregularity_H: Importance 0.0012\n",
      "Variance_H: Importance 0.0700\n",
      "Max_L: Importance 0.0024\n",
      "Min_L: Importance 0.0122\n",
      "Mean_L: Importance 0.1918\n",
      "Mean Deviation_L: Importance 0.0417\n",
      "RMS_L: Importance 0.1277\n",
      "Entropy_L: Importance 0.0091\n",
      "Variance_L: Importance 0.0905\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0020\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1001\n",
      "Mean_H: Importance 0.0833\n",
      "Std_H: Importance 0.0339\n",
      "Mean Deviation_H: Importance 0.0691\n",
      "RMS_H: Importance 0.1260\n",
      "Entropy_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0159\n",
      "Irregularity_H: Importance 0.0117\n",
      "Variance_H: Importance 0.0214\n",
      "Min_L: Importance 0.0386\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1241\n",
      "Entropy_L: Importance 0.0163\n",
      "Variance_L: Importance 0.0832\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0075\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0801\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0185\n",
      "Kurtosis_Freq_H: Importance 0.0109\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0881\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0199\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1215\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0144\n",
      "Variance_L: Importance 0.0833\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0347\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0805\n",
      "Mean_H: Importance 0.0828\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.1045\n",
      "Kurtosis_H: Importance 0.0001\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0187\n",
      "Kurtosis_Freq_H: Importance 0.0324\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0796\n",
      "Mean_L: Importance 0.1604\n",
      "Std_L: Importance 0.0202\n",
      "Mean Deviation_L: Importance 0.0802\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0198\n",
      "Mean_Freq_L: Importance 0.0381\n",
      "Variance_L: Importance 0.0602\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0025\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (332, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0801\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0599\n",
      "RMS_H: Importance 0.1244\n",
      "Peak-to-Peak_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0399\n",
      "Mean_Freq_H: Importance 0.0181\n",
      "Kurtosis_Freq_H: Importance 0.0157\n",
      "Irregularity_H: Importance 0.0007\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0401\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0411\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1200\n",
      "Entropy_L: Importance 0.0200\n",
      "Mean_Freq_L: Importance 0.0199\n",
      "Irregularity_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0124\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 1**\n",
    "To conduct experiments, **Probe 2** (SMA cable which transfers data) is used consistently throughout. **Probe 2** is kept constant, and the machine learning model is **always trained on ADC 1** data. The model is then tested on **ADC 2** and **ADC 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 1** data and Testing on **ADC 2** data\n",
    "To test ADC 1 with changing probe to ADC 2 and ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0178\n",
      "Mean_H: Importance 0.0563\n",
      "Std_H: Importance 0.0118\n",
      "Mean Deviation_H: Importance 0.0153\n",
      "RMS_H: Importance 0.1167\n",
      "Centroid_H: Importance 0.0096\n",
      "Entropy_H: Importance 0.0320\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Irregularity_H: Importance 0.0143\n",
      "Variance_H: Importance 0.0354\n",
      "Max_L: Importance 0.0187\n",
      "Min_L: Importance 0.0724\n",
      "Mean_L: Importance 0.1135\n",
      "RMS_L: Importance 0.1859\n",
      "Skewness_L: Importance 0.0125\n",
      "Centroid_L: Importance 0.0103\n",
      "Entropy_L: Importance 0.0303\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0820\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9755\n",
      "Balanced Accuracy: 0.9755\n",
      "MCC: 0.9522\n",
      "Log Loss: 0.0644\n",
      "F1 Score: 0.9750\n",
      "Recall: 0.9519\n",
      "Precision: 0.9992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.95      1.00      0.98      9998\n",
      "         CH2       1.00      0.95      0.97      9998\n",
      "\n",
      "    accuracy                           0.98     19996\n",
      "   macro avg       0.98      0.98      0.98     19996\n",
      "weighted avg       0.98      0.98      0.98     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9990    8]\n",
      " [ 481 9517]]\n",
      "False Positive Rate (FPR): 0.0008\n",
      "False Negative Rate (FNR): 0.0481\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1194\n",
      "Min_H: Importance 0.0143\n",
      "Mean_H: Importance 0.1108\n",
      "Mean Deviation_H: Importance 0.0075\n",
      "RMS_H: Importance 0.2006\n",
      "Entropy_H: Importance 0.0357\n",
      "Mean_Freq_H: Importance 0.0776\n",
      "Variance_H: Importance 0.0614\n",
      "Max_L: Importance 0.0141\n",
      "Min_L: Importance 0.0410\n",
      "Mean_L: Importance 0.1045\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1372\n",
      "Skewness_L: Importance 0.0021\n",
      "Kurtosis_L: Importance 0.0013\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0486\n",
      "Mean_Freq_L: Importance 0.0137\n",
      "Variance_L: Importance 0.0078\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9693\n",
      "Balanced Accuracy: 0.9693\n",
      "MCC: 0.9404\n",
      "Log Loss: 0.0367\n",
      "F1 Score: 0.9683\n",
      "Recall: 0.9386\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.94      1.00      0.97      4998\n",
      "         CH2       1.00      0.94      0.97      4998\n",
      "\n",
      "    accuracy                           0.97      9996\n",
      "   macro avg       0.97      0.97      0.97      9996\n",
      "weighted avg       0.97      0.97      0.97      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [ 307 4691]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0614\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0676\n",
      "Min_H: Importance 0.0269\n",
      "Mean_H: Importance 0.1364\n",
      "Mean Deviation_H: Importance 0.0034\n",
      "RMS_H: Importance 0.2181\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0171\n",
      "Mean_Freq_H: Importance 0.0392\n",
      "Irregularity_H: Importance 0.0009\n",
      "Variance_H: Importance 0.0507\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1537\n",
      "RMS_L: Importance 0.1082\n",
      "Kurtosis_L: Importance 0.0019\n",
      "Entropy_L: Importance 0.0264\n",
      "Mean_Freq_L: Importance 0.0095\n",
      "Irregularity_L: Importance 0.0008\n",
      "Variance_L: Importance 0.0949\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9658\n",
      "Balanced Accuracy: 0.9658\n",
      "MCC: 0.9338\n",
      "Log Loss: 0.0859\n",
      "F1 Score: 0.9646\n",
      "Recall: 0.9316\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.94      1.00      0.97      3331\n",
      "         CH2       1.00      0.93      0.96      3332\n",
      "\n",
      "    accuracy                           0.97      6663\n",
      "   macro avg       0.97      0.97      0.97      6663\n",
      "weighted avg       0.97      0.97      0.97      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [ 228 3104]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0684\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0664\n",
      "Min_H: Importance 0.0285\n",
      "Mean_H: Importance 0.0956\n",
      "Mean Deviation_H: Importance 0.0158\n",
      "RMS_H: Importance 0.1320\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Irregularity_H: Importance 0.0000\n",
      "Variance_H: Importance 0.0610\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.2098\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1206\n",
      "Entropy_L: Importance 0.0309\n",
      "Mean_Freq_L: Importance 0.0122\n",
      "Variance_L: Importance 0.1276\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0022\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0682\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.0993\n",
      "Std_H: Importance 0.0100\n",
      "Mean Deviation_H: Importance 0.0238\n",
      "RMS_H: Importance 0.1287\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0283\n",
      "Spread_H: Importance 0.0072\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0133\n",
      "Mean_L: Importance 0.1748\n",
      "Mean Deviation_L: Importance 0.0109\n",
      "RMS_L: Importance 0.1363\n",
      "Entropy_L: Importance 0.0447\n",
      "Mean_Freq_L: Importance 0.0143\n",
      "Variance_L: Importance 0.1081\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1078\n",
      "Min_H: Importance 0.0001\n",
      "Mean_H: Importance 0.0924\n",
      "Mean Deviation_H: Importance 0.0135\n",
      "RMS_H: Importance 0.1310\n",
      "Entropy_H: Importance 0.0146\n",
      "Mean_Freq_H: Importance 0.0398\n",
      "Kurtosis_Freq_H: Importance 0.0045\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0601\n",
      "Max_L: Importance 0.0009\n",
      "Min_L: Importance 0.0966\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0169\n",
      "RMS_L: Importance 0.1265\n",
      "Peak-to-Peak_L: Importance 0.0011\n",
      "Entropy_L: Importance 0.0139\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0873\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0035\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0848\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0811\n",
      "RMS_H: Importance 0.0635\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0154\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0802\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0806\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0194\n",
      "Mean_Freq_H: Importance 0.0184\n",
      "Kurtosis_Freq_H: Importance 0.0080\n",
      "Variance_H: Importance 0.0201\n",
      "Min_L: Importance 0.0732\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1216\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1019\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0400\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.0800\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9987\n",
      "Balanced Accuracy: 0.9987\n",
      "MCC: 0.9975\n",
      "Log Loss: 0.1699\n",
      "F1 Score: 0.9987\n",
      "Recall: 0.9975\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  1 397]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0174\n",
      "Min_L: Importance 0.0024\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0802\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9955\n",
      "Balanced Accuracy: 0.9955\n",
      "MCC: 0.9910\n",
      "Log Loss: 0.1852\n",
      "F1 Score: 0.9954\n",
      "Recall: 0.9909\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      1.00      1.00       331\n",
      "         CH2       1.00      0.99      1.00       331\n",
      "\n",
      "    accuracy                           1.00       662\n",
      "   macro avg       1.00      1.00      1.00       662\n",
      "weighted avg       1.00      1.00      1.00       662\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  3 328]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0091\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 1** data and Testing on **ADC 3** data\n",
    "To test ADC 1 with changing probe to ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0178\n",
      "Mean_H: Importance 0.0563\n",
      "Std_H: Importance 0.0118\n",
      "Mean Deviation_H: Importance 0.0153\n",
      "RMS_H: Importance 0.1167\n",
      "Centroid_H: Importance 0.0096\n",
      "Entropy_H: Importance 0.0320\n",
      "Mean_Freq_H: Importance 0.0574\n",
      "Irregularity_H: Importance 0.0143\n",
      "Variance_H: Importance 0.0354\n",
      "Max_L: Importance 0.0187\n",
      "Min_L: Importance 0.0724\n",
      "Mean_L: Importance 0.1135\n",
      "RMS_L: Importance 0.1859\n",
      "Skewness_L: Importance 0.0125\n",
      "Centroid_L: Importance 0.0103\n",
      "Entropy_L: Importance 0.0303\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0820\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9583\n",
      "Balanced Accuracy: 0.9583\n",
      "MCC: 0.9192\n",
      "Log Loss: 0.0842\n",
      "F1 Score: 0.9598\n",
      "Recall: 0.9950\n",
      "Precision: 0.9270\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.92      0.96      9998\n",
      "         CH2       0.93      0.99      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9215  783]\n",
      " [  50 9948]]\n",
      "False Positive Rate (FPR): 0.0783\n",
      "False Negative Rate (FNR): 0.0050\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1194\n",
      "Min_H: Importance 0.0143\n",
      "Mean_H: Importance 0.1108\n",
      "Mean Deviation_H: Importance 0.0075\n",
      "RMS_H: Importance 0.2006\n",
      "Entropy_H: Importance 0.0357\n",
      "Mean_Freq_H: Importance 0.0776\n",
      "Variance_H: Importance 0.0614\n",
      "Max_L: Importance 0.0141\n",
      "Min_L: Importance 0.0410\n",
      "Mean_L: Importance 0.1045\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1372\n",
      "Skewness_L: Importance 0.0021\n",
      "Kurtosis_L: Importance 0.0013\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0486\n",
      "Mean_Freq_L: Importance 0.0137\n",
      "Variance_L: Importance 0.0078\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9998\n",
      "Balanced Accuracy: 0.9998\n",
      "MCC: 0.9996\n",
      "Log Loss: 0.0032\n",
      "F1 Score: 0.9998\n",
      "Recall: 0.9996\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   2 4996]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0676\n",
      "Min_H: Importance 0.0269\n",
      "Mean_H: Importance 0.1364\n",
      "Mean Deviation_H: Importance 0.0034\n",
      "RMS_H: Importance 0.2181\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0171\n",
      "Mean_Freq_H: Importance 0.0392\n",
      "Irregularity_H: Importance 0.0009\n",
      "Variance_H: Importance 0.0507\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1537\n",
      "RMS_L: Importance 0.1082\n",
      "Kurtosis_L: Importance 0.0019\n",
      "Entropy_L: Importance 0.0264\n",
      "Mean_Freq_L: Importance 0.0095\n",
      "Irregularity_L: Importance 0.0008\n",
      "Variance_L: Importance 0.0949\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0167\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6664\n",
      "   macro avg       1.00      1.00      1.00      6664\n",
      "weighted avg       1.00      1.00      1.00      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0664\n",
      "Min_H: Importance 0.0285\n",
      "Mean_H: Importance 0.0956\n",
      "Mean Deviation_H: Importance 0.0158\n",
      "RMS_H: Importance 0.1320\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Irregularity_H: Importance 0.0000\n",
      "Variance_H: Importance 0.0610\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0283\n",
      "Mean_L: Importance 0.2098\n",
      "Mean Deviation_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1206\n",
      "Entropy_L: Importance 0.0309\n",
      "Mean_Freq_L: Importance 0.0122\n",
      "Variance_L: Importance 0.1276\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0009\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0682\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.0993\n",
      "Std_H: Importance 0.0100\n",
      "Mean Deviation_H: Importance 0.0238\n",
      "RMS_H: Importance 0.1287\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0283\n",
      "Spread_H: Importance 0.0072\n",
      "Mean_Freq_H: Importance 0.0567\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0006\n",
      "Min_L: Importance 0.0133\n",
      "Mean_L: Importance 0.1748\n",
      "Mean Deviation_L: Importance 0.0109\n",
      "RMS_L: Importance 0.1363\n",
      "Entropy_L: Importance 0.0447\n",
      "Mean_Freq_L: Importance 0.0143\n",
      "Variance_L: Importance 0.1081\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0029\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1078\n",
      "Min_H: Importance 0.0001\n",
      "Mean_H: Importance 0.0924\n",
      "Mean Deviation_H: Importance 0.0135\n",
      "RMS_H: Importance 0.1310\n",
      "Entropy_H: Importance 0.0146\n",
      "Mean_Freq_H: Importance 0.0398\n",
      "Kurtosis_Freq_H: Importance 0.0045\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0601\n",
      "Max_L: Importance 0.0009\n",
      "Min_L: Importance 0.0966\n",
      "Mean_L: Importance 0.1804\n",
      "Mean Deviation_L: Importance 0.0169\n",
      "RMS_L: Importance 0.1265\n",
      "Peak-to-Peak_L: Importance 0.0011\n",
      "Entropy_L: Importance 0.0139\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0873\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0023\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0848\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0811\n",
      "RMS_H: Importance 0.0635\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0161\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0154\n",
      "Variance_H: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0802\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0997\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.1257\n",
      "Entropy_H: Importance 0.0194\n",
      "Mean_Freq_H: Importance 0.0184\n",
      "Kurtosis_Freq_H: Importance 0.0080\n",
      "Variance_H: Importance 0.0201\n",
      "Min_L: Importance 0.0732\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1216\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0126\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1224\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0400\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.0800\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0923\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0174\n",
      "Min_L: Importance 0.0024\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0802\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0427\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 2**\n",
    "To conduct experiments, **Probe 2** (SMA cable which trandfers data) is used consistently throughout. **Probe 2** is kept constant, and the machine learning model is **always trained on ADC 2**. The model is then tested on **ADC 3** and **ADC 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 2** data and Testing on **ADC 3**\n",
    "To test ADC 2 with changing probe to ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0768\n",
      "RMS_H: Importance 0.1119\n",
      "Skewness_H: Importance 0.0067\n",
      "Entropy_H: Importance 0.0116\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Variance_H: Importance 0.0360\n",
      "Max_L: Importance 0.0059\n",
      "Min_L: Importance 0.1096\n",
      "Mean_L: Importance 0.1263\n",
      "Std_L: Importance 0.0059\n",
      "RMS_L: Importance 0.2135\n",
      "Skewness_L: Importance 0.0179\n",
      "Peak-to-Peak_L: Importance 0.0129\n",
      "Entropy_L: Importance 0.0403\n",
      "Spread_L: Importance 0.0075\n",
      "Mean_Freq_L: Importance 0.0380\n",
      "Irregularity_L: Importance 0.0114\n",
      "Variance_L: Importance 0.0734\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9656\n",
      "Balanced Accuracy: 0.9656\n",
      "MCC: 0.9334\n",
      "Log Loss: 0.0762\n",
      "F1 Score: 0.9667\n",
      "Recall: 1.0000\n",
      "Precision: 0.9356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.93      0.96      9998\n",
      "         CH2       0.94      1.00      0.97      9998\n",
      "\n",
      "    accuracy                           0.97     19996\n",
      "   macro avg       0.97      0.97      0.97     19996\n",
      "weighted avg       0.97      0.97      0.97     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9310  688]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.0688\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0350\n",
      "Mean_H: Importance 0.0743\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1520\n",
      "Peak-to-Peak_H: Importance 0.0048\n",
      "Centroid_H: Importance 0.0013\n",
      "Entropy_H: Importance 0.0346\n",
      "Spread_H: Importance 0.0016\n",
      "Mean_Freq_H: Importance 0.0758\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0140\n",
      "Min_L: Importance 0.0245\n",
      "Mean_L: Importance 0.1090\n",
      "RMS_L: Importance 0.2406\n",
      "Entropy_L: Importance 0.0278\n",
      "Skewness_Freq_L: Importance 0.0013\n",
      "Mean_Freq_L: Importance 0.0078\n",
      "Variance_L: Importance 0.0092\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0032\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1627\n",
      "Min_H: Importance 0.0243\n",
      "Mean_H: Importance 0.0740\n",
      "Mean Deviation_H: Importance 0.0027\n",
      "RMS_H: Importance 0.1712\n",
      "Entropy_H: Importance 0.0154\n",
      "Spread_H: Importance 0.0019\n",
      "Skewness_Freq_H: Importance 0.0019\n",
      "Mean_Freq_H: Importance 0.0451\n",
      "Variance_H: Importance 0.0392\n",
      "Min_L: Importance 0.0149\n",
      "Mean_L: Importance 0.1459\n",
      "RMS_L: Importance 0.2334\n",
      "Skewness_L: Importance 0.0181\n",
      "Kurtosis_L: Importance 0.0035\n",
      "Peak-to-Peak_L: Importance 0.0027\n",
      "Entropy_L: Importance 0.0114\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Variance_L: Importance 0.0216\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0082\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3332\n",
      "\n",
      "    accuracy                           1.00      6664\n",
      "   macro avg       1.00      1.00      1.00      6664\n",
      "weighted avg       1.00      1.00      1.00      6664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0663\n",
      "Min_H: Importance 0.0261\n",
      "Mean_H: Importance 0.0912\n",
      "Std_H: Importance 0.0218\n",
      "Mean Deviation_H: Importance 0.0151\n",
      "RMS_H: Importance 0.1408\n",
      "Centroid_H: Importance 0.0027\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0598\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1973\n",
      "RMS_L: Importance 0.1267\n",
      "Entropy_L: Importance 0.0290\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.1266\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0015\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0615\n",
      "Min_H: Importance 0.0399\n",
      "Mean_H: Importance 0.0919\n",
      "Mean Deviation_H: Importance 0.0249\n",
      "RMS_H: Importance 0.1047\n",
      "Peak-to-Peak_H: Importance 0.0002\n",
      "Centroid_H: Importance 0.0003\n",
      "Entropy_H: Importance 0.0158\n",
      "Mean_Freq_H: Importance 0.0409\n",
      "Variance_H: Importance 0.0604\n",
      "Max_L: Importance 0.0273\n",
      "Min_L: Importance 0.0908\n",
      "Mean_L: Importance 0.1409\n",
      "Mean Deviation_L: Importance 0.0036\n",
      "RMS_L: Importance 0.1265\n",
      "Kurtosis_L: Importance 0.0036\n",
      "Entropy_L: Importance 0.0326\n",
      "Mean_Freq_L: Importance 0.0139\n",
      "Variance_L: Importance 0.1202\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0056\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1176\n",
      "Min_H: Importance 0.0039\n",
      "Mean_H: Importance 0.1050\n",
      "Mean Deviation_H: Importance 0.0073\n",
      "RMS_H: Importance 0.1557\n",
      "Centroid_H: Importance 0.0071\n",
      "Entropy_H: Importance 0.0103\n",
      "Spread_H: Importance 0.0064\n",
      "Mean_Freq_H: Importance 0.0152\n",
      "Variance_H: Importance 0.0967\n",
      "Max_L: Importance 0.0013\n",
      "Min_L: Importance 0.0797\n",
      "Mean_L: Importance 0.1486\n",
      "Mean Deviation_L: Importance 0.0202\n",
      "RMS_L: Importance 0.1366\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0088\n",
      "Mean_Freq_L: Importance 0.0069\n",
      "Variance_L: Importance 0.0689\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0223\n",
      "RMS_H: Importance 0.0814\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9579\n",
      "Balanced Accuracy: 0.9579\n",
      "MCC: 0.9191\n",
      "Log Loss: 0.1628\n",
      "F1 Score: 0.9596\n",
      "Recall: 1.0000\n",
      "Precision: 0.9223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.92      0.96       665\n",
      "         CH2       0.92      1.00      0.96       665\n",
      "\n",
      "    accuracy                           0.96      1330\n",
      "   macro avg       0.96      0.96      0.96      1330\n",
      "weighted avg       0.96      0.96      0.96      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[609  56]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0842\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0826\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0834\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0399\n",
      "Kurtosis_Freq_H: Importance 0.0166\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0199\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0255\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Min_H: Importance 0.0189\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0811\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.0400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0200\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0842\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0838\n",
      "Std_H: Importance 0.0200\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0400\n",
      "Variance_H: Importance 0.0229\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0133\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0585\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 2** data and Testing on **ADC 1**\n",
    "To test ADC 2 with changing probe to ADC 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0768\n",
      "RMS_H: Importance 0.1119\n",
      "Skewness_H: Importance 0.0067\n",
      "Entropy_H: Importance 0.0116\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Variance_H: Importance 0.0360\n",
      "Max_L: Importance 0.0059\n",
      "Min_L: Importance 0.1096\n",
      "Mean_L: Importance 0.1263\n",
      "Std_L: Importance 0.0059\n",
      "RMS_L: Importance 0.2135\n",
      "Skewness_L: Importance 0.0179\n",
      "Peak-to-Peak_L: Importance 0.0129\n",
      "Entropy_L: Importance 0.0403\n",
      "Spread_L: Importance 0.0075\n",
      "Mean_Freq_L: Importance 0.0380\n",
      "Irregularity_L: Importance 0.0114\n",
      "Variance_L: Importance 0.0734\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9144\n",
      "Balanced Accuracy: 0.9144\n",
      "MCC: 0.8407\n",
      "Log Loss: 0.5116\n",
      "F1 Score: 0.9210\n",
      "Recall: 0.9980\n",
      "Precision: 0.8551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.83      0.91      9998\n",
      "         CH2       0.86      1.00      0.92      9998\n",
      "\n",
      "    accuracy                           0.91     19996\n",
      "   macro avg       0.93      0.91      0.91     19996\n",
      "weighted avg       0.93      0.91      0.91     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8307 1691]\n",
      " [  20 9978]]\n",
      "False Positive Rate (FPR): 0.1691\n",
      "False Negative Rate (FNR): 0.0020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0350\n",
      "Mean_H: Importance 0.0743\n",
      "Mean Deviation_H: Importance 0.0067\n",
      "RMS_H: Importance 0.1520\n",
      "Peak-to-Peak_H: Importance 0.0048\n",
      "Centroid_H: Importance 0.0013\n",
      "Entropy_H: Importance 0.0346\n",
      "Spread_H: Importance 0.0016\n",
      "Mean_Freq_H: Importance 0.0758\n",
      "Variance_H: Importance 0.0625\n",
      "Max_L: Importance 0.0140\n",
      "Min_L: Importance 0.0245\n",
      "Mean_L: Importance 0.1090\n",
      "RMS_L: Importance 0.2406\n",
      "Entropy_L: Importance 0.0278\n",
      "Skewness_Freq_L: Importance 0.0013\n",
      "Mean_Freq_L: Importance 0.0078\n",
      "Variance_L: Importance 0.0092\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9872\n",
      "Balanced Accuracy: 0.9872\n",
      "MCC: 0.9747\n",
      "Log Loss: 0.0368\n",
      "F1 Score: 0.9874\n",
      "Recall: 1.0000\n",
      "Precision: 0.9750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.97      0.99      4998\n",
      "         CH2       0.98      1.00      0.99      4998\n",
      "\n",
      "    accuracy                           0.99      9996\n",
      "   macro avg       0.99      0.99      0.99      9996\n",
      "weighted avg       0.99      0.99      0.99      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4870  128]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0256\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1627\n",
      "Min_H: Importance 0.0243\n",
      "Mean_H: Importance 0.0740\n",
      "Mean Deviation_H: Importance 0.0027\n",
      "RMS_H: Importance 0.1712\n",
      "Entropy_H: Importance 0.0154\n",
      "Spread_H: Importance 0.0019\n",
      "Skewness_Freq_H: Importance 0.0019\n",
      "Mean_Freq_H: Importance 0.0451\n",
      "Variance_H: Importance 0.0392\n",
      "Min_L: Importance 0.0149\n",
      "Mean_L: Importance 0.1459\n",
      "RMS_L: Importance 0.2334\n",
      "Skewness_L: Importance 0.0181\n",
      "Kurtosis_L: Importance 0.0035\n",
      "Peak-to-Peak_L: Importance 0.0027\n",
      "Entropy_L: Importance 0.0114\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Variance_L: Importance 0.0216\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9604\n",
      "Balanced Accuracy: 0.9604\n",
      "MCC: 0.9237\n",
      "Log Loss: 0.0824\n",
      "F1 Score: 0.9619\n",
      "Recall: 1.0000\n",
      "Precision: 0.9266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.92      0.96      3332\n",
      "         CH2       0.93      1.00      0.96      3331\n",
      "\n",
      "    accuracy                           0.96      6663\n",
      "   macro avg       0.96      0.96      0.96      6663\n",
      "weighted avg       0.96      0.96      0.96      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3068  264]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.0792\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0663\n",
      "Min_H: Importance 0.0261\n",
      "Mean_H: Importance 0.0912\n",
      "Std_H: Importance 0.0218\n",
      "Mean Deviation_H: Importance 0.0151\n",
      "RMS_H: Importance 0.1408\n",
      "Centroid_H: Importance 0.0027\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0598\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0001\n",
      "Min_L: Importance 0.0154\n",
      "Mean_L: Importance 0.1973\n",
      "RMS_L: Importance 0.1267\n",
      "Entropy_L: Importance 0.0290\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.1266\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0018\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0615\n",
      "Min_H: Importance 0.0399\n",
      "Mean_H: Importance 0.0919\n",
      "Mean Deviation_H: Importance 0.0249\n",
      "RMS_H: Importance 0.1047\n",
      "Peak-to-Peak_H: Importance 0.0002\n",
      "Centroid_H: Importance 0.0003\n",
      "Entropy_H: Importance 0.0158\n",
      "Mean_Freq_H: Importance 0.0409\n",
      "Variance_H: Importance 0.0604\n",
      "Max_L: Importance 0.0273\n",
      "Min_L: Importance 0.0908\n",
      "Mean_L: Importance 0.1409\n",
      "Mean Deviation_L: Importance 0.0036\n",
      "RMS_L: Importance 0.1265\n",
      "Kurtosis_L: Importance 0.0036\n",
      "Entropy_L: Importance 0.0326\n",
      "Mean_Freq_L: Importance 0.0139\n",
      "Variance_L: Importance 0.1202\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0068\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1176\n",
      "Min_H: Importance 0.0039\n",
      "Mean_H: Importance 0.1050\n",
      "Mean Deviation_H: Importance 0.0073\n",
      "RMS_H: Importance 0.1557\n",
      "Centroid_H: Importance 0.0071\n",
      "Entropy_H: Importance 0.0103\n",
      "Spread_H: Importance 0.0064\n",
      "Mean_Freq_H: Importance 0.0152\n",
      "Variance_H: Importance 0.0967\n",
      "Max_L: Importance 0.0013\n",
      "Min_L: Importance 0.0797\n",
      "Mean_L: Importance 0.1486\n",
      "Mean Deviation_L: Importance 0.0202\n",
      "RMS_L: Importance 0.1366\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Entropy_L: Importance 0.0088\n",
      "Mean_Freq_L: Importance 0.0069\n",
      "Variance_L: Importance 0.0689\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0223\n",
      "RMS_H: Importance 0.0814\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9602\n",
      "Balanced Accuracy: 0.9602\n",
      "MCC: 0.9232\n",
      "Log Loss: 0.1591\n",
      "F1 Score: 0.9617\n",
      "Recall: 1.0000\n",
      "Precision: 0.9262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.92      0.96       665\n",
      "         CH2       0.93      1.00      0.96       665\n",
      "\n",
      "    accuracy                           0.96      1330\n",
      "   macro avg       0.96      0.96      0.96      1330\n",
      "weighted avg       0.96      0.96      0.96      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[612  53]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0797\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0800\n",
      "Mean_H: Importance 0.0826\n",
      "Std_H: Importance 0.1000\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0834\n",
      "Entropy_H: Importance 0.1200\n",
      "Spread_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0399\n",
      "Kurtosis_Freq_H: Importance 0.0166\n",
      "Irregularity_H: Importance 0.0001\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0200\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0199\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1984\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2 # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Min_H: Importance 0.0189\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0811\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.0400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0200\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9987\n",
      "Balanced Accuracy: 0.9987\n",
      "MCC: 0.9975\n",
      "Log Loss: 0.1500\n",
      "F1 Score: 0.9987\n",
      "Recall: 1.0000\n",
      "Precision: 0.9975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[397   1]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0025\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0838\n",
      "Std_H: Importance 0.0200\n",
      "Mean Deviation_H: Importance 0.0600\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0800\n",
      "Kurtosis_Freq_H: Importance 0.0400\n",
      "Variance_H: Importance 0.0229\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1400\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0133\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1149\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 3**\n",
    "To conduct experiments, **Probe 2** (SMA cable which trandfers data) is used consistently throughout. **Probe 2** is kept constant, and the machine learning model is **always trained on ADC 3**. The model is then tested on **ADC 1** and **ADC 2** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 3** data and Testing on **ADC 1**\n",
    "To test ADC 3 with changing probe to ADC 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0320\n",
      "Min_H: Importance 0.0522\n",
      "Mean_H: Importance 0.1101\n",
      "Mean Deviation_H: Importance 0.0019\n",
      "RMS_H: Importance 0.1822\n",
      "Peak-to-Peak_H: Importance 0.0037\n",
      "Entropy_H: Importance 0.1499\n",
      "Spread_H: Importance 0.0031\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0194\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0582\n",
      "RMS_L: Importance 0.0378\n",
      "Skewness_L: Importance 0.0034\n",
      "Entropy_L: Importance 0.0394\n",
      "Mean_Freq_L: Importance 0.0220\n",
      "Irregularity_L: Importance 0.0045\n",
      "Variance_L: Importance 0.0828\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9319\n",
      "Balanced Accuracy: 0.9319\n",
      "MCC: 0.8681\n",
      "Log Loss: 0.2459\n",
      "F1 Score: 0.9351\n",
      "Recall: 0.9819\n",
      "Precision: 0.8926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      0.88      0.93      9998\n",
      "         CH2       0.89      0.98      0.94      9998\n",
      "\n",
      "    accuracy                           0.93     19996\n",
      "   macro avg       0.94      0.93      0.93     19996\n",
      "weighted avg       0.94      0.93      0.93     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8817 1181]\n",
      " [ 181 9817]]\n",
      "False Positive Rate (FPR): 0.1181\n",
      "False Negative Rate (FNR): 0.0181\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0825\n",
      "Min_H: Importance 0.0317\n",
      "Mean_H: Importance 0.1433\n",
      "Mean Deviation_H: Importance 0.0046\n",
      "RMS_H: Importance 0.2186\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0184\n",
      "Mean_Freq_H: Importance 0.0679\n",
      "Variance_H: Importance 0.0391\n",
      "Max_L: Importance 0.0008\n",
      "Min_L: Importance 0.0281\n",
      "Mean_L: Importance 0.1291\n",
      "Std_L: Importance 0.0007\n",
      "RMS_L: Importance 0.1220\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0025\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0160\n",
      "Variance_L: Importance 0.0615\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9990\n",
      "Balanced Accuracy: 0.9990\n",
      "MCC: 0.9980\n",
      "Log Loss: 0.0131\n",
      "F1 Score: 0.9990\n",
      "Recall: 1.0000\n",
      "Precision: 0.9980\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4988   10]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0020\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1070\n",
      "Min_H: Importance 0.0266\n",
      "Mean_H: Importance 0.1418\n",
      "Mean Deviation_H: Importance 0.0104\n",
      "RMS_H: Importance 0.1501\n",
      "Centroid_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0169\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0627\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0279\n",
      "Mean_L: Importance 0.1151\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1620\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0302\n",
      "Mean_Freq_L: Importance 0.0102\n",
      "Variance_L: Importance 0.0685\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9422\n",
      "Balanced Accuracy: 0.9422\n",
      "MCC: 0.8904\n",
      "Log Loss: 0.1310\n",
      "F1 Score: 0.9454\n",
      "Recall: 1.0000\n",
      "Precision: 0.8964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.88      0.94      3332\n",
      "         CH2       0.90      1.00      0.95      3331\n",
      "\n",
      "    accuracy                           0.94      6663\n",
      "   macro avg       0.95      0.94      0.94      6663\n",
      "weighted avg       0.95      0.94      0.94      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2947  385]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.1155\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0634\n",
      "Min_H: Importance 0.0298\n",
      "Mean_H: Importance 0.0915\n",
      "Std_H: Importance 0.0221\n",
      "Mean Deviation_H: Importance 0.0164\n",
      "RMS_H: Importance 0.1400\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0141\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1975\n",
      "RMS_L: Importance 0.1205\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0293\n",
      "Mean_Freq_L: Importance 0.0045\n",
      "Variance_L: Importance 0.1270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0016\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0669\n",
      "Min_H: Importance 0.0105\n",
      "Mean_H: Importance 0.0930\n",
      "Std_H: Importance 0.0183\n",
      "RMS_H: Importance 0.1493\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0313\n",
      "Mean_Freq_H: Importance 0.0492\n",
      "Variance_H: Importance 0.0832\n",
      "Max_L: Importance 0.0121\n",
      "Min_L: Importance 0.0222\n",
      "Mean_L: Importance 0.1501\n",
      "Mean Deviation_L: Importance 0.0117\n",
      "RMS_L: Importance 0.1342\n",
      "Kurtosis_L: Importance 0.0022\n",
      "Centroid_L: Importance 0.0014\n",
      "Entropy_L: Importance 0.0419\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0027\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0042\n",
      "Mean_H: Importance 0.1130\n",
      "Std_H: Importance 0.0109\n",
      "Mean Deviation_H: Importance 0.0261\n",
      "RMS_H: Importance 0.1467\n",
      "Entropy_H: Importance 0.0170\n",
      "Spread_H: Importance 0.0052\n",
      "Mean_Freq_H: Importance 0.0232\n",
      "Variance_H: Importance 0.0819\n",
      "Max_L: Importance 0.0041\n",
      "Min_L: Importance 0.0677\n",
      "Mean_L: Importance 0.1654\n",
      "Mean Deviation_L: Importance 0.0099\n",
      "RMS_L: Importance 0.1270\n",
      "Peak-to-Peak_L: Importance 0.0016\n",
      "Centroid_L: Importance 0.0013\n",
      "Entropy_L: Importance 0.0100\n",
      "Variance_L: Importance 0.0709\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1035\n",
      "Mean_H: Importance 0.0925\n",
      "Std_H: Importance 0.0121\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1071\n",
      "Entropy_H: Importance 0.0152\n",
      "Skewness_Freq_H: Importance 0.0012\n",
      "Mean_Freq_H: Importance 0.0122\n",
      "Irregularity_H: Importance 0.0736\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0375\n",
      "Mean_L: Importance 0.1700\n",
      "Mean Deviation_L: Importance 0.0522\n",
      "RMS_L: Importance 0.1300\n",
      "Entropy_L: Importance 0.0094\n",
      "Irregularity_L: Importance 0.0174\n",
      "Variance_L: Importance 0.0889\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0041\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0832\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1259\n",
      "Entropy_H: Importance 0.0180\n",
      "Spread_H: Importance 0.0102\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0083\n",
      "Min_L: Importance 0.1283\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0806\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1280\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0640\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0867\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0606\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0202\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0400\n",
      "Min_L: Importance 0.1006\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0194\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0162\n",
      "Variance_L: Importance 0.0817\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0059\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC1 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0625\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1022\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0353\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0801\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0337\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       664\n",
      "   macro avg       1.00      1.00      1.00       664\n",
      "weighted avg       1.00      1.00      1.00       664\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 3** data and Testing on **ADC 2**\n",
    "To test ADC 3 with changing probe to ADC 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0320\n",
      "Min_H: Importance 0.0522\n",
      "Mean_H: Importance 0.1101\n",
      "Mean Deviation_H: Importance 0.0019\n",
      "RMS_H: Importance 0.1822\n",
      "Peak-to-Peak_H: Importance 0.0037\n",
      "Entropy_H: Importance 0.1499\n",
      "Spread_H: Importance 0.0031\n",
      "Mean_Freq_H: Importance 0.0801\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0194\n",
      "Min_L: Importance 0.0472\n",
      "Mean_L: Importance 0.0582\n",
      "RMS_L: Importance 0.0378\n",
      "Skewness_L: Importance 0.0034\n",
      "Entropy_L: Importance 0.0394\n",
      "Mean_Freq_L: Importance 0.0220\n",
      "Irregularity_L: Importance 0.0045\n",
      "Variance_L: Importance 0.0828\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9704\n",
      "Balanced Accuracy: 0.9704\n",
      "MCC: 0.9420\n",
      "Log Loss: 0.0725\n",
      "F1 Score: 0.9696\n",
      "Recall: 0.9447\n",
      "Precision: 0.9959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.95      1.00      0.97      9998\n",
      "         CH2       1.00      0.94      0.97      9998\n",
      "\n",
      "    accuracy                           0.97     19996\n",
      "   macro avg       0.97      0.97      0.97     19996\n",
      "weighted avg       0.97      0.97      0.97     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9959   39]\n",
      " [ 553 9445]]\n",
      "False Positive Rate (FPR): 0.0039\n",
      "False Negative Rate (FNR): 0.0553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0825\n",
      "Min_H: Importance 0.0317\n",
      "Mean_H: Importance 0.1433\n",
      "Mean Deviation_H: Importance 0.0046\n",
      "RMS_H: Importance 0.2186\n",
      "Centroid_H: Importance 0.0004\n",
      "Entropy_H: Importance 0.0184\n",
      "Mean_Freq_H: Importance 0.0679\n",
      "Variance_H: Importance 0.0391\n",
      "Max_L: Importance 0.0008\n",
      "Min_L: Importance 0.0281\n",
      "Mean_L: Importance 0.1291\n",
      "Std_L: Importance 0.0007\n",
      "RMS_L: Importance 0.1220\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0025\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0160\n",
      "Variance_L: Importance 0.0615\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9772\n",
      "Balanced Accuracy: 0.9772\n",
      "MCC: 0.9554\n",
      "Log Loss: 0.0257\n",
      "F1 Score: 0.9767\n",
      "Recall: 0.9544\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.96      1.00      0.98      4998\n",
      "         CH2       1.00      0.95      0.98      4998\n",
      "\n",
      "    accuracy                           0.98      9996\n",
      "   macro avg       0.98      0.98      0.98      9996\n",
      "weighted avg       0.98      0.98      0.98      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [ 228 4770]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (3332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1070\n",
      "Min_H: Importance 0.0266\n",
      "Mean_H: Importance 0.1418\n",
      "Mean Deviation_H: Importance 0.0104\n",
      "RMS_H: Importance 0.1501\n",
      "Centroid_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0169\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0627\n",
      "Variance_H: Importance 0.0667\n",
      "Min_L: Importance 0.0279\n",
      "Mean_L: Importance 0.1151\n",
      "Std_L: Importance 0.0008\n",
      "RMS_L: Importance 0.1620\n",
      "Kurtosis_L: Importance 0.0018\n",
      "Peak-to-Peak_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0302\n",
      "Mean_Freq_L: Importance 0.0102\n",
      "Variance_L: Importance 0.0685\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9895\n",
      "Balanced Accuracy: 0.9895\n",
      "MCC: 0.9792\n",
      "Log Loss: 0.0565\n",
      "F1 Score: 0.9894\n",
      "Recall: 0.9793\n",
      "Precision: 0.9997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      1.00      0.99      3331\n",
      "         CH2       1.00      0.98      0.99      3332\n",
      "\n",
      "    accuracy                           0.99      6663\n",
      "   macro avg       0.99      0.99      0.99      6663\n",
      "weighted avg       0.99      0.99      0.99      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3330    1]\n",
      " [  69 3263]]\n",
      "False Positive Rate (FPR): 0.0003\n",
      "False Negative Rate (FNR): 0.0207\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0634\n",
      "Min_H: Importance 0.0298\n",
      "Mean_H: Importance 0.0915\n",
      "Std_H: Importance 0.0221\n",
      "Mean Deviation_H: Importance 0.0164\n",
      "RMS_H: Importance 0.1400\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0141\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Variance_H: Importance 0.0671\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1975\n",
      "RMS_L: Importance 0.1205\n",
      "Peak-to-Peak_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0293\n",
      "Mean_Freq_L: Importance 0.0045\n",
      "Variance_L: Importance 0.1270\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0021\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0669\n",
      "Min_H: Importance 0.0105\n",
      "Mean_H: Importance 0.0930\n",
      "Std_H: Importance 0.0183\n",
      "RMS_H: Importance 0.1493\n",
      "Centroid_H: Importance 0.0006\n",
      "Entropy_H: Importance 0.0313\n",
      "Mean_Freq_H: Importance 0.0492\n",
      "Variance_H: Importance 0.0832\n",
      "Max_L: Importance 0.0121\n",
      "Min_L: Importance 0.0222\n",
      "Mean_L: Importance 0.1501\n",
      "Mean Deviation_L: Importance 0.0117\n",
      "RMS_L: Importance 0.1342\n",
      "Kurtosis_L: Importance 0.0022\n",
      "Centroid_L: Importance 0.0014\n",
      "Entropy_L: Importance 0.0419\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0001\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1139\n",
      "Min_H: Importance 0.0042\n",
      "Mean_H: Importance 0.1130\n",
      "Std_H: Importance 0.0109\n",
      "Mean Deviation_H: Importance 0.0261\n",
      "RMS_H: Importance 0.1467\n",
      "Entropy_H: Importance 0.0170\n",
      "Spread_H: Importance 0.0052\n",
      "Mean_Freq_H: Importance 0.0232\n",
      "Variance_H: Importance 0.0819\n",
      "Max_L: Importance 0.0041\n",
      "Min_L: Importance 0.0677\n",
      "Mean_L: Importance 0.1654\n",
      "Mean Deviation_L: Importance 0.0099\n",
      "RMS_L: Importance 0.1270\n",
      "Peak-to-Peak_L: Importance 0.0016\n",
      "Centroid_L: Importance 0.0013\n",
      "Entropy_L: Importance 0.0100\n",
      "Variance_L: Importance 0.0709\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0000\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1035\n",
      "Mean_H: Importance 0.0925\n",
      "Std_H: Importance 0.0121\n",
      "Mean Deviation_H: Importance 0.0165\n",
      "RMS_H: Importance 0.1071\n",
      "Entropy_H: Importance 0.0152\n",
      "Skewness_Freq_H: Importance 0.0012\n",
      "Mean_Freq_H: Importance 0.0122\n",
      "Irregularity_H: Importance 0.0736\n",
      "Variance_H: Importance 0.0608\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0375\n",
      "Mean_L: Importance 0.1700\n",
      "Mean Deviation_L: Importance 0.0522\n",
      "RMS_L: Importance 0.1300\n",
      "Entropy_L: Importance 0.0094\n",
      "Irregularity_L: Importance 0.0174\n",
      "Variance_L: Importance 0.0889\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0079\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0832\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1259\n",
      "Entropy_H: Importance 0.0180\n",
      "Spread_H: Importance 0.0102\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0083\n",
      "Min_L: Importance 0.1283\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0806\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1280\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0640\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0010\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0606\n",
      "RMS_H: Importance 0.1258\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0202\n",
      "Kurtosis_Freq_H: Importance 0.0161\n",
      "Variance_H: Importance 0.0400\n",
      "Min_L: Importance 0.1006\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0194\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0199\n",
      "Mean_Freq_L: Importance 0.0162\n",
      "Variance_L: Importance 0.0817\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0131\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe2 ADC3 CH1: (332, 38)\n",
      "✅ Loaded Probe2 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe2 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe2 ADC2 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0625\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1022\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0353\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0801\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.1000\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0850\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       662\n",
      "   macro avg       1.00      1.00      1.00       662\n",
      "weighted avg       1.00      1.00      1.00       662\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 2  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 1**\n",
    "To conduct experiments, **Probe 3** (SMA cable which transfers data) is used consistently throughout. **Probe 3** is kept constant, and the machine learning model is **always trained on ADC 1** data. The model is then tested on **ADC 2** and **ADC 3** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 1** data and Testing on **ADC 2** data\n",
    "To test ADC 1 with changing probe to ADC 2 and ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0062\n",
      "Min_H: Importance 0.0174\n",
      "Mean_H: Importance 0.0772\n",
      "Mean Deviation_H: Importance 0.0048\n",
      "RMS_H: Importance 0.1177\n",
      "Skewness_H: Importance 0.0029\n",
      "Entropy_H: Importance 0.0231\n",
      "Spread_H: Importance 0.0037\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Irregularity_H: Importance 0.0030\n",
      "Variance_H: Importance 0.0457\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.1182\n",
      "Mean_L: Importance 0.1648\n",
      "RMS_L: Importance 0.2142\n",
      "Skewness_L: Importance 0.0026\n",
      "Entropy_L: Importance 0.0385\n",
      "Mean_Freq_L: Importance 0.0161\n",
      "Variance_L: Importance 0.0790\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9210\n",
      "Balanced Accuracy: 0.9210\n",
      "MCC: 0.8430\n",
      "Log Loss: 0.2436\n",
      "F1 Score: 0.9192\n",
      "Recall: 0.8980\n",
      "Precision: 0.9414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.90      0.94      0.92      9998\n",
      "         CH2       0.94      0.90      0.92      9998\n",
      "\n",
      "    accuracy                           0.92     19996\n",
      "   macro avg       0.92      0.92      0.92     19996\n",
      "weighted avg       0.92      0.92      0.92     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9439  559]\n",
      " [1020 8978]]\n",
      "False Positive Rate (FPR): 0.0559\n",
      "False Negative Rate (FNR): 0.1020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1215\n",
      "Min_H: Importance 0.0255\n",
      "Mean_H: Importance 0.1520\n",
      "Mean Deviation_H: Importance 0.0050\n",
      "RMS_H: Importance 0.2228\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0771\n",
      "Variance_H: Importance 0.0366\n",
      "Max_L: Importance 0.0011\n",
      "Min_L: Importance 0.0233\n",
      "Mean_L: Importance 0.1243\n",
      "Std_L: Importance 0.0013\n",
      "RMS_L: Importance 0.1203\n",
      "Skewness_L: Importance 0.0012\n",
      "Kurtosis_L: Importance 0.0030\n",
      "Entropy_L: Importance 0.0282\n",
      "Spread_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0117\n",
      "Variance_L: Importance 0.0252\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0010\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **Probe 1** and tested on **Probe 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1332\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.1297\n",
      "Mean Deviation_H: Importance 0.0056\n",
      "RMS_H: Importance 0.2115\n",
      "Peak-to-Peak_H: Importance 0.0100\n",
      "Centroid_H: Importance 0.0061\n",
      "Entropy_H: Importance 0.0443\n",
      "Spread_H: Importance 0.0060\n",
      "Mean_Freq_H: Importance 0.0810\n",
      "Irregularity_H: Importance 0.0072\n",
      "Variance_H: Importance 0.0425\n",
      "Min_L: Importance 0.0276\n",
      "Mean_L: Importance 0.0888\n",
      "RMS_L: Importance 0.1151\n",
      "Skewness_L: Importance 0.0093\n",
      "Entropy_L: Importance 0.0251\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.0249\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9080\n",
      "Balanced Accuracy: 0.9080\n",
      "MCC: 0.8302\n",
      "Log Loss: 0.8901\n",
      "F1 Score: 0.8987\n",
      "Recall: 0.8160\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.84      1.00      0.92      3331\n",
      "         CH2       1.00      0.82      0.90      3332\n",
      "\n",
      "    accuracy                           0.91      6663\n",
      "   macro avg       0.92      0.91      0.91      6663\n",
      "weighted avg       0.92      0.91      0.91      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3331    0]\n",
      " [ 613 2719]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.1840\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0150\n",
      "Mean_H: Importance 0.0941\n",
      "Mean Deviation_H: Importance 0.0277\n",
      "RMS_H: Importance 0.1154\n",
      "Peak-to-Peak_H: Importance 0.0033\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0142\n",
      "Mean_Freq_H: Importance 0.0588\n",
      "Variance_H: Importance 0.0632\n",
      "Max_L: Importance 0.0138\n",
      "Min_L: Importance 0.0464\n",
      "Mean_L: Importance 0.1817\n",
      "RMS_L: Importance 0.1361\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0457\n",
      "Spread_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0130\n",
      "Variance_L: Importance 0.1077\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0023\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0094\n",
      "Mean_H: Importance 0.0944\n",
      "Mean Deviation_H: Importance 0.0007\n",
      "RMS_H: Importance 0.1411\n",
      "Peak-to-Peak_H: Importance 0.0051\n",
      "Entropy_H: Importance 0.0147\n",
      "Mean_Freq_H: Importance 0.0505\n",
      "Variance_H: Importance 0.0836\n",
      "Max_L: Importance 0.0092\n",
      "Min_L: Importance 0.0402\n",
      "Mean_L: Importance 0.1728\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.0123\n",
      "RMS_L: Importance 0.1333\n",
      "Kurtosis_L: Importance 0.0020\n",
      "Entropy_L: Importance 0.0449\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0012\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0771\n",
      "Min_H: Importance 0.0103\n",
      "Mean_H: Importance 0.1020\n",
      "Mean Deviation_H: Importance 0.0307\n",
      "RMS_H: Importance 0.1455\n",
      "Skewness_H: Importance 0.0126\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0105\n",
      "Variance_H: Importance 0.0713\n",
      "Max_L: Importance 0.0098\n",
      "Min_L: Importance 0.0197\n",
      "Mean_L: Importance 0.1949\n",
      "Mean Deviation_L: Importance 0.0219\n",
      "RMS_L: Importance 0.1386\n",
      "Kurtosis_L: Importance 0.0161\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Skewness_Freq_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0024\n",
      "Variance_L: Importance 0.1334\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0019\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0807\n",
      "Mean Deviation_H: Importance 0.0430\n",
      "RMS_H: Importance 0.1256\n",
      "Entropy_H: Importance 0.0165\n",
      "Mean_Freq_H: Importance 0.0323\n",
      "Kurtosis_Freq_H: Importance 0.0108\n",
      "Variance_H: Importance 0.0206\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1805\n",
      "Std_L: Importance 0.0005\n",
      "Mean Deviation_L: Importance 0.1077\n",
      "RMS_L: Importance 0.1231\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0189\n",
      "Skewness_Freq_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0155\n",
      "Variance_L: Importance 0.0836\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0278\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1042\n",
      "Entropy_H: Importance 0.0600\n",
      "Mean_Freq_H: Importance 0.0186\n",
      "Irregularity_H: Importance 0.0116\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0858\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0208\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1007\n",
      "Kurtosis_L: Importance 0.0031\n",
      "Centroid_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1249\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0014\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0824\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0556\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 1** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0200\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0600\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Irregularity_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0769\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 1** data and Testing on **ADC 3** data\n",
    "To test ADC 1 with changing probe to ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0062\n",
      "Min_H: Importance 0.0174\n",
      "Mean_H: Importance 0.0772\n",
      "Mean Deviation_H: Importance 0.0048\n",
      "RMS_H: Importance 0.1177\n",
      "Skewness_H: Importance 0.0029\n",
      "Entropy_H: Importance 0.0231\n",
      "Spread_H: Importance 0.0037\n",
      "Mean_Freq_H: Importance 0.0421\n",
      "Irregularity_H: Importance 0.0030\n",
      "Variance_H: Importance 0.0457\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.1182\n",
      "Mean_L: Importance 0.1648\n",
      "RMS_L: Importance 0.2142\n",
      "Skewness_L: Importance 0.0026\n",
      "Entropy_L: Importance 0.0385\n",
      "Mean_Freq_L: Importance 0.0161\n",
      "Variance_L: Importance 0.0790\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9788\n",
      "Balanced Accuracy: 0.9788\n",
      "MCC: 0.9584\n",
      "Log Loss: 0.0432\n",
      "F1 Score: 0.9792\n",
      "Recall: 0.9999\n",
      "Precision: 0.9594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.96      0.98      9998\n",
      "         CH2       0.96      1.00      0.98      9998\n",
      "\n",
      "    accuracy                           0.98     19996\n",
      "   macro avg       0.98      0.98      0.98     19996\n",
      "weighted avg       0.98      0.98      0.98     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9575  423]\n",
      " [   1 9997]]\n",
      "False Positive Rate (FPR): 0.0423\n",
      "False Negative Rate (FNR): 0.0001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1215\n",
      "Min_H: Importance 0.0255\n",
      "Mean_H: Importance 0.1520\n",
      "Mean Deviation_H: Importance 0.0050\n",
      "RMS_H: Importance 0.2228\n",
      "Entropy_H: Importance 0.0175\n",
      "Mean_Freq_H: Importance 0.0771\n",
      "Variance_H: Importance 0.0366\n",
      "Max_L: Importance 0.0011\n",
      "Min_L: Importance 0.0233\n",
      "Mean_L: Importance 0.1243\n",
      "Std_L: Importance 0.0013\n",
      "RMS_L: Importance 0.1203\n",
      "Skewness_L: Importance 0.0012\n",
      "Kurtosis_L: Importance 0.0030\n",
      "Entropy_L: Importance 0.0282\n",
      "Spread_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0117\n",
      "Variance_L: Importance 0.0252\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9998\n",
      "Balanced Accuracy: 0.9998\n",
      "MCC: 0.9996\n",
      "Log Loss: 0.0031\n",
      "F1 Score: 0.9998\n",
      "Recall: 0.9996\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   2 4996]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1332\n",
      "Min_H: Importance 0.0077\n",
      "Mean_H: Importance 0.1297\n",
      "Mean Deviation_H: Importance 0.0056\n",
      "RMS_H: Importance 0.2115\n",
      "Peak-to-Peak_H: Importance 0.0100\n",
      "Centroid_H: Importance 0.0061\n",
      "Entropy_H: Importance 0.0443\n",
      "Spread_H: Importance 0.0060\n",
      "Mean_Freq_H: Importance 0.0810\n",
      "Irregularity_H: Importance 0.0072\n",
      "Variance_H: Importance 0.0425\n",
      "Min_L: Importance 0.0276\n",
      "Mean_L: Importance 0.0888\n",
      "RMS_L: Importance 0.1151\n",
      "Skewness_L: Importance 0.0093\n",
      "Entropy_L: Importance 0.0251\n",
      "Mean_Freq_L: Importance 0.0112\n",
      "Variance_L: Importance 0.0249\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9514\n",
      "Balanced Accuracy: 0.9514\n",
      "MCC: 0.9070\n",
      "Log Loss: 0.0663\n",
      "F1 Score: 0.9536\n",
      "Recall: 1.0000\n",
      "Precision: 0.9114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.90      0.95      3332\n",
      "         CH2       0.91      1.00      0.95      3331\n",
      "\n",
      "    accuracy                           0.95      6663\n",
      "   macro avg       0.96      0.95      0.95      6663\n",
      "weighted avg       0.96      0.95      0.95      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3008  324]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.0972\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0150\n",
      "Mean_H: Importance 0.0941\n",
      "Mean Deviation_H: Importance 0.0277\n",
      "RMS_H: Importance 0.1154\n",
      "Peak-to-Peak_H: Importance 0.0033\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0142\n",
      "Mean_Freq_H: Importance 0.0588\n",
      "Variance_H: Importance 0.0632\n",
      "Max_L: Importance 0.0138\n",
      "Min_L: Importance 0.0464\n",
      "Mean_L: Importance 0.1817\n",
      "RMS_L: Importance 0.1361\n",
      "Centroid_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0457\n",
      "Spread_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0130\n",
      "Variance_L: Importance 0.1077\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0046\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0636\n",
      "Min_H: Importance 0.0094\n",
      "Mean_H: Importance 0.0944\n",
      "Mean Deviation_H: Importance 0.0007\n",
      "RMS_H: Importance 0.1411\n",
      "Peak-to-Peak_H: Importance 0.0051\n",
      "Entropy_H: Importance 0.0147\n",
      "Mean_Freq_H: Importance 0.0505\n",
      "Variance_H: Importance 0.0836\n",
      "Max_L: Importance 0.0092\n",
      "Min_L: Importance 0.0402\n",
      "Mean_L: Importance 0.1728\n",
      "Std_L: Importance 0.0007\n",
      "Mean Deviation_L: Importance 0.0123\n",
      "RMS_L: Importance 0.1333\n",
      "Kurtosis_L: Importance 0.0020\n",
      "Entropy_L: Importance 0.0449\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1101\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0006\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0771\n",
      "Min_H: Importance 0.0103\n",
      "Mean_H: Importance 0.1020\n",
      "Mean Deviation_H: Importance 0.0307\n",
      "RMS_H: Importance 0.1455\n",
      "Skewness_H: Importance 0.0126\n",
      "Centroid_H: Importance 0.0021\n",
      "Entropy_H: Importance 0.0105\n",
      "Variance_H: Importance 0.0713\n",
      "Max_L: Importance 0.0098\n",
      "Min_L: Importance 0.0197\n",
      "Mean_L: Importance 0.1949\n",
      "Mean Deviation_L: Importance 0.0219\n",
      "RMS_L: Importance 0.1386\n",
      "Kurtosis_L: Importance 0.0161\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Skewness_Freq_L: Importance 0.0007\n",
      "Mean_Freq_L: Importance 0.0024\n",
      "Variance_L: Importance 0.1334\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0039\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Mean_H: Importance 0.0807\n",
      "Mean Deviation_H: Importance 0.0430\n",
      "RMS_H: Importance 0.1256\n",
      "Entropy_H: Importance 0.0165\n",
      "Mean_Freq_H: Importance 0.0323\n",
      "Kurtosis_Freq_H: Importance 0.0108\n",
      "Variance_H: Importance 0.0206\n",
      "Min_L: Importance 0.0394\n",
      "Mean_L: Importance 0.1805\n",
      "Std_L: Importance 0.0005\n",
      "Mean Deviation_L: Importance 0.1077\n",
      "RMS_L: Importance 0.1231\n",
      "Peak-to-Peak_L: Importance 0.0001\n",
      "Entropy_L: Importance 0.0189\n",
      "Skewness_Freq_L: Importance 0.0000\n",
      "Mean_Freq_L: Importance 0.0155\n",
      "Variance_L: Importance 0.0836\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0277\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0601\n",
      "RMS_H: Importance 0.1042\n",
      "Entropy_H: Importance 0.0600\n",
      "Mean_Freq_H: Importance 0.0186\n",
      "Irregularity_H: Importance 0.0116\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0858\n",
      "Mean_L: Importance 0.1800\n",
      "Std_L: Importance 0.0208\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1007\n",
      "Kurtosis_L: Importance 0.0031\n",
      "Centroid_L: Importance 0.0000\n",
      "Entropy_L: Importance 0.0197\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.1405\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Min_H: Importance 0.0014\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0824\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0600\n",
      "Kurtosis_Freq_H: Importance 0.0163\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0800\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9987\n",
      "Balanced Accuracy: 0.9987\n",
      "MCC: 0.9975\n",
      "Log Loss: 0.1622\n",
      "F1 Score: 0.9987\n",
      "Recall: 0.9975\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  1 397]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 1** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0600\n",
      "Std_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.0800\n",
      "Entropy_H: Importance 0.0400\n",
      "Mean_Freq_H: Importance 0.0400\n",
      "Irregularity_H: Importance 0.0200\n",
      "Min_L: Importance 0.0400\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0600\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0600\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0800\n",
      "Irregularity_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.2065\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3 # Fixed probe, change as needed\n",
    "train_adc_num = 1\n",
    "test_adc_num = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 2**\n",
    "To conduct experiments, **Probe 3** (SMA cable which trandfers data) is used consistently throughout. **Probe 3** is kept constant, and the machine learning model is **always trained on ADC 2**. The model is then tested on **ADC 3** and **ADC 1** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 2** data and Testing on **ADC 3**\n",
    "To test ADC 2 with changing probe to ADC 3 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1694\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0882\n",
      "Mean Deviation_H: Importance 0.0057\n",
      "RMS_H: Importance 0.1780\n",
      "Skewness_H: Importance 0.0048\n",
      "Peak-to-Peak_H: Importance 0.0044\n",
      "Entropy_H: Importance 0.1111\n",
      "Spread_H: Importance 0.0043\n",
      "Mean_Freq_H: Importance 0.0611\n",
      "Variance_H: Importance 0.0211\n",
      "Max_L: Importance 0.0040\n",
      "Min_L: Importance 0.0971\n",
      "Mean_L: Importance 0.0526\n",
      "RMS_L: Importance 0.0720\n",
      "Skewness_L: Importance 0.0055\n",
      "Entropy_L: Importance 0.0421\n",
      "Mean_Freq_L: Importance 0.0188\n",
      "Variance_L: Importance 0.0180\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9139\n",
      "Balanced Accuracy: 0.9139\n",
      "MCC: 0.8403\n",
      "Log Loss: 0.1718\n",
      "F1 Score: 0.9207\n",
      "Recall: 1.0000\n",
      "Precision: 0.8531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.83      0.91      9998\n",
      "         CH2       0.85      1.00      0.92      9998\n",
      "\n",
      "    accuracy                           0.91     19996\n",
      "   macro avg       0.93      0.91      0.91     19996\n",
      "weighted avg       0.93      0.91      0.91     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8276 1722]\n",
      " [   0 9998]]\n",
      "False Positive Rate (FPR): 0.1722\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0596\n",
      "Min_H: Importance 0.0351\n",
      "Mean_H: Importance 0.0945\n",
      "Mean Deviation_H: Importance 0.0063\n",
      "RMS_H: Importance 0.1270\n",
      "Centroid_H: Importance 0.0036\n",
      "Entropy_H: Importance 0.0172\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0650\n",
      "Variance_H: Importance 0.0612\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0258\n",
      "Mean_L: Importance 0.2093\n",
      "RMS_L: Importance 0.1217\n",
      "Skewness_L: Importance 0.0000\n",
      "Kurtosis_L: Importance 0.0065\n",
      "Entropy_L: Importance 0.0241\n",
      "Mean_Freq_L: Importance 0.0091\n",
      "Variance_L: Importance 0.1331\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9859\n",
      "Balanced Accuracy: 0.9859\n",
      "MCC: 0.9722\n",
      "Log Loss: 0.0199\n",
      "F1 Score: 0.9857\n",
      "Recall: 0.9718\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.97      1.00      0.99      4998\n",
      "         CH2       1.00      0.97      0.99      4998\n",
      "\n",
      "    accuracy                           0.99      9996\n",
      "   macro avg       0.99      0.99      0.99      9996\n",
      "weighted avg       0.99      0.99      0.99      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [ 141 4857]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0282\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0055\n",
      "Min_H: Importance 0.0148\n",
      "Mean_H: Importance 0.1371\n",
      "Mean Deviation_H: Importance 0.0022\n",
      "RMS_H: Importance 0.2265\n",
      "Skewness_H: Importance 0.0004\n",
      "Centroid_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.0114\n",
      "Spread_H: Importance 0.0003\n",
      "Mean_Freq_H: Importance 0.0249\n",
      "Variance_H: Importance 0.0565\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0935\n",
      "Mean_L: Importance 0.1321\n",
      "RMS_L: Importance 0.1545\n",
      "Skewness_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0235\n",
      "Mean_Freq_L: Importance 0.0073\n",
      "Variance_L: Importance 0.1044\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0050\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      3332\n",
      "         CH2       1.00      1.00      1.00      3331\n",
      "\n",
      "    accuracy                           1.00      6663\n",
      "   macro avg       1.00      1.00      1.00      6663\n",
      "weighted avg       1.00      1.00      1.00      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3332    0]\n",
      " [   0 3331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0648\n",
      "Min_H: Importance 0.0288\n",
      "Mean_H: Importance 0.0963\n",
      "Mean Deviation_H: Importance 0.0137\n",
      "RMS_H: Importance 0.1324\n",
      "Skewness_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0138\n",
      "Mean_Freq_H: Importance 0.0559\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0003\n",
      "Min_L: Importance 0.0224\n",
      "Mean_L: Importance 0.2112\n",
      "RMS_L: Importance 0.1272\n",
      "Skewness_L: Importance 0.0005\n",
      "Kurtosis_L: Importance 0.0004\n",
      "Centroid_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0292\n",
      "Mean_Freq_L: Importance 0.0115\n",
      "Variance_L: Importance 0.1301\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0007\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3 # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0599\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0892\n",
      "Std_H: Importance 0.0296\n",
      "Mean Deviation_H: Importance 0.0339\n",
      "RMS_H: Importance 0.1279\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0245\n",
      "Mean_Freq_H: Importance 0.0806\n",
      "Irregularity_H: Importance 0.0090\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0022\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1905\n",
      "Mean Deviation_L: Importance 0.0232\n",
      "RMS_L: Importance 0.1217\n",
      "Entropy_L: Importance 0.0160\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.1250\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0054\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1024\n",
      "Mean_H: Importance 0.0815\n",
      "Std_H: Importance 0.0167\n",
      "Mean Deviation_H: Importance 0.0179\n",
      "RMS_H: Importance 0.1285\n",
      "Entropy_H: Importance 0.0198\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Kurtosis_Freq_H: Importance 0.0084\n",
      "Variance_H: Importance 0.0604\n",
      "Min_L: Importance 0.0837\n",
      "Mean_L: Importance 0.1815\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1216\n",
      "Skewness_L: Importance 0.0002\n",
      "Kurtosis_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0373\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0819\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0057\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1038\n",
      "Min_H: Importance 0.0006\n",
      "Mean_H: Importance 0.1025\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0415\n",
      "RMS_H: Importance 0.1069\n",
      "Entropy_H: Importance 0.0120\n",
      "Spread_H: Importance 0.0049\n",
      "Mean_Freq_H: Importance 0.0215\n",
      "Variance_H: Importance 0.0687\n",
      "Min_L: Importance 0.0630\n",
      "Mean_L: Importance 0.1443\n",
      "Mean Deviation_L: Importance 0.0921\n",
      "RMS_L: Importance 0.1328\n",
      "Skewness_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0111\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Irregularity_L: Importance 0.0063\n",
      "Variance_L: Importance 0.0690\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0013\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1004\n",
      "Mean_H: Importance 0.0807\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0641\n",
      "RMS_H: Importance 0.1223\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0390\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0392\n",
      "Mean_L: Importance 0.1410\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0808\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0047\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1002\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0206\n",
      "RMS_H: Importance 0.1009\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0185\n",
      "Irregularity_H: Importance 0.0001\n",
      "Min_L: Importance 0.0190\n",
      "Mean_L: Importance 0.1208\n",
      "Std_L: Importance 0.1202\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0179\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 2** and tested on **ADC 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0850\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.0856\n",
      "Skewness_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.1400\n",
      "Spread_H: Importance 0.0157\n",
      "Mean_Freq_H: Importance 0.0405\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0398\n",
      "Mean_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0196\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0022\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 3\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 2** data and Testing on **ADC 1**\n",
    "To test ADC 2 with changing probe to ADC 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1694\n",
      "Min_H: Importance 0.0161\n",
      "Mean_H: Importance 0.0882\n",
      "Mean Deviation_H: Importance 0.0057\n",
      "RMS_H: Importance 0.1780\n",
      "Skewness_H: Importance 0.0048\n",
      "Peak-to-Peak_H: Importance 0.0044\n",
      "Entropy_H: Importance 0.1111\n",
      "Spread_H: Importance 0.0043\n",
      "Mean_Freq_H: Importance 0.0611\n",
      "Variance_H: Importance 0.0211\n",
      "Max_L: Importance 0.0040\n",
      "Min_L: Importance 0.0971\n",
      "Mean_L: Importance 0.0526\n",
      "RMS_L: Importance 0.0720\n",
      "Skewness_L: Importance 0.0055\n",
      "Entropy_L: Importance 0.0421\n",
      "Mean_Freq_L: Importance 0.0188\n",
      "Variance_L: Importance 0.0180\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.8755\n",
      "Balanced Accuracy: 0.8755\n",
      "MCC: 0.7754\n",
      "Log Loss: 0.3328\n",
      "F1 Score: 0.8893\n",
      "Recall: 0.9999\n",
      "Precision: 0.8007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.75      0.86      9998\n",
      "         CH2       0.80      1.00      0.89      9998\n",
      "\n",
      "    accuracy                           0.88     19996\n",
      "   macro avg       0.90      0.88      0.87     19996\n",
      "weighted avg       0.90      0.88      0.87     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7510 2488]\n",
      " [   1 9997]]\n",
      "False Positive Rate (FPR): 0.2488\n",
      "False Negative Rate (FNR): 0.0001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0596\n",
      "Min_H: Importance 0.0351\n",
      "Mean_H: Importance 0.0945\n",
      "Mean Deviation_H: Importance 0.0063\n",
      "RMS_H: Importance 0.1270\n",
      "Centroid_H: Importance 0.0036\n",
      "Entropy_H: Importance 0.0172\n",
      "Spread_H: Importance 0.0005\n",
      "Mean_Freq_H: Importance 0.0650\n",
      "Variance_H: Importance 0.0612\n",
      "Max_L: Importance 0.0004\n",
      "Min_L: Importance 0.0258\n",
      "Mean_L: Importance 0.2093\n",
      "RMS_L: Importance 0.1217\n",
      "Skewness_L: Importance 0.0000\n",
      "Kurtosis_L: Importance 0.0065\n",
      "Entropy_L: Importance 0.0241\n",
      "Mean_Freq_L: Importance 0.0091\n",
      "Variance_L: Importance 0.1331\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9898\n",
      "Balanced Accuracy: 0.9898\n",
      "MCC: 0.9798\n",
      "Log Loss: 0.0223\n",
      "F1 Score: 0.9897\n",
      "Recall: 0.9798\n",
      "Precision: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.98      1.00      0.99      4998\n",
      "         CH2       1.00      0.98      0.99      4998\n",
      "\n",
      "    accuracy                           0.99      9996\n",
      "   macro avg       0.99      0.99      0.99      9996\n",
      "weighted avg       0.99      0.99      0.99      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4997    1]\n",
      " [ 101 4897]]\n",
      "False Positive Rate (FPR): 0.0002\n",
      "False Negative Rate (FNR): 0.0202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0055\n",
      "Min_H: Importance 0.0148\n",
      "Mean_H: Importance 0.1371\n",
      "Mean Deviation_H: Importance 0.0022\n",
      "RMS_H: Importance 0.2265\n",
      "Skewness_H: Importance 0.0004\n",
      "Centroid_H: Importance 0.0022\n",
      "Entropy_H: Importance 0.0114\n",
      "Spread_H: Importance 0.0003\n",
      "Mean_Freq_H: Importance 0.0249\n",
      "Variance_H: Importance 0.0565\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0935\n",
      "Mean_L: Importance 0.1321\n",
      "RMS_L: Importance 0.1545\n",
      "Skewness_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0235\n",
      "Mean_Freq_L: Importance 0.0073\n",
      "Variance_L: Importance 0.1044\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9473\n",
      "Balanced Accuracy: 0.9473\n",
      "MCC: 0.8990\n",
      "Log Loss: 0.2715\n",
      "F1 Score: 0.9498\n",
      "Recall: 0.9967\n",
      "Precision: 0.9071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.90      0.94      3331\n",
      "         CH2       0.91      1.00      0.95      3332\n",
      "\n",
      "    accuracy                           0.95      6663\n",
      "   macro avg       0.95      0.95      0.95      6663\n",
      "weighted avg       0.95      0.95      0.95      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2991  340]\n",
      " [  11 3321]]\n",
      "False Positive Rate (FPR): 0.1021\n",
      "False Negative Rate (FNR): 0.0033\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0648\n",
      "Min_H: Importance 0.0288\n",
      "Mean_H: Importance 0.0963\n",
      "Mean Deviation_H: Importance 0.0137\n",
      "RMS_H: Importance 0.1324\n",
      "Skewness_H: Importance 0.0001\n",
      "Entropy_H: Importance 0.0138\n",
      "Mean_Freq_H: Importance 0.0559\n",
      "Variance_H: Importance 0.0613\n",
      "Max_L: Importance 0.0003\n",
      "Min_L: Importance 0.0224\n",
      "Mean_L: Importance 0.2112\n",
      "RMS_L: Importance 0.1272\n",
      "Skewness_L: Importance 0.0005\n",
      "Kurtosis_L: Importance 0.0004\n",
      "Centroid_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0292\n",
      "Mean_Freq_L: Importance 0.0115\n",
      "Variance_L: Importance 0.1301\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0025\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0599\n",
      "Min_H: Importance 0.0000\n",
      "Mean_H: Importance 0.0892\n",
      "Std_H: Importance 0.0296\n",
      "Mean Deviation_H: Importance 0.0339\n",
      "RMS_H: Importance 0.1279\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0245\n",
      "Mean_Freq_H: Importance 0.0806\n",
      "Irregularity_H: Importance 0.0090\n",
      "Variance_H: Importance 0.0656\n",
      "Max_L: Importance 0.0022\n",
      "Min_L: Importance 0.0006\n",
      "Mean_L: Importance 0.1905\n",
      "Mean Deviation_L: Importance 0.0232\n",
      "RMS_L: Importance 0.1217\n",
      "Entropy_L: Importance 0.0160\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.1250\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0145\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1024\n",
      "Mean_H: Importance 0.0815\n",
      "Std_H: Importance 0.0167\n",
      "Mean Deviation_H: Importance 0.0179\n",
      "RMS_H: Importance 0.1285\n",
      "Entropy_H: Importance 0.0198\n",
      "Spread_H: Importance 0.0001\n",
      "Mean_Freq_H: Importance 0.0578\n",
      "Kurtosis_Freq_H: Importance 0.0084\n",
      "Variance_H: Importance 0.0604\n",
      "Min_L: Importance 0.0837\n",
      "Mean_L: Importance 0.1815\n",
      "Std_L: Importance 0.0001\n",
      "RMS_L: Importance 0.1216\n",
      "Skewness_L: Importance 0.0002\n",
      "Kurtosis_L: Importance 0.0002\n",
      "Entropy_L: Importance 0.0373\n",
      "Mean_Freq_L: Importance 0.0001\n",
      "Variance_L: Importance 0.0819\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0085\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1038\n",
      "Min_H: Importance 0.0006\n",
      "Mean_H: Importance 0.1025\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0415\n",
      "RMS_H: Importance 0.1069\n",
      "Entropy_H: Importance 0.0120\n",
      "Spread_H: Importance 0.0049\n",
      "Mean_Freq_H: Importance 0.0215\n",
      "Variance_H: Importance 0.0687\n",
      "Min_L: Importance 0.0630\n",
      "Mean_L: Importance 0.1443\n",
      "Mean Deviation_L: Importance 0.0921\n",
      "RMS_L: Importance 0.1328\n",
      "Skewness_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0111\n",
      "Mean_Freq_L: Importance 0.0071\n",
      "Irregularity_L: Importance 0.0063\n",
      "Variance_L: Importance 0.0690\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0084\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1004\n",
      "Mean_H: Importance 0.0807\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0641\n",
      "RMS_H: Importance 0.1223\n",
      "Centroid_H: Importance 0.0009\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0390\n",
      "Kurtosis_Freq_H: Importance 0.0137\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0392\n",
      "Mean_L: Importance 0.1410\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0808\n",
      "Entropy_L: Importance 0.0189\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0399\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1002\n",
      "Mean_H: Importance 0.0800\n",
      "Std_H: Importance 0.0191\n",
      "Mean Deviation_H: Importance 0.0206\n",
      "RMS_H: Importance 0.1009\n",
      "Entropy_H: Importance 0.1000\n",
      "Skewness_Freq_H: Importance 0.0007\n",
      "Mean_Freq_H: Importance 0.0199\n",
      "Kurtosis_Freq_H: Importance 0.0185\n",
      "Irregularity_H: Importance 0.0001\n",
      "Min_L: Importance 0.0190\n",
      "Mean_L: Importance 0.1208\n",
      "Std_L: Importance 0.1202\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Entropy_L: Importance 0.0600\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0129\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 2** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1000\n",
      "Mean_H: Importance 0.0850\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.0856\n",
      "Skewness_H: Importance 0.0000\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.1400\n",
      "Spread_H: Importance 0.0157\n",
      "Mean_Freq_H: Importance 0.0405\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0398\n",
      "Mean_L: Importance 0.1200\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0003\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0196\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0217\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 2\n",
    "test_adc_num = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing of ADC 3**\n",
    "To conduct experiments, **Probe 3** (SMA cable which trandfers data) is used consistently throughout. **Probe 3** is kept constant, and the machine learning model is **always trained on ADC 3**. The model is then tested on **ADC 1** and **ADC 2** to evaluate its generalization.\n",
    "\n",
    "The **sampling rate** is varied across different tests, ranging from **10 MSPS to 300 MSPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 3** data and Testing on **ADC 1**\n",
    "To test ADC 3 with changing probe to ADC 1 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0117\n",
      "Min_H: Importance 0.0313\n",
      "Mean_H: Importance 0.0859\n",
      "Mean Deviation_H: Importance 0.0079\n",
      "RMS_H: Importance 0.1399\n",
      "Skewness_H: Importance 0.0035\n",
      "Peak-to-Peak_H: Importance 0.0054\n",
      "Centroid_H: Importance 0.0052\n",
      "Entropy_H: Importance 0.2163\n",
      "Spread_H: Importance 0.0080\n",
      "Mean_Freq_H: Importance 0.0864\n",
      "Variance_H: Importance 0.0453\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0804\n",
      "Mean_L: Importance 0.0361\n",
      "RMS_L: Importance 0.0507\n",
      "Entropy_L: Importance 0.0519\n",
      "Mean_Freq_L: Importance 0.0253\n",
      "Variance_L: Importance 0.0882\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9635\n",
      "Balanced Accuracy: 0.9635\n",
      "MCC: 0.9281\n",
      "Log Loss: 0.0772\n",
      "F1 Score: 0.9644\n",
      "Recall: 0.9872\n",
      "Precision: 0.9426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.99      0.94      0.96      9998\n",
      "         CH2       0.94      0.99      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9397  601]\n",
      " [ 128 9870]]\n",
      "False Positive Rate (FPR): 0.0601\n",
      "False Negative Rate (FNR): 0.0128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1189\n",
      "Min_H: Importance 0.0311\n",
      "Mean_H: Importance 0.1143\n",
      "RMS_H: Importance 0.2099\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0368\n",
      "Mean_Freq_H: Importance 0.0777\n",
      "Variance_H: Importance 0.0597\n",
      "Max_L: Importance 0.0157\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1254\n",
      "Std_L: Importance 0.0009\n",
      "RMS_L: Importance 0.1253\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0339\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0127\n",
      "Variance_L: Importance 0.0070\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9988\n",
      "Balanced Accuracy: 0.9988\n",
      "MCC: 0.9976\n",
      "Log Loss: 0.0148\n",
      "F1 Score: 0.9988\n",
      "Recall: 1.0000\n",
      "Precision: 0.9976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4986   12]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0024\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1478\n",
      "Min_H: Importance 0.0273\n",
      "Mean_H: Importance 0.1356\n",
      "Mean Deviation_H: Importance 0.0106\n",
      "RMS_H: Importance 0.1502\n",
      "Entropy_H: Importance 0.0193\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0682\n",
      "Variance_H: Importance 0.0617\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0353\n",
      "Mean_L: Importance 0.1086\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1615\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0107\n",
      "Variance_L: Importance 0.0273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.8763\n",
      "Balanced Accuracy: 0.8763\n",
      "MCC: 0.7765\n",
      "Log Loss: 0.2295\n",
      "F1 Score: 0.8899\n",
      "Recall: 0.9994\n",
      "Precision: 0.8020\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.75      0.86      3331\n",
      "         CH2       0.80      1.00      0.89      3332\n",
      "\n",
      "    accuracy                           0.88      6663\n",
      "   macro avg       0.90      0.88      0.87      6663\n",
      "weighted avg       0.90      0.88      0.87      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2509  822]\n",
      " [   2 3330]]\n",
      "False Positive Rate (FPR): 0.2468\n",
      "False Negative Rate (FNR): 0.0006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Min_H: Importance 0.0300\n",
      "Mean_H: Importance 0.0895\n",
      "Std_H: Importance 0.0259\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1371\n",
      "Centroid_H: Importance 0.0030\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0665\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1919\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1319\n",
      "Entropy_L: Importance 0.0301\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0862\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0128\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0918\n",
      "Min_H: Importance 0.0211\n",
      "Mean_H: Importance 0.0998\n",
      "Std_H: Importance 0.0101\n",
      "Mean Deviation_H: Importance 0.0110\n",
      "RMS_H: Importance 0.1347\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0520\n",
      "Variance_H: Importance 0.0640\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.0187\n",
      "Mean_L: Importance 0.1890\n",
      "Mean Deviation_L: Importance 0.0100\n",
      "RMS_L: Importance 0.1280\n",
      "Kurtosis_L: Importance 0.0015\n",
      "Entropy_L: Importance 0.0286\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1073\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0045\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1196\n",
      "Min_H: Importance 0.0044\n",
      "Mean_H: Importance 0.0928\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0270\n",
      "RMS_H: Importance 0.1247\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0333\n",
      "Spread_H: Importance 0.0015\n",
      "Mean_Freq_H: Importance 0.0496\n",
      "Variance_H: Importance 0.0980\n",
      "Max_L: Importance 0.0087\n",
      "Min_L: Importance 0.0298\n",
      "Mean_L: Importance 0.1661\n",
      "Mean Deviation_L: Importance 0.0101\n",
      "RMS_L: Importance 0.1245\n",
      "Entropy_L: Importance 0.0075\n",
      "Variance_L: Importance 0.0911\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0002\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1017\n",
      "Mean_H: Importance 0.0875\n",
      "Std_H: Importance 0.0299\n",
      "Mean Deviation_H: Importance 0.0443\n",
      "RMS_H: Importance 0.1300\n",
      "Entropy_H: Importance 0.0151\n",
      "Mean_Freq_H: Importance 0.0147\n",
      "Irregularity_H: Importance 0.0056\n",
      "Variance_H: Importance 0.0216\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0409\n",
      "Mean_L: Importance 0.1808\n",
      "Std_L: Importance 0.0041\n",
      "Mean Deviation_L: Importance 0.1105\n",
      "RMS_L: Importance 0.1201\n",
      "Entropy_L: Importance 0.0124\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0210\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0402\n",
      "RMS_H: Importance 0.1245\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0103\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0876\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0398\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1020\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0840\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0864\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 3** and tested on **ADC 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (398, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0829\n",
      "Mean Deviation_H: Importance 0.0400\n",
      "RMS_H: Importance 0.1024\n",
      "Peak-to-Peak_H: Importance 0.0007\n",
      "Entropy_H: Importance 0.1000\n",
      "Mean_Freq_H: Importance 0.0198\n",
      "Kurtosis_Freq_H: Importance 0.0142\n",
      "Irregularity_H: Importance 0.0160\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.1220\n",
      "Mean_L: Importance 0.1000\n",
      "Std_L: Importance 0.0799\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0831\n",
      "Entropy_L: Importance 0.0193\n",
      "Mean_Freq_L: Importance 0.0197\n",
      "Variance_L: Importance 0.0600\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0404\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **Probe 3** and tested on **Probe 1**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH1: (332, 38)\n",
      "✅ Loaded Probe3 ADC1 CH2: (331, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1011\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0401\n",
      "Kurtosis_Freq_H: Importance 0.0182\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0938\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       332\n",
      "         CH2       1.00      1.00      1.00       331\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332   0]\n",
      " [  0 331]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 1\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training **ADC 3** data and Testing on **ADC 2**\n",
    "To test ADC 3 with changing probe to ADC 2 results are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 10MSPS**\n",
    "This section contains data collected using a sampling rate of **10MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (9998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (9998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0117\n",
      "Min_H: Importance 0.0313\n",
      "Mean_H: Importance 0.0859\n",
      "Mean Deviation_H: Importance 0.0079\n",
      "RMS_H: Importance 0.1399\n",
      "Skewness_H: Importance 0.0035\n",
      "Peak-to-Peak_H: Importance 0.0054\n",
      "Centroid_H: Importance 0.0052\n",
      "Entropy_H: Importance 0.2163\n",
      "Spread_H: Importance 0.0080\n",
      "Mean_Freq_H: Importance 0.0864\n",
      "Variance_H: Importance 0.0453\n",
      "Max_L: Importance 0.0042\n",
      "Min_L: Importance 0.0804\n",
      "Mean_L: Importance 0.0361\n",
      "RMS_L: Importance 0.0507\n",
      "Entropy_L: Importance 0.0519\n",
      "Mean_Freq_L: Importance 0.0253\n",
      "Variance_L: Importance 0.0882\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9608\n",
      "Balanced Accuracy: 0.9608\n",
      "MCC: 0.9235\n",
      "Log Loss: 0.5320\n",
      "F1 Score: 0.9595\n",
      "Recall: 0.9290\n",
      "Precision: 0.9921\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       0.93      0.99      0.96      9998\n",
      "         CH2       0.99      0.93      0.96      9998\n",
      "\n",
      "    accuracy                           0.96     19996\n",
      "   macro avg       0.96      0.96      0.96     19996\n",
      "weighted avg       0.96      0.96      0.96     19996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9924   74]\n",
      " [ 710 9288]]\n",
      "False Positive Rate (FPR): 0.0074\n",
      "False Negative Rate (FNR): 0.0710\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 10\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 20MSPS**\n",
    "This section contains data collected using a sampling rate of **20MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (4998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (4998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1189\n",
      "Min_H: Importance 0.0311\n",
      "Mean_H: Importance 0.1143\n",
      "RMS_H: Importance 0.2099\n",
      "Centroid_H: Importance 0.0002\n",
      "Entropy_H: Importance 0.0368\n",
      "Mean_Freq_H: Importance 0.0777\n",
      "Variance_H: Importance 0.0597\n",
      "Max_L: Importance 0.0157\n",
      "Min_L: Importance 0.0274\n",
      "Mean_L: Importance 0.1254\n",
      "Std_L: Importance 0.0009\n",
      "RMS_L: Importance 0.1253\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0339\n",
      "Spread_L: Importance 0.0004\n",
      "Mean_Freq_L: Importance 0.0127\n",
      "Variance_L: Importance 0.0070\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0008\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      4998\n",
      "         CH2       1.00      1.00      1.00      4998\n",
      "\n",
      "    accuracy                           1.00      9996\n",
      "   macro avg       1.00      1.00      1.00      9996\n",
      "weighted avg       1.00      1.00      1.00      9996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4998    0]\n",
      " [   0 4998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3 # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 20\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 30MSPS**\n",
    "This section contains data collected using a sampling rate of **30MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (3332, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (3331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (3332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1478\n",
      "Min_H: Importance 0.0273\n",
      "Mean_H: Importance 0.1356\n",
      "Mean Deviation_H: Importance 0.0106\n",
      "RMS_H: Importance 0.1502\n",
      "Entropy_H: Importance 0.0193\n",
      "Spread_H: Importance 0.0010\n",
      "Mean_Freq_H: Importance 0.0682\n",
      "Variance_H: Importance 0.0617\n",
      "Max_L: Importance 0.0018\n",
      "Min_L: Importance 0.0353\n",
      "Mean_L: Importance 0.1086\n",
      "Std_L: Importance 0.0005\n",
      "RMS_L: Importance 0.1615\n",
      "Kurtosis_L: Importance 0.0016\n",
      "Peak-to-Peak_L: Importance 0.0004\n",
      "Entropy_L: Importance 0.0305\n",
      "Mean_Freq_L: Importance 0.0107\n",
      "Variance_L: Importance 0.0273\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 0.9871\n",
      "Balanced Accuracy: 0.9871\n",
      "MCC: 0.9745\n",
      "Log Loss: 0.0862\n",
      "F1 Score: 0.9873\n",
      "Recall: 1.0000\n",
      "Precision: 0.9748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      0.97      0.99      3331\n",
      "         CH2       0.97      1.00      0.99      3332\n",
      "\n",
      "    accuracy                           0.99      6663\n",
      "   macro avg       0.99      0.99      0.99      6663\n",
      "weighted avg       0.99      0.99      0.99      6663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3245   86]\n",
      " [   0 3332]]\n",
      "False Positive Rate (FPR): 0.0258\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 30\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 40MSPS**\n",
    "This section contains data collected using a sampling rate of **40MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (2481, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (2481, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1009\n",
      "Min_H: Importance 0.0300\n",
      "Mean_H: Importance 0.0895\n",
      "Std_H: Importance 0.0259\n",
      "Mean Deviation_H: Importance 0.0172\n",
      "RMS_H: Importance 0.1371\n",
      "Centroid_H: Importance 0.0030\n",
      "Entropy_H: Importance 0.0139\n",
      "Mean_Freq_H: Importance 0.0570\n",
      "Variance_H: Importance 0.0665\n",
      "Max_L: Importance 0.0005\n",
      "Min_L: Importance 0.0183\n",
      "Mean_L: Importance 0.1919\n",
      "Std_L: Importance 0.0000\n",
      "RMS_L: Importance 0.1319\n",
      "Entropy_L: Importance 0.0301\n",
      "Kurtosis_Freq_L: Importance 0.0000\n",
      "Variance_L: Importance 0.0862\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0038\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      2481\n",
      "         CH2       1.00      1.00      1.00      2481\n",
      "\n",
      "    accuracy                           1.00      4962\n",
      "   macro avg       1.00      1.00      1.00      4962\n",
      "weighted avg       1.00      1.00      1.00      4962\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2481    0]\n",
      " [   0 2481]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 40\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 50MSPS**\n",
    "This section contains data collected using a sampling rate of **50MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (1998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (1998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0918\n",
      "Min_H: Importance 0.0211\n",
      "Mean_H: Importance 0.0998\n",
      "Std_H: Importance 0.0101\n",
      "Mean Deviation_H: Importance 0.0110\n",
      "RMS_H: Importance 0.1347\n",
      "Centroid_H: Importance 0.0005\n",
      "Entropy_H: Importance 0.0176\n",
      "Mean_Freq_H: Importance 0.0520\n",
      "Variance_H: Importance 0.0640\n",
      "Max_L: Importance 0.0028\n",
      "Min_L: Importance 0.0187\n",
      "Mean_L: Importance 0.1890\n",
      "Mean Deviation_L: Importance 0.0100\n",
      "RMS_L: Importance 0.1280\n",
      "Kurtosis_L: Importance 0.0015\n",
      "Entropy_L: Importance 0.0286\n",
      "Mean_Freq_L: Importance 0.0113\n",
      "Variance_L: Importance 0.1073\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0013\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00      1998\n",
      "         CH2       1.00      1.00      1.00      1998\n",
      "\n",
      "    accuracy                           1.00      3996\n",
      "   macro avg       1.00      1.00      1.00      3996\n",
      "weighted avg       1.00      1.00      1.00      3996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998    0]\n",
      " [   0 1998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 50\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 100MSPS**\n",
    "This section contains data collected using a sampling rate of **100MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (998, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (998, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1196\n",
      "Min_H: Importance 0.0044\n",
      "Mean_H: Importance 0.0928\n",
      "Std_H: Importance 0.0110\n",
      "Mean Deviation_H: Importance 0.0270\n",
      "RMS_H: Importance 0.1247\n",
      "Centroid_H: Importance 0.0000\n",
      "Entropy_H: Importance 0.0333\n",
      "Spread_H: Importance 0.0015\n",
      "Mean_Freq_H: Importance 0.0496\n",
      "Variance_H: Importance 0.0980\n",
      "Max_L: Importance 0.0087\n",
      "Min_L: Importance 0.0298\n",
      "Mean_L: Importance 0.1661\n",
      "Mean Deviation_L: Importance 0.0101\n",
      "RMS_L: Importance 0.1245\n",
      "Entropy_L: Importance 0.0075\n",
      "Variance_L: Importance 0.0911\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0003\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       998\n",
      "         CH2       1.00      1.00      1.00       998\n",
      "\n",
      "    accuracy                           1.00      1996\n",
      "   macro avg       1.00      1.00      1.00      1996\n",
      "weighted avg       1.00      1.00      1.00      1996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[998   0]\n",
      " [  0 998]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3 # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 100\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 150MSPS**\n",
    "This section contains data collected using a sampling rate of **150MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (665, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (665, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.1017\n",
      "Mean_H: Importance 0.0875\n",
      "Std_H: Importance 0.0299\n",
      "Mean Deviation_H: Importance 0.0443\n",
      "RMS_H: Importance 0.1300\n",
      "Entropy_H: Importance 0.0151\n",
      "Mean_Freq_H: Importance 0.0147\n",
      "Irregularity_H: Importance 0.0056\n",
      "Variance_H: Importance 0.0216\n",
      "Max_L: Importance 0.0000\n",
      "Min_L: Importance 0.0409\n",
      "Mean_L: Importance 0.1808\n",
      "Std_L: Importance 0.0041\n",
      "Mean Deviation_L: Importance 0.1105\n",
      "RMS_L: Importance 0.1201\n",
      "Entropy_L: Importance 0.0124\n",
      "Variance_L: Importance 0.0810\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0028\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       665\n",
      "         CH2       1.00      1.00      1.00       665\n",
      "\n",
      "    accuracy                           1.00      1330\n",
      "   macro avg       1.00      1.00      1.00      1330\n",
      "weighted avg       1.00      1.00      1.00      1330\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[665   0]\n",
      " [  0 665]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 150\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 200MSPS**\n",
    "This section contains data collected using a sampling rate of **200MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (498, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (498, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0402\n",
      "RMS_H: Importance 0.1245\n",
      "Entropy_H: Importance 0.0182\n",
      "Mean_Freq_H: Importance 0.0180\n",
      "Kurtosis_Freq_H: Importance 0.0103\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0876\n",
      "Mean_L: Importance 0.1600\n",
      "Std_L: Importance 0.0398\n",
      "Mean Deviation_L: Importance 0.0800\n",
      "RMS_L: Importance 0.1020\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0154\n",
      "Variance_L: Importance 0.0840\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0076\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       498\n",
      "         CH2       1.00      1.00      1.00       498\n",
      "\n",
      "    accuracy                           1.00       996\n",
      "   macro avg       1.00      1.00      1.00       996\n",
      "weighted avg       1.00      1.00      1.00       996\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[498   0]\n",
      " [  0 498]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3 # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 200\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 250MSPS**\n",
    "This section contains data collected using a sampling rate of **250MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe1 ADC3 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC3 CH2: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH1: (398, 38)\n",
      "✅ Loaded Probe1 ADC2 CH2: (398, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0805\n",
      "Mean_H: Importance 0.0828\n",
      "Mean Deviation_H: Importance 0.0800\n",
      "RMS_H: Importance 0.1045\n",
      "Kurtosis_H: Importance 0.0001\n",
      "Peak-to-Peak_H: Importance 0.0000\n",
      "Mean_Freq_H: Importance 0.0187\n",
      "Kurtosis_Freq_H: Importance 0.0324\n",
      "Variance_H: Importance 0.0200\n",
      "Min_L: Importance 0.0796\n",
      "Mean_L: Importance 0.1604\n",
      "Std_L: Importance 0.0202\n",
      "Mean Deviation_L: Importance 0.0802\n",
      "RMS_L: Importance 0.1219\n",
      "Centroid_L: Importance 0.0006\n",
      "Entropy_L: Importance 0.0198\n",
      "Mean_Freq_L: Importance 0.0381\n",
      "Variance_L: Importance 0.0602\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0025\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       398\n",
      "         CH2       1.00      1.00      1.00       398\n",
      "\n",
      "    accuracy                           1.00       796\n",
      "   macro avg       1.00      1.00      1.00       796\n",
      "weighted avg       1.00      1.00      1.00       796\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398   0]\n",
      " [  0 398]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 1  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 250\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Rate 300MSPS**\n",
    "This section contains data collected using a sampling rate of **300MSPS**. The model is trained on **ADC 3** and tested on **ADC 2**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Probe3 ADC3 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC3 CH2: (332, 38)\n",
      "✅ Loaded Probe3 ADC2 CH1: (331, 38)\n",
      "✅ Loaded Probe3 ADC2 CH2: (332, 38)\n",
      "\n",
      "Top Important Features:\n",
      "Max_H: Importance 0.0600\n",
      "Mean_H: Importance 0.0800\n",
      "Mean Deviation_H: Importance 0.0200\n",
      "RMS_H: Importance 0.1011\n",
      "Entropy_H: Importance 0.0800\n",
      "Mean_Freq_H: Importance 0.0401\n",
      "Kurtosis_Freq_H: Importance 0.0182\n",
      "Min_L: Importance 0.0600\n",
      "Mean_L: Importance 0.1200\n",
      "Std_L: Importance 0.1000\n",
      "Mean Deviation_L: Importance 0.0600\n",
      "RMS_L: Importance 0.0800\n",
      "Peak-to-Peak_L: Importance 0.0007\n",
      "Entropy_L: Importance 0.0600\n",
      "Mean_Freq_L: Importance 0.0399\n",
      "Variance_L: Importance 0.0800\n",
      "\n",
      "Model Performance After Feature Selection:\n",
      "Test Accuracy: 1.0000\n",
      "Balanced Accuracy: 1.0000\n",
      "MCC: 1.0000\n",
      "Log Loss: 0.0069\n",
      "F1 Score: 1.0000\n",
      "Recall: 1.0000\n",
      "Precision: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CH1       1.00      1.00      1.00       331\n",
      "         CH2       1.00      1.00      1.00       332\n",
      "\n",
      "    accuracy                           1.00       663\n",
      "   macro avg       1.00      1.00      1.00       663\n",
      "weighted avg       1.00      1.00      1.00       663\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[331   0]\n",
      " [  0 332]]\n",
      "False Positive Rate (FPR): 0.0000\n",
      "False Negative Rate (FNR): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, matthews_corrcoef, \n",
    "    balanced_accuracy_score, log_loss, f1_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "# Fixed Configuration\n",
    "# ==================================================================\n",
    "base_directory = \"C:\\\\Users\\\\awm21\\\\Documents\\\\Probe_Vari\\\\Neural_Networks\\\\Features_Vitis\"\n",
    "\n",
    "probe_dataset = 3  # Fixed probe, change as needed\n",
    "train_adc_num = 3\n",
    "test_adc_num = 2\n",
    "sample_rate = 300\n",
    "\n",
    "# Automatically determine probe name\n",
    "probe = f\"Probe{probe_dataset}\"\n",
    "\n",
    "# Feature Names (Time High, Freq High, Time Low, Freq Low)\n",
    "feature_names = [\n",
    "    \"Max_H\", \"Min_H\", \"Mean_H\", \"Std_H\", \"Mean Deviation_H\", \"RMS_H\", \"Skewness_H\", \"Kurtosis_H\", \"Peak-to-Peak_H\", \"Zero Crossing Rate_H\",\n",
    "    \"Centroid_H\", \"Entropy_H\", \"Spread_H\", \"Skewness_Freq_H\", \"Mean_Freq_H\", \"Kurtosis_Freq_H\", \"Irregularity_H\", \"Variance_H\", \"Dominant_Freq_H\",\n",
    "    \"Max_L\", \"Min_L\", \"Mean_L\", \"Std_L\", \"Mean Deviation_L\", \"RMS_L\", \"Skewness_L\", \"Kurtosis_L\", \"Peak-to-Peak_L\", \"Zero Crossing Rate_L\",\n",
    "    \"Centroid_L\", \"Entropy_L\", \"Spread_L\", \"Skewness_Freq_L\", \"Mean_Freq_L\", \"Kurtosis_Freq_L\", \"Irregularity_L\", \"Variance_L\", \"Dominant_Freq_L\"\n",
    "]\n",
    "\n",
    "def load_channel_data(probe_name, adc_num, channel):\n",
    "    \"\"\"Load specific ADC/channel data for a probe and verify ADC consistency.\"\"\"\n",
    "    folder = os.path.join(base_directory, f\"P{probe_name[-1]}_fd\")\n",
    "    filename = f'fd_{probe_name}_ADC{adc_num}_CH{channel}_{sample_rate}_MSPS.npy'\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    print(f\"✅ Loaded {probe_name} ADC{adc_num} CH{channel}: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# ==================================================================\n",
    "# Training & Test Data\n",
    "# ==================================================================\n",
    "train_ch0 = load_channel_data(probe, train_adc_num, 1)\n",
    "train_ch1 = load_channel_data(probe, train_adc_num, 2)\n",
    "test_ch0 = load_channel_data(probe, test_adc_num, 1)\n",
    "test_ch1 = load_channel_data(probe, test_adc_num, 2)\n",
    "\n",
    "X_train = np.vstack([train_ch0, train_ch1])\n",
    "y_train = np.concatenate([np.zeros(train_ch0.shape[0]), np.ones(train_ch1.shape[0])])\n",
    "X_test = np.vstack([test_ch0, test_ch1])\n",
    "y_test = np.concatenate([np.zeros(test_ch0.shape[0]), np.ones(test_ch1.shape[0])])\n",
    "\n",
    "# ==================================================================\n",
    "# Data Preprocessing\n",
    "# ==================================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==================================================================\n",
    "# Train Initial Model for Feature Selection\n",
    "# ==================================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify Important Features\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = np.where(feature_importances > np.percentile(feature_importances, 50))[0]\n",
    "X_train_selected = X_train_scaled[:, important_features]\n",
    "X_test_selected = X_test_scaled[:, important_features]\n",
    "\n",
    "# Print Important Features\n",
    "print(\"\\nTop Important Features:\")\n",
    "for feature_idx in important_features:\n",
    "    print(f\"{feature_names[feature_idx]}: Importance {feature_importances[feature_idx]:.4f}\")\n",
    "\n",
    "# ==================================================================\n",
    "# Retrain Model with Selected Features\n",
    "# ==================================================================\n",
    "rf_selected = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Create a unique folder name dynamically\n",
    "model_dir = f\"Models/Tuning_ADC{train_adc_num}_to_ADC{test_adc_num}_Probe{probe_dataset}_{sample_rate}_MSPS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the Model and Scaler\n",
    "joblib.dump(rf_selected, os.path.join(model_dir, \"RandomForest_Model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"Scaler.pkl\"))\n",
    "joblib.dump(important_features, os.path.join(model_dir, \"Selected_Features.pkl\"))\n",
    "\n",
    "# Predictions\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_probs_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ==================================================================\n",
    "# Performance Metrics After Feature Selection\n",
    "# ==================================================================\n",
    "print(\"\\nModel Performance After Feature Selection:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs_selected):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selected):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selected):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selected, target_names=['CH1', 'CH2']))\n",
    "\n",
    "# Compute Confusion Matrix, FPR, and FNR\n",
    "cm = confusion_matrix(y_test, y_pred_selected)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "fp_rate = cm[0][1] / (cm[0][1] + cm[0][0])\n",
    "fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "print(f\"False Positive Rate (FPR): {fp_rate:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fn_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
